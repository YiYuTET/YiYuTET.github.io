<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>1.Flink简介</title>
      <link href="/post/971239a4.html"/>
      <url>/post/971239a4.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Flink简介"><a href="#1-Flink简介" class="headerlink" title="1.Flink简介"></a>1.Flink简介</h1><h2 id="1-1-离线批计算与实时流计算"><a href="#1-1-离线批计算与实时流计算" class="headerlink" title="1.1 离线批计算与实时流计算"></a>1.1 离线批计算与实时流计算</h2><p><strong><font color=OrangeRed><br>批计算与流计算，本质上就是对有界流和无界流的计算。<br></font></strong></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131702169.png" alt="Flink图1"></p><center>图1</center><ul><li><strong><font color=#0066FF>批计算</font></strong><br>  针对有界流，由于在产出计算结果前可以看到整个(完整)数据集，因而如下计算都可实现，对数据排序，计算全局统计值，对输入数据的整体产出最终汇总聚合报表。</li><li><strong><font color=#0066FF>流计算</font></strong><br>  针对无界流，由于永远无法看到输入数据的整体(数据的输入永远无法结束)，只能每逢数据到达就进行计算，并输出“当时”的计算结果（因而计算结果也不会是一个一次性的结果，而是源源不断的无界的结果流）</li></ul><h2 id="1-2-Apache-Flink-是什么"><a href="#1-2-Apache-Flink-是什么" class="headerlink" title="1.2 Apache Flink 是什么?"></a>1.2 Apache Flink 是什么?</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131703655.png" alt="Flink图2"></p><center>图2</center><p><strong><font color=OrangeRed>Flink是一个以流为核心的高可用，高性能的分布式计算引擎。具备流批一体、高吞吐、低延迟、容错能力、大规模复杂计算等特点，在数据流上提供数据分发、通信等功能。</font></strong></p><ul><li><p><strong><font color=#0066FF>数据流</font></strong><br>  所有产生的数据都天然带有时间概念，把事件按照时间顺序排列起来，就形成了一个事件流，也被称作数据流。</p></li><li><p><strong><font color=#0066FF>流批一体</font></strong><br>  首先必须明白什么是有界数据和无界数据</p><ul><li><strong><font color=#0066FF>有界数据</font></strong><br>  有界数据就是在一个确定的时间范围内的数据流，有开始，有结束，一旦确定就不会再改变，一般批处理用来处理有界数据，如图1的bounded stream。</li><li><strong><font color=#0066FF>无界数据</font></strong><br>  无界数据就是持续产生的数据流，数据是无限的，有开始，无结束，一般流处理用来处理无界数据，如图1的unbounded stream。</li></ul><p>  Flink的设计思想是以流为核心，批是流的特例，擅长处理无界和有界数据，Flink提供精确的时间控制能力和有状态计算机制，可以轻松应对无界数据流，同时提供窗口处理有界数据流，所以被称为流批一体。</p></li><li><p><strong><font color=#0066FF>容错能力</font></strong><br>  在分布式系统中，硬件故障、进程异常、应用异常、网络故障等异常无处不在，Flink引擎必须保证故障发生后不仅可以重启应用程序，还要确保其内部状态保持一致，从最后一次正确的时间点重新出发。<br>  Flink提供集群级容错和应用级容错能力</p><ul><li><strong><font color=#0066FF>集群级容错</font></strong><br>  Flink与集群管理器紧密连接，如YARN、Kubernetes，当进程挂掉后，自动重启新进程接管之前的工作，同时具备高可用性，可消除所有单点故障。</li><li><strong><font color=#0066FF>应用级容错能力</font></strong><br>  Flink使用轻量级分布式快照，设计检查点(checkpoint)实现可靠容错。</li></ul><p>  Flink利用检查点特性，在框架层面提供 Exactly-once 语义，即端到端的一致性，确保数据仅处理一次，不会重复也不会丢失，即使出现故障，也能保证数据只写一次。</p></li></ul><h2 id="1-3-Flink的架构"><a href="#1-3-Flink的架构" class="headerlink" title="1.3 Flink的架构"></a>1.3 Flink的架构</h2><p><strong>Flink架构分为技术架构和运行架构两部分。</strong></p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>如下图为Flink技术架构</td></tr></table><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131705097.png" alt="Flink图3"/><p>Flink 作为流批一体的分布式计算引擎，必须提供面向开发人员的API层，同时还需要跟外部数据存储进行交互，需要连接器，作业开发、测试完毕后，需要提交集群执行，需要部署层，同时还需要运维人员能够管理和监控，还提供图计算、机器学习、SQL等，需要应用框架层。</p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>如下图为Flink运行架构</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131706211.png" alt="Flink图4"></p><p>Flink 集群采取 Master - Slave 架构，Master的角色为 JobManager，负责集群和作业管理，Slave的角色是 TaskManager，负责执行计算任务，同时，Flink 提供客户端 Client 来管理集群和提交任务，JobManager 和 TaskManager 是集群的进程。</p><p><strong>（1）Client</strong></p><p>Flink 客户端是Flink 提供的 CLI 命令行工具，用来提交 Flink 作业到 Flink 集群，在客户端中负责 StreamGraph (流图)和 Job Graph (作业图)的构建。</p><p><strong>（2）JobManager</strong></p><p>JobManager 根据并行度将 Flink 客户端提交的Flink 应用分解为子任务，从资源管理器 ResourceManager 申请所需的计算资源，资源具备之后，开始分发任务到 TaskManager 执行 Task，并负责应用容错、跟踪作业的执行状态、发现异常后恢复作业等。</p><p><strong>（3）TaskManager</strong></p><p>TaskManager 接收 JobManager 分发的子任务，根据自身的资源情况管理子任务的启动、停止、销毁、异常恢复等生命周期阶段。Flink程序中必须有一个TaskManager。</p><h2 id="1-4-Flink的特性"><a href="#1-4-Flink的特性" class="headerlink" title="1.4 Flink的特性"></a>1.4 Flink的特性</h2><table><tr><td bgcolor=Gainsboro><font size=5><b>适用于几乎所有流式数据处理场景</td></tr></table><ul><li><strong>事件驱动型应用</strong></li><li><strong>流、批数据分析</strong></li><li><strong>数据管道及ETL</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131710215.jpeg" alt="Flink图5"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>自带状态管理机制</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131708285.png" alt="Flink图7"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>强大的准确性保证</td></tr></table><ul><li><strong>Exactly-once 状态一致性</strong></li><li><strong>事件时间处理</strong></li><li><strong>专业的迟到数据处理</strong></li></ul><table><tr><td bgcolor=Gainsboro><font size=5><b>灵活丰富的多层API</td></tr></table><ul><li><strong>流、批数据之上的SQL查询</strong></li><li><strong>流、批数据之上的TableAPI</strong></li><li><strong>DataStream流处理API、DataSet批处理API</strong></li><li><strong>精细可控的processFunction</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131712730.png" alt="Flink图6"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>规模弹性扩展</td></tr></table><ul><li><strong>可扩展的分布式架构</strong></li><li><strong>支持超大状态管理</strong></li><li><strong>增量checkpoint机制</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131712016.png" alt="Flink图8"></p><center><font color=OrangeRed size=4><b>算子粒度的独立并行度灵活配置(槽位资源可扩展、算子任务实例可扩展）</font></center><table><tr><td bgcolor=Gainsboro><font size=5><b>强大的运维能力</td></tr></table><ul><li><strong>弹性实施部署机制</strong></li><li><strong>高可用机制</strong></li><li><strong>保存点恢复机制</strong></li></ul><table><tr><td bgcolor=Gainsboro><font size=5><b>优秀的性能</font></td></tr></table><ul><li><strong>低延迟</strong></li><li><strong>高吞吐</strong></li><li><strong>内存计算</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Flink</title>
      <link href="/post/9aba4271.html"/>
      <url>/post/9aba4271.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Flink"><a href="#云服务器安装Flink" class="headerlink" title="云服务器安装Flink"></a>云服务器安装Flink</h1><h2 id="1-Flink下载地址"><a href="#1-Flink下载地址" class="headerlink" title="1.Flink下载地址"></a>1.Flink<a href="https://flink.apache.org/downloads.html">下载地址</a></h2><h2 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf flink-1.13.2-bin-scala_2.11.tgz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export FLINK_HOME=/home/software/flink-1.13.2</span><br><span class="line">export PATH=$PATH:/$FLINK_HOME/bin</span><br><span class="line">export HADOOP_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br></pre></td></tr></table></figure><h2 id="4-配置conf文件"><a href="#4-配置conf文件" class="headerlink" title="4.配置conf文件"></a>4.配置conf文件</h2><p>修改conf目录下的flink-conf.yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim flink-conf.yaml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.219</span></span><br><span class="line"></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://192.168.0.219:9000/flink-checkpoints</span></span><br><span class="line"><span class="attr">state.savepoints.dir:</span> <span class="string">hdfs://192.168.0.219:9000/flink-savepoints</span></span><br><span class="line"></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.219</span><span class="string">:2181</span></span><br><span class="line"><span class="attr">high-availability.storageDir:</span> <span class="string">hdfs://192.168.0.219:9000/flink/ha/</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.client.acl:</span> <span class="string">open</span></span><br></pre></td></tr></table></figure><p>修改masters的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">114.116.24.98:8082</span><br></pre></td></tr></table></figure><p>修改workers的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">114.116.24.98</span><br></pre></td></tr></table></figure><p>修改conf下zoo.cfg配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/home/software/flink-1.13.2/tmp/zookeeper</span><br><span class="line">server.1=localhost:2887:3887</span><br></pre></td></tr></table></figure><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>进入bin目录下，启动Flink</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><p>jps查看进程出现TaskManagerRunner和StandaloneSessionClusterEntrypoint</p><p>在浏览器输入<a href="http://114.116.24.98:8082可进入Flink的Web页面">http://114.116.24.98:8082可进入Flink的Web页面</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Spark</title>
      <link href="/post/41b91bba.html"/>
      <url>/post/41b91bba.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器伪分布式部署Spark"><a href="#云服务器伪分布式部署Spark" class="headerlink" title="云服务器伪分布式部署Spark"></a>云服务器伪分布式部署Spark</h1><h2 id="1-下载Spark-下载地址"><a href="#1-下载Spark-下载地址" class="headerlink" title="1.下载Spark 下载地址"></a>1.下载Spark <a href="https://spark.apache.org/downloads.html">下载地址</a></h2><p>安装spark前需要安装scala （<a href="https://www.scala-lang.org/download/">scala下载地址</a>）</p><h2 id="2-上传至服务器并解压"><a href="#2-上传至服务器并解压" class="headerlink" title="2.上传至服务器并解压"></a>2.上传至服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.1.2-bin-hadoop3.2.tgz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">这里没有添加sbin目录，因为会和hadoop的sbin目录冲突</span></span><br><span class="line">export SPARK_HOME=/home/software/spark-3.1.2-bin-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><h2 id="4-配置conf"><a href="#4-配置conf" class="headerlink" title="4.配置conf"></a>4.配置conf</h2><h3 id="4-1-配置spark-env-sh"><a href="#4-1-配置spark-env-sh" class="headerlink" title="4.1 配置spark-env.sh"></a>4.1 配置spark-env.sh</h3><p>进入conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/spark-3.1.2-bin-hadoop3.2/conf/</span><br></pre></td></tr></table></figure><p>复制spark-env.sh.template，重命名为spark-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/home/software/hadoop-3.2.2/bin/hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/home/software/scala-2.12.4</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/software/spark-3.1.2-bin-hadoop3.2</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=192.168.0.219</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="comment">#spark master节点的网页端口（默认是8080）可自行设置</span></span><br><span class="line"><span class="comment">#export SPARK_MASTER_WEBUI_PORT=8080</span></span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_CORES=2</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_INSTANCES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_MEMORY=2G</span><br><span class="line"></span><br><span class="line"><span class="comment">#spark worker节点的网页端口（默认是8081）可自行设置</span></span><br><span class="line"><span class="comment">#export SPARK_WORKER_WEBUI_PORT=8081</span></span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_CORES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_MEMORY=1G</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$HADOOP_HOME</span>/lib/native</span><br></pre></td></tr></table></figure><p>内容说明：</p><table><thead><tr><th>变量名</th><th align="left">说明</th></tr></thead><tbody><tr><td>JAVA_HOME</td><td align="left">jdk的安装目录</td></tr><tr><td>HADOOP_HOME</td><td align="left">hadoop的安装目录</td></tr><tr><td>HADOOP_CONF_DIR</td><td align="left">hadoop的配置文件存放目录</td></tr><tr><td>SCALA_HOME</td><td align="left">scala的安装目录</td></tr><tr><td>SPARK_HOME</td><td align="left">spark的安装目录</td></tr><tr><td>SPARK_MASTER_IP</td><td align="left">spark主节点绑定的ip地址</td></tr><tr><td>SPARK_MASTER_PORT</td><td align="left">spark主节点绑定的端口号</td></tr><tr><td>SPARK_MASTER_WEBUI_PORT</td><td align="left">spark master节点的网页端口（默认是8088）</td></tr><tr><td>SPARK_WORKER_CORES</td><td align="left">worker使用的cpu核心数</td></tr><tr><td>SPARK_WORKER_INSTANCES</td><td align="left">最多能够同时启动的EXECUTOR的实例个数</td></tr><tr><td>SPARK_WORKER_MEMORY</td><td align="left">worker分配的内存数量</td></tr><tr><td>SPARK_WORKER_WEBUI_PORT</td><td align="left">worker的网页查看绑定的端口号（默认是8081）</td></tr><tr><td>SPARK_EXECUTOR_CORES</td><td align="left">每个executor分配的cpu核心数</td></tr><tr><td>SPARK_EXECUTOR_MEMORY</td><td align="left">每个executor分配的内存数</td></tr><tr><td>LD_LIBRARY_PATH</td><td align="left">指定查找共享库</td></tr></tbody></table><h3 id="4-2-配置workers"><a href="#4-2-配置workers" class="headerlink" title="4.2 配置workers"></a>4.2 配置workers</h3><p>复制workers.template，重命名为workers</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp workers.template workers</span><br></pre></td></tr></table></figure><p>添加ip地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.219</span><br></pre></td></tr></table></figure><h3 id="4-3-配置spark-defaults-conf"><a href="#4-3-配置spark-defaults-conf" class="headerlink" title="4.3 配置spark-defaults.conf"></a>4.3 配置spark-defaults.conf</h3><p>复制spark-defaults.conf.template，重命名为spark-defaults.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://192.168.0.219:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://192.168.0.219:9000/sparkLogs</span><br><span class="line">spark.history.fs.logDirectory    hdfs://192.168.0.219:9000/sparkLogs</span><br></pre></td></tr></table></figure><p>内容说明：</p><table><thead><tr><th align="left">变量名</th><th>说明</th></tr></thead><tbody><tr><td align="left">spark.master</td><td>spark主节点所在机器及端口，默认写法是spark:&#x2F;&#x2F;</td></tr><tr><td align="left">spark.eventLog.enabled</td><td>是否打开任务日志功能，默认为false</td></tr><tr><td align="left">spark.eventLog.dir</td><td>任务日志默认存放位置，配置为一个HDFS路径即可</td></tr><tr><td align="left">spark.history.fs.logDirectory</td><td>存放历史应用日志文件的目录</td></tr></tbody></table><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>进入spark的sbin目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/spark-3.1.2-bin-hadoop3.2/sbin/</span><br></pre></td></tr></table></figure><p>启动spark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>因为这里没有配置spark&#x2F;sbin目录的环境变量 所以需要cd到spark的sbin目录下再进行启动（没配置此目录的环境变量是因为spark的启动文件 start-all.sh与hadoop的启动文件名重名，配了会发生冲突，解决办法可以将两个文件中的其中一个重命名即可，这里读者就没有进行相关的操作了，是直接全路径指定执行启动的）</p><p>查看java进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如有Master，Worker两个进程则说明伪分布式部署成功！</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成HBase</title>
      <link href="/post/e6cd08d.html"/>
      <url>/post/e6cd08d.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成HBase"><a href="#Hue集成HBase" class="headerlink" title="Hue集成HBase"></a>Hue集成HBase</h1><h2 id="1-修改hbase-site-xml"><a href="#1-修改hbase-site-xml" class="headerlink" title="1.修改hbase-site.xml"></a>1.修改hbase-site.xml</h2><p>增加如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--指定regionserver thrift端口号 默认9090 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1124<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-启动HBase，thrift服务"><a href="#2-启动HBase，thrift服务" class="headerlink" title="2.启动HBase，thrift服务"></a>2.启动HBase，thrift服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br><span class="line"></span><br><span class="line">hbase-daemon.sh start thrift -p 1124</span><br></pre></td></tr></table></figure><h2 id="3-修改hue-ini"><a href="#3-修改hue-ini" class="headerlink" title="3.修改hue.ini"></a>3.修改hue.ini</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[hbase]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">hbase_clusters</span>=(HBase|<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">1124</span>)</span><br><span class="line"></span><br><span class="line"><span class="attr">hbase_conf_dir</span>=/home/software/hbase-<span class="number">2.3</span>.<span class="number">6</span>/conf</span><br></pre></td></tr></table></figure><h2 id="4-启动Hue"><a href="#4-启动Hue" class="headerlink" title="4.启动Hue"></a>4.启动Hue</h2><p>进入hue根目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成MySQL</title>
      <link href="/post/2349cfef.html"/>
      <url>/post/2349cfef.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成MySQL"><a href="#Hue集成MySQL" class="headerlink" title="Hue集成MySQL"></a>Hue集成MySQL</h1><h2 id="1-添加MySQL方言"><a href="#1-添加MySQL方言" class="headerlink" title="1.添加MySQL方言"></a>1.添加MySQL方言</h2><p>进入hue根目录，添加到Hue Python虚拟环境中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/env/bin/pip install mysqlclient</span><br></pre></td></tr></table></figure><h2 id="2-修改hue-ini"><a href="#2-修改hue-ini" class="headerlink" title="2.修改hue.ini"></a>2.修改hue.ini</h2><p>需要把mysql的注释去掉，大概在1821行。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[notebook]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[interpreters]]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[mysql]]]</span></span><br><span class="line"><span class="attr">name</span> = MySQL</span><br><span class="line"><span class="attr">interface</span>=sqlalchemy</span><br><span class="line"><span class="attr">options</span>=<span class="string">&#x27;&#123;&quot;url&quot;: &quot;mysql://root:123456@192.168.0.219:3306/&quot;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[hive]]]</span></span><br><span class="line"><span class="attr">name</span>=Hive</span><br><span class="line"><span class="attr">interface</span>=hiveserver2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[databases]]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[mysql]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nice_name</span>=<span class="string">&quot;My SQL DB&quot;</span></span><br><span class="line"><span class="attr">engine</span>=mysql</span><br><span class="line"><span class="attr">host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="attr">port</span>=<span class="number">3306</span></span><br><span class="line"><span class="attr">user</span>=root</span><br><span class="line"><span class="attr">password</span>=<span class="number">123456</span></span><br></pre></td></tr></table></figure><h2 id="2-重启Hue"><a href="#2-重启Hue" class="headerlink" title="2.重启Hue"></a>2.重启Hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成Hive</title>
      <link href="/post/7d7d578d.html"/>
      <url>/post/7d7d578d.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成Hive"><a href="#Hue集成Hive" class="headerlink" title="Hue集成Hive"></a>Hue集成Hive</h1><h2 id="1-时间同步"><a href="#1-时间同步" class="headerlink" title="1.时间同步"></a>1.时间同步</h2><p>安装ntp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp</span><br></pre></td></tr></table></figure><p>与上海电信服务器同步时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate api.ntp.bz</span><br></pre></td></tr></table></figure><h2 id="2-修改hive-site-xml"><a href="#2-修改hive-site-xml" class="headerlink" title="2.修改hive-site.xml"></a>2.修改hive-site.xml</h2><p>进入hive的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/conf</span><br></pre></td></tr></table></figure><p>增加hive-site.xml配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hiveserver2运行绑定host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hiveserver2运行绑定端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>11240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 远程模式部署metastore服务地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.0.219:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-启动Hadoop"><a href="#2-启动Hadoop" class="headerlink" title="2.启动Hadoop"></a>2.启动Hadoop</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动任务历史服务器</span></span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><h2 id="3-修改hue-ini"><a href="#3-修改hue-ini" class="headerlink" title="3.修改hue.ini"></a>3.修改hue.ini</h2><p>进入hue的conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue/desktop/conf</span><br></pre></td></tr></table></figure><p><font color=#dd0000>修改hue.ini</font></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[beeswax]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">hive_server_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="comment">#hiveserver2的thrift端口号</span></span><br><span class="line"><span class="attr">hive_server_port</span>=<span class="number">11240</span></span><br><span class="line"><span class="attr">hive_conf_dir</span>=/home/software/apache-hive-<span class="number">3.1</span>.<span class="number">2</span>-bin/conf</span><br><span class="line"></span><br><span class="line"><span class="attr">server_conn_timeout</span>=<span class="number">120</span></span><br><span class="line"></span><br><span class="line"><span class="attr">auth_username</span>=root</span><br><span class="line"><span class="attr">auth_password</span>=<span class="number">123456</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[metastore]</span></span><br><span class="line"><span class="comment">#允许使用hive创建数据库表等操作</span></span><br><span class="line"><span class="attr">enable_new_create_table</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="4-启动hiveserver2和metastore服务"><a href="#4-启动hiveserver2和metastore服务" class="headerlink" title="4.启动hiveserver2和metastore服务"></a>4.启动hiveserver2和metastore服务</h2><p><font color=#dd0000>启动metastore</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="built_in">nohup</span> hive --service metastore &amp;</span></span><br></pre></td></tr></table></figure><p><font color=#dd0000>启动hiveserver2</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">或者</span></span><br><span class="line">nohup hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h2 id="5-启动hue"><a href="#5-启动hue" class="headerlink" title="5.启动hue"></a>5.启动hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Hue并集成Hadoop</title>
      <link href="/post/254593a4.html"/>
      <url>/post/254593a4.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器安装Hue"><a href="#云服务器安装Hue" class="headerlink" title="云服务器安装Hue"></a>云服务器安装Hue</h2><h2 id="1-源码下载"><a href="#1-源码下载" class="headerlink" title="1.源码下载"></a>1.源码下载</h2><p>Hue官网并没有提供二进制安装包，我们需要自行编译。<br>源码文件下载地址<a href="https://github.com/cloudera/hue/releases">https://github.com/cloudera/hue/releases</a></p><h2 id="1-上传解压"><a href="#1-上传解压" class="headerlink" title="1.上传解压"></a>1.上传解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;sourceSoftware&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hue-release-4.7.1.tar.gz  -C /home/software/sourceSoftware/</span><br></pre></td></tr></table></figure><h2 id="2-安装依赖包"><a href="#2-安装依赖包" class="headerlink" title="2.安装依赖包"></a>2.安装依赖包</h2><p>在编译之前必须安装各种依赖软件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel gmp-devel rsync openssl-devel</span><br></pre></td></tr></table></figure><h2 id="3-源码编译"><a href="#3-源码编译" class="headerlink" title="3.源码编译"></a>3.源码编译</h2><p>进入源码文件解压根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/sourceSoftware/hue-release-4.7.1</span><br></pre></td></tr></table></figure><p>执行以下命令进行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PREFIX=/home/software/hue-4.7.1 make install</span><br></pre></td></tr></table></figure><p>编译完成后在&#x2F;home&#x2F;software&#x2F;hue-4.7.1目录下就会有hue软件包</p><h2 id="4-配置环境变量"><a href="#4-配置环境变量" class="headerlink" title="4.配置环境变量"></a>4.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HUE_HOME=/home/software/hue-4.7.1/hue</span><br><span class="line">export PATH=$PATH:/$HUE_HOME/build/env/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="5-修改配置"><a href="#5-修改配置" class="headerlink" title="5.修改配置"></a>5.修改配置</h2><p>在 core-site.xml 中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HUE --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- #设置Hadoop集群的代理用户 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- #设置Hadoop集群的代理用户组 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 hdfs-site.xml 中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- datanode 通信是否使用域名,默认为false，改为true --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether datanodes should use datanode hostnames when</span><br><span class="line">        connecting to other datanodes for data transfer.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在yarn-site.xml中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--打开HDFS上日志记录功能--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--在HDFS上聚合的日志最长保留多少秒。3天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>259200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在mapred-site.xml中增加配置，否则运行mapreduce任务会报错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>增加 httpfs-site.xml 文件，加入配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HUE --&gt;</span>        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="6-配置hue-ini"><a href="#6-配置hue-ini" class="headerlink" title="6.配置hue.ini"></a>6.配置hue.ini</h2><p>进入hue的desktop&#x2F;conf目录，里面有一个pseudo-distributed.ini.tmpl文件，复制一份改名为hue.ini</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/software/hue-4.7.1/hue/desktop/conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp pseudo-distributed.ini.tmpl  hue.ini</span><br></pre></td></tr></table></figure><p>修改hue.ini</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[desktop]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">secret_key</span>=jFE93j<span class="comment">;2[290-eiw.KEiwN2s3[&#x27;d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span></span><br><span class="line"></span><br><span class="line"><span class="attr">http_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="attr">http_port</span>=<span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">time_zone</span>=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="attr">server_user</span>=root</span><br><span class="line"><span class="attr">server_group</span>=root</span><br><span class="line"></span><br><span class="line"><span class="attr">default_user</span>=root</span><br><span class="line"><span class="attr">default_hdfs_superuser</span>=root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[database]]</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">engine</span>=mysql</span><br><span class="line">    <span class="attr">host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line">    <span class="attr">port</span>=<span class="number">3306</span></span><br><span class="line">    <span class="attr">user</span>=root</span><br><span class="line">    <span class="attr">password</span>=<span class="number">123456</span></span><br><span class="line">    <span class="attr">name</span>=hue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[hadoop]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[hdfs_clusters]]</span></span><br><span class="line"><span class="section">[[[default]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fs_defaultfs</span>=hdfs://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">webhdfs_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">9870</span>/webhdfs/v1</span><br><span class="line"></span><br><span class="line"><span class="attr">hadoop_conf_dir</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span>/etc/hadoop</span><br><span class="line"><span class="attr">hadoop_hdfs_home</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span></span><br><span class="line"><span class="attr">hadoop_bin</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span>/bin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[yarn_clusters]]</span></span><br><span class="line"><span class="section">[[[default]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_port</span>=<span class="number">8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">7776</span></span><br><span class="line"><span class="attr">proxy_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">7776</span></span><br><span class="line"><span class="attr">history_server_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">19888</span></span><br></pre></td></tr></table></figure><p>在mysql中创建hue数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database hue <span class="keyword">default</span> <span class="type">character</span> <span class="keyword">set</span> utf8 <span class="keyword">default</span> <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure><p>去hue&#x2F;build&#x2F;env&#x2F;bin目录下，执行以下命令，在mysql的hue数据库里看到对应的hue元数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">新建数据库并初始化</span></span><br><span class="line">cd hue/build/env/bin</span><br><span class="line">./hue syncdb</span><br><span class="line">./hue migrate</span><br></pre></td></tr></table></figure><h2 id="7-启动hue"><a href="#7-启动hue" class="headerlink" title="7.启动hue"></a>7.启动hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>这里必须登录zyw用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su zyw</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure><p>启动之后，在Web浏览器访问8000端口即可进入hue界面。<br><strong>登录用户：root<br>密码：123456</strong></p>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase问题解决</title>
      <link href="/post/7542b90f.html"/>
      <url>/post/7542b90f.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase问题解决"><a href="#Hbase问题解决" class="headerlink" title="Hbase问题解决"></a>Hbase问题解决</h1><p><strong><font color=Red><br>报错：关闭HBase时无法找到Master：no hbase master found<br></font></strong></p><p><font color=OrangeRed size=5><b>原因</b></font></p><p>此时可以大体确定报错原因，系统找不到HBase的pid文件，pid文件里面是HBase的进程号，找不到进程号系统就没有办法去结束这个进程。</p><p>HBase的pid文件默认存放路径为 &#x2F;tmp 路径，可以进去看一下有没有和HBase相关的文件。肯定没有，因为很有可能被操作系统删掉了。</p><p><font color=#0066FF size=5><b>解决方案</b></font></p><p>修改pid文件存放路径，</p><p>进入 hbase 的 conf 目录，找到 hbase-env.sh 进行修改</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_PID_DIR=/home/software/hbase-2.3.6/pids</span><br></pre></td></tr></table></figure><p>之后重新启动即可。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署HBase</title>
      <link href="/post/f2089145.html"/>
      <url>/post/f2089145.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器伪分布式部署HBase"><a href="#云服务器伪分布式部署HBase" class="headerlink" title="云服务器伪分布式部署HBase"></a>云服务器伪分布式部署HBase</h2><h2 id="1-解压tar包"><a href="#1-解压tar包" class="headerlink" title="1.解压tar包"></a>1.解压tar包</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hbase-2.3.6-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="2-配置conf"><a href="#2-配置conf" class="headerlink" title="2.配置conf"></a>2.配置conf</h2><h3 id="2-1-hbase-env-sh"><a href="#2-1-hbase-env-sh" class="headerlink" title="2.1 hbase-env.sh"></a>2.1 hbase-env.sh</h3><p>进入conf目录，在hbase-env.sh文件中添加如下，这里我不用HBase自带的Zookeeper，下面HBASE_MANAGES_ZK设置为fales，使用独立配置的Zookeeper。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><h3 id="2-2-hbase-env-sh"><a href="#2-2-hbase-env-sh" class="headerlink" title="2.2 hbase-env.sh"></a>2.2 hbase-env.sh</h3><p>在hbase-site.xml文件中添加如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.0.219:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!--配置zk本地数据存放目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hbase-2.3.6/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-regionservers"><a href="#2-2-regionservers" class="headerlink" title="2.2 regionservers"></a>2.2 regionservers</h3><p>在regionservers文件中添加服务器ip地址</p><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/home/software/hbase-2.3.6</span><br><span class="line">export PATH=$PATH:/$HBASE_HOME/bin</span><br></pre></td></tr></table></figure><h2 id="3-启动"><a href="#3-启动" class="headerlink" title="3.启动"></a>3.启动</h2><p>注意，在启动HBase之前，先启动Zookeeper，然后启动HBase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>jps进程出现HMaster，HRegionServer则说明启动成功，在浏览器输入IP地址和端口号16010，出现HMaster的Web界面。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Azkaban</title>
      <link href="/post/4995146.html"/>
      <url>/post/4995146.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Azkaban"><a href="#云服务器安装Azkaban" class="headerlink" title="云服务器安装Azkaban"></a>云服务器安装Azkaban</h1><h2 id="1-源码编译"><a href="#1-源码编译" class="headerlink" title="1.源码编译"></a>1.源码编译</h2><p>Azkaban官方并没有提供二进制安装包，需要我们自行编译。<br><a href="https://azkaban.github.io/">源码文件下载</a></p><h3 id="1-1编译环境"><a href="#1-1编译环境" class="headerlink" title="1.1编译环境"></a>1.1编译环境</h3><p>编译之前需要提前安装好Maven，Ant，Node等软件，还有git和gcc-c++环境。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum  install -y git</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum  install -y gcc-c++</span><br></pre></td></tr></table></figure><h3 id="1-1下载源码解压"><a href="#1-1下载源码解压" class="headerlink" title="1.1下载源码解压"></a>1.1下载源码解压</h3><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-4.0.0.tar.gz -C /home/software/</span><br></pre></td></tr></table></figure><h3 id="1-2源码文件编辑"><a href="#1-2源码文件编辑" class="headerlink" title="1.2源码文件编辑"></a>1.2源码文件编辑</h3><p>在编译之前对文件进行修改，避免编译过程出现问题。</p><h3 id="1-2-1修改build-gradle文件"><a href="#1-2-1修改build-gradle文件" class="headerlink" title="1.2.1修改build.gradle文件"></a>1.2.1修改build.gradle文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim build.gradle</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">repositories</span> &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/central&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/gradle-plugin&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/public&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/google&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/nexus/content/groups/public/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://plugins.gradle.org/m2/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>具体参考<a href="https://blog.csdn.net/chenxi5404/article/details/120512109">https://blog.csdn.net/chenxi5404/article/details/120512109</a></p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">allprojects</span> &#123;</span><br><span class="line">  apply plugin: <span class="string">&#x27;jacoco&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">repositories</span> &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    mavenLocal()</span><br><span class="line"><span class="comment">//  need this for rest.li/pegasus 28.* artifacts until they are in Maven Central:</span></span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://linkedin.jfrog.io/artifactory/open-source/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体参考<a href="https://blog.csdn.net/NKDark0214/article/details/122601181">https://blog.csdn.net/NKDark0214/article/details/122601181</a></p><h3 id="1-2-2修改gradle-wrapper-properties文件"><a href="#1-2-2修改gradle-wrapper-properties文件" class="headerlink" title="1.2.2修改gradle-wrapper.properties文件"></a>1.2.2修改gradle-wrapper.properties文件</h3><p>Azkaban使用的是gradle进行构建的，我这里直接下载一个gradle安装包，使用的版本为4.6的。<br>把下载好的安装包上传到服务器上并移动到Azkaban的源码目录中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp gradle-4.6-bin.zip /home/software/azkaban-4.0.0/gradle/wrapper</span><br></pre></td></tr></table></figure><p>修改这个目录的gradle-wrapper.properties配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim gradle-wrapper.properties</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">distributionUrl</span>=<span class="string">gradle-4.6-bin.zip</span></span><br></pre></td></tr></table></figure><h3 id="1-3编译"><a href="#1-3编译" class="headerlink" title="1.3编译"></a>1.3编译</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure><p>出现BUILD SUCCESSFUL表示编译成功。<br>然后在<br>azkaban-exec-server&#x2F;build&#x2F;distributions&#x2F;<br>azkaban-web-server&#x2F;build&#x2F;distributions&#x2F;<br>azkaban-db&#x2F;build&#x2F;distributions&#x2F;<br>这些目录下会有相关安装包。</p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2.解压安装包"></a>2.解压安装包</h2><p>创建azkaban安装目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/software/azkaban</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-db-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br><span class="line">tar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br><span class="line">tar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br></pre></td></tr></table></figure><h2 id="3-MySQL初始化"><a href="#3-MySQL初始化" class="headerlink" title="3.MySQL初始化"></a>3.MySQL初始化</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-db-0.1.0-SNAPSHOT</span><br></pre></td></tr></table></figure><p>创建azkaban数据库，并加载初始化sql脚本。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database azkaban;</span><br><span class="line"></span><br><span class="line">use azkaban;</span><br><span class="line"></span><br><span class="line">source <span class="operator">/</span>home<span class="operator">/</span>software<span class="operator">/</span>azkaban<span class="operator">/</span>azkaban<span class="operator">-</span>db<span class="number">-0.1</span><span class="number">.0</span><span class="operator">-</span>SNAPSHOT<span class="operator">/</span><span class="keyword">create</span><span class="operator">-</span><span class="keyword">all</span><span class="operator">-</span><span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span><span class="operator">-</span>SNAPSHOT.sql</span><br></pre></td></tr></table></figure><h2 id="4-web-server服务器配置"><a href="#4-web-server服务器配置" class="headerlink" title="4.web-server服务器配置"></a>4.web-server服务器配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban</span><br></pre></td></tr></table></figure><h3 id="4-1生成ssl证书"><a href="#4-1生成ssl证书" class="headerlink" title="4.1生成ssl证书"></a>4.1生成ssl证书</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore keystore -alias jetty -genkey -keyalg RSA</span><br></pre></td></tr></table></figure><p>运行此命令后，会提示输入当前生成keystore的密码和相关信息，输入的密码请记住（所以密码均为123456）。</p><p>然后将证书复制到web-server服务器根目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp keystore azkaban-web-server-0.1.0-SNAPSHOT/</span><br></pre></td></tr></table></figure><h3 id="4-2配置azkaban-properties"><a href="#4-2配置azkaban-properties" class="headerlink" title="4.2配置azkaban.properties"></a>4.2配置azkaban.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-web-server-0.1.0-SNAPSHOT/conf</span><br></pre></td></tr></table></figure><p>修改如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jetty.use.ssl</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">jetty.port</span>=<span class="string">8443</span></span><br><span class="line"></span><br><span class="line"><span class="attr">executor.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">executor.port</span>=<span class="string">12321</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jetty.keystore</span>=<span class="string">keystore</span></span><br><span class="line"><span class="attr">jetty.password</span>=<span class="string">123456</span></span><br><span class="line"><span class="attr">jetty.keypassword</span>=<span class="string">123456</span></span><br><span class="line"><span class="attr">jetty.truststore</span>=<span class="string">keystore</span></span><br><span class="line"><span class="attr">jetty.trustpassword</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">mysql.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="attr">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">mysql.password</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">azkaban.use.multiple.executors</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#注释</span></span><br><span class="line"><span class="comment">#azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span></span><br></pre></td></tr></table></figure><h3 id="4-3配置commonprivate-properties"><a href="#4-3配置commonprivate-properties" class="headerlink" title="4.3配置commonprivate.properties"></a>4.3配置commonprivate.properties</h3><p>新建文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p plugins/jobtypes</span><br></pre></td></tr></table></figure><p>新建commonprivate.properties文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim commonprivate.properties</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">azkaban.native.lib</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">execute.as.user</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">memCheck.enabled</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure><h2 id="4-exec-server服务器配置"><a href="#4-exec-server服务器配置" class="headerlink" title="4.exec-server服务器配置"></a>4.exec-server服务器配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /home/software/azkaban/azkaban-exec-server-0.1.0-SNAPSHOT/conf</span><br></pre></td></tr></table></figure><h3 id="4-1配置azkaban-properties"><a href="#4-1配置azkaban-properties" class="headerlink" title="4.1配置azkaban.properties"></a>4.1配置azkaban.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim azkaban.properties</span><br></pre></td></tr></table></figure><p>修改如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"></span><br><span class="line"><span class="attr">azkaban.webserver.url</span>=<span class="string">https://192.168.0.219:8443</span></span><br><span class="line"></span><br><span class="line"><span class="attr">mysql.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="attr">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">mysql.password</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">executor.port</span>=<span class="string">12321</span></span><br></pre></td></tr></table></figure><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>先启动exec-server<br>再启动web-server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-exec.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-web.sh</span><br></pre></td></tr></table></figure><p><strong>注意，必须在安装包根目录下执行</strong><br>启动web-server后进程失败，可以通过安装包下对应启动日志进行排查</p><p>访问<a href="https://114.116.24.98:8443/">https://114.116.24.98:8443</a><br>进入AzkabanWeb页面<br><strong>Username：azkaban<br>password：azkaban</strong></p><h2 id="6-exec-server激活问题"><a href="#6-exec-server激活问题" class="headerlink" title="6.exec-server激活问题"></a>6.exec-server激活问题</h2><p><strong>每次关闭exec-server后都要重新激活execute</strong></p><p>进入exec-server根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-exec-server-0.1.0-SNAPSHOT</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;192.168.0.219:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure><p>然后再重新启动web-server</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Hive</title>
      <link href="/post/b607d502.html"/>
      <url>/post/b607d502.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Hive"><a href="#云服务器安装Hive" class="headerlink" title="云服务器安装Hive"></a>云服务器安装Hive</h1><h2 id="1-下载Hive-下载地址"><a href="#1-下载Hive-下载地址" class="headerlink" title="1.下载Hive(下载地址)"></a>1.下载Hive(<a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/">下载地址</a>)</h2><h2 id="2-上传至服务器并解压"><a href="#2-上传至服务器并解压" class="headerlink" title="2.上传至服务器并解压"></a>2.上传至服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-3.1.2-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下"><a href="#3-添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下" class="headerlink" title="3.添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下"></a>3.添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-5.1.46-bin.jar /home/software/apache-hive-3.1.2-bin/lib/</span><br></pre></td></tr></table></figure><h2 id="4-添加环境变量"><a href="#4-添加环境变量" class="headerlink" title="4.添加环境变量"></a>4.添加环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/home/software/apache-hive-3.1.2-bin</span><br><span class="line">export PATH=$PATH:/$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>使环境变量立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h2><h3 id="4-1-配置hive-env-sh文件"><a href="#4-1-配置hive-env-sh文件" class="headerlink" title="4.1 配置hive-env.sh文件"></a>4.1 配置hive-env.sh文件</h3><p>进入hive下的conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/conf/</span><br></pre></td></tr></table></figure><p>拷贝hive-env.sh.template文件，并命名为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line"></span><br><span class="line">export HIVE_CONF_DIR=/home/software/apache-hive-3.1.2-bin/conf</span><br><span class="line"></span><br><span class="line">export HIVE_AUX_JARS_PATH=/home/software/apache-hive-3.1.2-bin/lib</span><br></pre></td></tr></table></figure><p>第一个为Hadoop目录，第二个为Hive配置目录，最后一个为驱动jar包路径</p><h3 id="4-2-配置hive-site-xml文件"><a href="#4-2-配置hive-site-xml文件" class="headerlink" title="4.2 配置hive-site.xml文件"></a>4.2 配置hive-site.xml文件</h3><p>新建hive-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 本地模式 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 URL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.0.219:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!-- jdbc 连接的 Driver--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--新版本8.0版本的驱动为com.mysql.cj.jdbc.Driver--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--旧版本5.x版本的驱动为com.mysql.jdbc.Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 username(MySQL用户名)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 password(MySQL密码) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!-- Hive 元数据存储版本的验证(Hive元数据默认是存储在Derby中，正常开启时它会去校验Derby，现在要使用MySQL存储元数据，</span></span><br><span class="line"><span class="comment">               就需要把这个关闭即可，如果开启，MySQL和Derby会导致Hive启动不起来的) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive  默认在 HDFS 的工作目录(可以不配置，因为默认就是/user/hive/warehouse，如果不使用默认的位置，可以进行手动修改) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hiveserver2运行绑定host --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 自定义hiveserver2运行绑定端口，默认10000 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>11240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 远程模式部署metastore 服务地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.0.219:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 关闭元数据存储授权  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在HDFS中创建hive工作目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/hive/warehouse</span><br></pre></td></tr></table></figure><h2 id="5-配置log日志文件"><a href="#5-配置log日志文件" class="headerlink" title="5.配置log日志文件"></a>5.配置log日志文件</h2><p>拷贝并重命名hive-log4j2.properties.template为 hive-log4j2.properties文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-log4j2.properties.template  hive-log4j2.properties</span><br></pre></td></tr></table></figure><p>修改内容 property.hive.log.dir 这个属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">property.hive.log.dir = /home/software/apache-hive-3.1.2-bin/temp</span><br></pre></td></tr></table></figure><p>并在hive目录下创建temp文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/software/apache-hive-3.1.2-bin/temp</span><br></pre></td></tr></table></figure><h2 id="6-初始化元数据库"><a href="#6-初始化元数据库" class="headerlink" title="6.初始化元数据库"></a>6.初始化元数据库</h2><p>进入hive安装目录下的bin目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/bin</span><br></pre></td></tr></table></figure><p>初始化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><p>可能出现问题，<a href="https://blog.csdn.net/m0_59705760/article/details/125116629">解决方案</a></p><p>出现Initialization script completed<br>schemaTool completed则说明初始化成功</p><h2 id="7-启动Hive"><a href="#7-启动Hive" class="headerlink" title="7.启动Hive"></a>7.启动Hive</h2><h3 id="7-1-本地模式启动"><a href="#7-1-本地模式启动" class="headerlink" title="7.1 本地模式启动"></a>7.1 本地模式启动</h3><p>本地模式特点是：<strong>需要安装MySQL来存储元数据，但是不需要启动metastore服务</strong><br>直接使用hive命令在本地启动客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><h3 id="7-2-远程模式启动"><a href="#7-2-远程模式启动" class="headerlink" title="7.2 远程模式启动"></a>7.2 远程模式启动</h3><p>远程模式特点是：<strong>需要安装MySQL来存储元数据，需要单独启动metastore服务</strong></p><p><em>nohup 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。<br>nohup 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME&#x2F;nohup.out 文件中。<br>参数说明：&amp;：让命令在后台执行，终端退出后命令仍旧执行。</em></p><p>这里使用nohup命令将将HiveServer2远程连接进程在后台运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p>HiveServer2通过metastore服务读写元数据，所以远程模式下，启动HiveHiveServer2之前必须启动metastore服务<br>先开启metastore服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>再启动HiveServer2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h2 id="8-Hive客户端"><a href="#8-Hive客户端" class="headerlink" title="8.Hive客户端"></a>8.Hive客户端</h2><p>第一代客户端（不推荐使用）：$HIVE_HOME&#x2F;bin&#x2F;hive，是一个shellUtil，主要功能：一是可以用于交互或批处理模式运行Hive查询；二是用于Hive相关服务的启动，比如metastore服务。</p><p>第二代客户端（推荐使用）：$HIVE_HOME&#x2F;bin&#x2F;beeline，是一个JDBC客户端，是官方强烈推荐使用的Hive命令行工具，和第一代客户端相比，性能加强安全性提高。<br><strong>在远程模式下，beeline需要通过Thrift连接到单独的HiveServer2服务上，所以在使用前需要启动metastore服务和HiveServer2服务。</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://192.168.0.219:11240</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2://192.168.0.219:11240</span><br><span class="line">Enter username for jdbc:hive2://192.168.0.219:11240: root</span><br><span class="line">Enter password for jdbc:hive2://192.168.0.219:11240: 123456</span><br></pre></td></tr></table></figure><p>或者一步到位：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://192.168.0.219:11240 -n root</span><br></pre></td></tr></table></figure><p>设置mapreduce本地运行</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> mapreduce.framework.name=<span class="built_in">local</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7---ZooKeeper伪分布式安装部署</title>
      <link href="/post/9c657266.html"/>
      <url>/post/9c657266.html</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS7—ZooKeeper伪分布式安装部署"><a href="#CentOS7—ZooKeeper伪分布式安装部署" class="headerlink" title="CentOS7—ZooKeeper伪分布式安装部署"></a>CentOS7—ZooKeeper伪分布式安装部署</h1><h2 id="1-下载ZooKeeper安装包：下载地址"><a href="#1-下载ZooKeeper安装包：下载地址" class="headerlink" title="1. 下载ZooKeeper安装包：下载地址"></a>1. 下载ZooKeeper安装包：<a href="https://www.apache.org/dyn/closer.cgi/zookeeper">下载地址</a></h2><p>注意，随着版本的更新，3.5版本以后的压缩包分成了两种<br>带bin的压缩包是真正的标准压缩包<br>而不带bin的压缩包是源码压缩包<br>我们需要使用文件名带有bin 的那个压缩包，例如：apache-zookeeper-3.6.3-bin.tar.gz 这样解压后才会有lib目录下的那些jar包。</p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2. 解压安装包"></a>2. 解压安装包</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-zookeeper-3.6.3-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-复制oo-example-cfg文件并改名"><a href="#3-复制oo-example-cfg文件并改名" class="headerlink" title="3. 复制oo_example.cfg文件并改名"></a>3. 复制oo_example.cfg文件并改名</h2><p>进入解压后的路径zookeeper-3.4.10&#x2F;conf路径下，复制zoo_example.cfg配置文件，因为是伪分布式安装，一台机器上有3个zookeeper进程，所以这里复制3份。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp zoo_example.cfg zoo1.cfg</span><br><span class="line">cp zoo_example.cfg zoo2.cfg</span><br><span class="line">cp zoo_example.cfg zoo3.cfg</span><br></pre></td></tr></table></figure><h2 id="4-分别输入如下配置"><a href="#4-分别输入如下配置" class="headerlink" title="4.分别输入如下配置"></a>4.分别输入如下配置</h2><h2 id="zoo1-cfg"><a href="#zoo1-cfg" class="headerlink" title="zoo1.cfg"></a>zoo1.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir1</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><h2 id="zoo2-cfg"><a href="#zoo2-cfg" class="headerlink" title="zoo2.cfg"></a>zoo2.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir2</span><br><span class="line">clientPort=2182</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><h2 id="zoo3-cfg"><a href="#zoo3-cfg" class="headerlink" title="zoo3.cfg"></a>zoo3.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir3</span><br><span class="line">clientPort=2183</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><p>tickTime - 心跳的时间ms<br>initLimit - 初始化限制时间<br>syncLimit - 同步限制时间<br>dataDir - 数据存放物理路径<br>clientPort - 供客户端连接的端口<br>server.n&#x3D;host:mainPort:selectPort – 表示集群的配置，其中n表示集群中的n个节点，host表示集群对应的ip，mainPort – 表示该节点作为主节点对应的端口，selectPort – 表示集群主节点选举时彼此通信的端口</p><h2 id="5-新建对应的dataDir文件夹和myid文件"><a href="#5-新建对应的dataDir文件夹和myid文件" class="headerlink" title="5.新建对应的dataDir文件夹和myid文件"></a>5.新建对应的dataDir文件夹和myid文件</h2><p>在ZooKeeper根目录下，新建dataDir文件夹，分别是zoo1，zoo2，zoo3的dataDir路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir dataDir1</span><br><span class="line">mkdir dataDir2</span><br><span class="line">mkdir dataDir3</span><br></pre></td></tr></table></figure><p>然后分别在每个dataDir中创建myid文件并写入zoo1，zoo2，zoo3的id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt;&gt;  dataDir1/myid</span><br><span class="line">echo 2 &gt;&gt;  dataDir2/myid</span><br><span class="line">echo 3 &gt;&gt;  dataDir3/myid</span><br></pre></td></tr></table></figure><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6. 启动"></a>6. 启动</h2><p>进入zookeeper的bin目录下，通过脚本文件zkServer.sh启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh start ../conf/zoo1.cfg</span><br><span class="line">./zkServer.sh start ../conf/zoo2.cfg</span><br><span class="line">./zkServer.sh start ../conf/zoo3.cfg</span><br></pre></td></tr></table></figure><p>查看每个zoo的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh status ../conf/zoo1.cfg</span><br><span class="line">./zkServer.sh status ../conf/zoo2.cfg</span><br><span class="line">./zkServer.sh status ../conf/zoo3.cfg</span><br></pre></td></tr></table></figure><p>通过集群暴露的客户端连接接口，可以直接访问集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> ZooKeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10. Kafka 关键原理加强</title>
      <link href="/post/216d6a00.html"/>
      <url>/post/216d6a00.html</url>
      
        <content type="html"><![CDATA[<h1 id="10-Kafka-关键原理加强"><a href="#10-Kafka-关键原理加强" class="headerlink" title="10. Kafka 关键原理加强"></a>10. Kafka 关键原理加强</h1><h2 id="10-1-日志分段切分条件"><a href="#10-1-日志分段切分条件" class="headerlink" title="10.1 日志分段切分条件"></a>10.1 日志分段切分条件</h2><p><font size=3><b>日志分段文件切分包含以下4个条件，满足其一即可：</b></font></p><p>(1) <font size=3><b>当前日志分段文件的大小超过了broker端参数<u>log.segment.bytes</u>配置的值。<br>log.segment.bytes参数的默认值为1073741824，即1GB</b></font><br>(2) <font size=3><b>当前日志分段中消息的最小时间戳与当前系统的时间戳的差值大于<u>log.roll.ms</u>或++log.roll.hours++参数配置的值。如果同时配置了log.roll.ms和log.roll.hours参数，那么log.roll.ms的优先级高。默认情况下，只配置了log.roll.hours参数，其值为168，即7天。</b></font><br>(3) <font size=3><b>偏移量索引文件或时间戳索引文件的大小达到broker端参数<u>log.index.size.max.bytes</u>配置的值。log.index.size.max.bytes的默认值为10485760，即10MB</b></font><br>(4) <font size=3><b>追加的消息的偏移量与当前日志分段的起始偏移量之间的差值大于Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量(offset - baseOffset &gt; Integer.MAX_VALUE)。<br></b></font></p><h2 id="10-2-controller控制器"><a href="#10-2-controller控制器" class="headerlink" title="10.2 controller控制器"></a>10.2 controller控制器</h2><p><font size=3><b><u>Controller简单来说，就是kafka集群的状态管理者</u> <br>在kafka集群中会有一个或者多个broker，<u>其中有一个broker会被选举为控制器（Kafka Controller），</u><br><u>它负责维护整个集群中所有分区和副本的状态及分区leader的选举</u>。当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。当使用kafka-topic.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重新分配。</b></font>   </p><p><font size=3><b>Kafka中的控制器选举的工作依赖于Zookeeper，成功竞选为控制器的broker会在Zookeeper中创建&#x2F;controller这个临时(EPHEMERAL)节点，此临时节点的内容参考如下: <br>{“version”:1,”broker”:0,”timestamp”:”1529210278988”}<br>其中version在目前版本中固定为1，brokerid表示成为控制器的broker的id编号，timestamp表示竞选成为控制器时的时间戳。 <br><u>在任意时刻，集群中有且仅有一个控制器。</u>每个broker启动的时候会去尝试读取zookeeper上的&#x2F;controller节点的brokerid的值，如果读取到brokerid的值不为-1，则表示已经有其它broker节点成功竞选为控制器，所有当前就会放弃竞选；如果zookeeper不存在&#x2F;controller这个节点，或者这个节点中的数据异常，那么就会尝试去创建&#x2F;controller这个节点，当前broker去创建节点的时候，也有可能其它broker同时去尝试创建这个节点，只有创建成功的那个broker才会成为控制器，而创建失败的broker则表示竞选失败。每个broker都会在内存中保存当前控制器的brokerid值，这个值可以标识为activeControllerId。</b></font></p><p><font size=3><b><u>controller竞选机制，简单说，先来先上</u><br></b></font></p><p>具备控制器身份的broker需要比其它普通的broker多一些职责，具体细节如下：</p><ul><li><p>监听partition相关变化<br>  对Zookeeper中的&#x2F;admin&#x2F;reassign_partitions节点注册PartitionReassignmentListener，用来处理分区重分配的动作。<br>  对Zookeeper中的&#x2F;isr_change_notification节点注册IsrChangeNotificationListener，用来处理ISR集合变更的动作。<br>  对Zookeeper中的&#x2F;admin&#x2F;preferred-replica-election节点添加PreferredReplicaElectionListener，用来处理优先副本选举。</p></li><li><p>监听topic增减变化<br>  对Zookeeper中的&#x2F;brokers&#x2F;topics节点添加TopicChangeListener，用来处理topic增减的变化。<br>  对Zookeeper中的&#x2F;admin&#x2F;delete_topics节点添加TopicDeletionListener，用来处理删除topic的动作。</p></li><li><p>监听broker相关的变化<br>  对Zookeeper中的&#x2F;brokers&#x2F;ids&#x2F;节点添加BrokerChangeListener，用来处理broker增减的变化。</p></li><li><p>更新集群的元数据信息<br>  从Zookeeper中读取获取当前所有与topic、partition以及broker有关的信息并进行相应的管理，对各topic所对应的Zookeeper中的&#x2F;brokers&#x2F;topics&#x2F;[topic]节点添加PartitionModificationsListener，用来监听topic中的分区分配变化，并将最新信息同步给其它所有broker。</p></li><li><p>启动并管理分区状态机和副本状态机</p></li><li><p>如果参数auto.leader.rebalance.enable设置为true，则还会开启一个名为“auto-leader-rebalance-task”的定时任务来维护分区的leader副本的均衡</p></li></ul><h2 id="10-3-分区的负载分布"><a href="#10-3-分区的负载分布" class="headerlink" title="10.3 分区的负载分布"></a>10.3 分区的负载分布</h2><p><font size=3><b>客户端请求创建一个topic时，每一个分期副本在broker上的分配，是由集群controller来决定；<br>其分布策略源码如下：<br></b></font></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokersRackUnaware</span></span>(nPartitions: <span class="type">Int</span>,</span><br><span class="line">                                                 replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                                                 brokerList: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">                                                 fixedStartIndex: <span class="type">Int</span>,</span><br><span class="line">                                                 startPartitionId: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]]()</span><br><span class="line">    <span class="keyword">val</span> brokerArray = brokerList.toArray</span><br><span class="line">    <span class="keyword">val</span> startIndex = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">var</span> currentPartitionId = math.max(<span class="number">0</span>, startPartitionId)</span><br><span class="line">    <span class="keyword">var</span> nextReplicaShift = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nPartitions) &#123;</span><br><span class="line">      <span class="keyword">if</span> (currentPartitionId &gt; <span class="number">0</span> &amp;&amp; (currentPartitionId % brokerArray.length == <span class="number">0</span>))</span><br><span class="line">        nextReplicaShift += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length</span><br><span class="line">      <span class="keyword">val</span> replicaBuffer = mutable.<span class="type">ArrayBuffer</span>(brokerArray(firstReplicaIndex))</span><br><span class="line">      <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until replicationFactor - <span class="number">1</span>)</span><br><span class="line">        replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))</span><br><span class="line">      ret.put(currentPartitionId, replicaBuffer)</span><br><span class="line">      currentPartitionId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex : <span class="type">Int</span>, secondReplicaShift : <span class="type">Int</span>, replicaIndex : <span class="type">Int</span>, nBrokers : <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)</span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>副本因子不能大于Broker的个数；</li><li>partition_0的第1个副本(leader副本)放置位置是随机从brokerList选择的；</li><li>其它分区的第1个副本放置位置相对与partition_0分区依次往后移（也就是说如果我们有5个Broker，5个分区，假设partition0分区放在broker4上，那么partition1将会放在broker5上；partition2将会放在broker1上，partition3在broker2，依此类推）</li><li>各分区剩余的副本相对于分区前一个副本偏移随机数nextReplicaShift</li></ul><h2 id="10-4-分区Leader的选举机制"><a href="#10-4-分区Leader的选举机制" class="headerlink" title="10.4 分区Leader的选举机制"></a>10.4 分区Leader的选举机制</h2><p><u>分区leader副本的选举由控制器controller负责具体实施。</u><br>当创建分区(创建主题或增加分区都有创建分区的动作)或Leader下线(此时分区需要选举一个新的leader上线来对外提供服务)的时候都需要执行leader的选举动作。</p><p><u>选举策略：按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中；</u><br>一个分区的AR集合在partition分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变；</p><h2 id="10-5-分区数与吞吐量"><a href="#10-5-分区数与吞吐量" class="headerlink" title="10.5 分区数与吞吐量"></a>10.5 分区数与吞吐量</h2><p>Kafka本身提供用于生产者性能测试的 kafka-producer-perf-test.sh 和用于消费者性能测试的 kafka-consumer-perf-test.sh，主要参数如下：</p><ul><li>topic用来指定生产者发送消息的目标主题；</li><li>num-records用来指定发送消息的总条数</li><li>record-size用来设置每条消息的字节数；</li><li>producer-props参数用来指定生产者的配置，可同时指定多组配置，各组配置之间以空格分隔与producer-props参数对应的还有一个producer-config参数，它用来指定生产者的配置文件</li><li>throughput用来进行限流控制，当设定的值小于0时不限流，当设定的值大于0时，当发送的吞吐量大于该值时就会被阻塞一段时间。</li></ul><h2 id="10-6-生产者原理解析"><a href="#10-6-生产者原理解析" class="headerlink" title="10.6 生产者原理解析"></a>10.6 生产者原理解析</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114154.jpeg" alt="Kafka图9"></p><p>一个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender线程。<br>在主线程中由kafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存<br>到消息累加器(RecordAccumulator，也称为消息收集器)中。</p><p>Sender线程负责从RecordAccumulator获取消息并将其发送到Kafka中；</p><p>RecordAccumulator主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。RecordAccumulator缓存的大小可以通过生产者客户端参数++buffer.memory++配置，默认值为33554432B，即32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer.send()方法调用要么被阻塞，要么抛出异常，这个取决于参数++max.block.ms++的配置，此参数的默认值为60000，即60秒。</p><p>主线程中发送过来的消息都会被追加到RecordAccumulator的某个双端队列(Deque)中，++RecordAccumulator内部为每个分区都维护了一个双端队列++，即Deque&lt;ProducerBatch&gt;。<br>消息写入缓存时，追加到双端队列的尾部；</p><p>Sender读取消息时，从双端队列的头部读取。注意：ProducerBatch是指一个消息批次；<br>与此同时，会将较小的ProducerBatch凑成一个较大ProducerBatch，也可以减少网络请求的次数以<br>提升整体的吞吐量。</p><p>ProducerBatch大小和++batch.size++参数也有着密切的关系。当一条消息(ProducerRecord)流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端<br>队列的尾部获取一个ProducerBatch(如果没有则新建)，查看ProducerBatch中是否还可以写入这个ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的Producer Batch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数大小，如果不超过，那么就以batch.size参数的大小来创建ProducerBatch。</p><p>如果生产者客户端需要向很多分区发送消息，则可以将buffer.memory参数适当调大以增加整体的吞吐量。</p><p>Sender从RecordAccumulator获取缓存的消息之后，会进一步将&lt;分区，Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node，List&lt;ProducerBatch&gt;&gt;的形式，其中Node表示Kafka集群broker节点。对于网络连接来说，生产者客户端是与具体broker节点建立的连接，也就是向具体的broker节点发送消息，而并不关心消息属于哪一个分区；而对于KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I&#x2F;O层面的转换。<br>在转换成&lt;Node，List&lt;ProducerBatch&gt;&gt;的形式之后，Sender会进一步封装成&lt;Node，Request&gt;的形式，这样就可以将Request请求发往各个Node了，这里的Request是Kafka各种协议请求；</p><p>请求在从sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为Map&lt;Nodeld，Deque&lt;request&gt;&gt;，++它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld是一个String类型，表示节点的id编号)++。与此同时，InFlightRequests还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与Node之间的连接)最多缓存的请求数。这个配置参数为++max.in.flight.request.per.connection++，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response)。++通过比较Deque&lt;Request&gt;的size与这个参数的大小来判断对应的Node中是否己经堆积了很多未响应的消息，如果真是如此，那么说明这个Node节点负载较大或网络连接有问题++，再继续发送请求会增大请求超时的可能。</p><h2 id="10-7-重要的生产者参数"><a href="#10-7-重要的生产者参数" class="headerlink" title="10.7 重要的生产者参数"></a>10.7 重要的生产者参数</h2><h3 id="10-7-1-acks"><a href="#10-7-1-acks" class="headerlink" title="10.7.1 acks"></a>10.7.1 acks</h3><p><font size=3><b>acks是控制生产者在发送出消息后如何得到确认；<br>生产者根据得到的确认信息，来判断消息发送是否成功；<br></b></font></p><table><thead><tr><th>acks</th><th>含义</th></tr></thead><tbody><tr><td>0</td><td>Producer往集群发送数据不需要等到集群的确认信息，不确保消息发送成功。安全性最低但是效率最高。</td></tr><tr><td>1</td><td>Producer往集群发送数据只要leader成功写入消息就可以发送下一条，只确保Leader接收成功。</td></tr><tr><td>-1或all</td><td>Producer往集群发送数据需要所有的ISR Follower都完成从Leader的同步才会发送下一条，确保Leader发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</td></tr></tbody></table><p>生产者将acks设置为all，是否就一定不会丢数据呢？<br>否！如果在某个时刻ISR列表只剩leader自己了，那么就算acks&#x3D;all，收到这条数据还是只有一个节点；</p><p>可以配合另外一个参数缓解此情况：最小同步副本数 &gt;&#x3D; 2<br>Broker端参数：min.insync.replicas（默认1）</p><h3 id="10-7-2-max-request-size"><a href="#10-7-2-max-request-size" class="headerlink" title="10.7.2 max.request.size"></a>10.7.2 max.request.size</h3><p><font size=3><b>这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即1MB<br>一般情况下，这个默认值就可以满足大多数的应用场景了。<br>这个参数还涉及一些其它参数的联动，比如broker端（topic级别参数）的++message.max.bytes++参数<br>(默认1000012)，如果配置错误可能会引起一些不必要的异常：比如将broker端的<br>message.max.bytes参数配置为10，而max.request.size参数配置为20，那么当发送一条大小为15B<br>的消息时，生产者客户端就会报出异常；<br></b></font></p><h3 id="10-7-3-retries-和-retry-backoff-ms"><a href="#10-7-3-retries-和-retry-backoff-ms" class="headerlink" title="10.7.3 retries 和 retry.backoff.ms"></a>10.7.3 retries 和 retry.backoff.ms</h3><p><u>retries参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作</u>。<br>消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、leader副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置retries大于0的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。<u>如果重试达到设定的次数，那么生产者就会放弃重试并返回异</u>常。</p><p>重试还和另一个参数<u>retry.backoff.ms</u>有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。</p><p>Kafka可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。对于某些应用来说，顺序性非常重要，比如MySQL binlog的传输，如果出现错误就会造成非常严重的后果；</p><p>如果将retries参数配置为非零值，并且<u>max.in.flight.requests.per.connection</u>参数配置为大于1的值，那可能会出现错序的现象：如果批次1消息写入失败，而批次2消息写入成功，那么生产者会重试发送批次1的消息，此时如果批次1的消息写入成功，那么这两个批次的消息就出现了错序。</p><p>一般而言，在需要保证消息顺序的场合建议把参数<u>max.in.flight.requests.per.connection</u>配置为1，而不是把retries配置为0，不过这样也会影响整体的吞吐。</p><h3 id="10-7-4-compression-type"><a href="#10-7-4-compression-type" class="headerlink" title="10.7.4 compression.type"></a>10.7.4 compression.type</h3><p><font size=3><b>这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。<br>该参数还可以配置为”gzip”，”snappy”和”lz4”。<br>对消息进行压缩可以极大地减少网络传输、降低网络I&#x2F;O，从而提高整体的性能。<br>消息压缩是一种以时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩；<br></b></font></p><h3 id="10-7-5-batch-size"><a href="#10-7-5-batch-size" class="headerlink" title="10.7.5 batch.size"></a>10.7.5 batch.size</h3><p><font size=3><b>每个Batch要存放batch.size大小的数据后，才可以发送出去。比如说++batch.size默认值是16KB++，那么里面凑够16KB的数据才会发送。<br>理论上来说，提升batch.size的大小，可以允许更多的数据缓冲在recordAccumulator里面，那么一次Request发送出去的数据量就更多了，这样吞吐量可能会有所提升。<br>但是batch.size也不能过大，要是数据老是缓冲在Batch里迟迟不发送出去，那么发送消息的延迟就<br>会很高。<br>一般可以尝试把这个参数调节大些，利用生产环境发消息负载测试一下。<br></b></font></p><h3 id="10-7-6-linger-ms"><a href="#10-7-6-linger-ms" class="headerlink" title="10.7.6 linger.ms"></a>10.7.6 linger.ms</h3><p><font size=3><b>这个参数用来指定生产者发送ProducerBatch之前等待更多消息(ProducerRecord)加入<br>ProducerBatch时间，默认值为0。<br>生产者客户端会在ProducerBatch填满或等待时间超过linger.ms值时发送出去。<br>增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。<br></b></font></p><h3 id="10-7-7-enable-idempotence"><a href="#10-7-7-enable-idempotence" class="headerlink" title="10.7.7 enable.idempotence"></a>10.7.7 enable.idempotence</h3><p>是否开启幂等性功能，详见后续原理加强；<br><u>幂等性，就是一个操作重复做，也不会影响最终的结果！</u><br>int a &#x3D; 1;<br>a++;  &#x2F;&#x2F;非幂等操作<br>val map &#x3D; new HashMap()<br>map.put(a,l); &#x2F;&#x2F;幂等操作</p><p>在kafka中，同一条消息，生产者如果多次重试发送，在服务器中的结果如果还是只有一条，这就是<br>具备幂等性；否则，就不具备幂等性！</p><h3 id="10-7-8-partitioner-class"><a href="#10-7-8-partitioner-class" class="headerlink" title="10.7.8 partitioner.class"></a>10.7.8 partitioner.class</h3><p>用来指定分区器，默认：org.apache.kafka.internals.DefaultPartitioner<br>默认分区器的分区规则：</p><ul><li>如果数据中有key，则按key的murmur hash值 % topie分区总数得到目标分区</li><li>如果数据只有value，则在各个分区间轮询</li></ul><p>自定义partitioner需要实现org.apache.kafka.clients.producer.Partitioner接口</p><h2 id="10-8-消费者组再均衡分区分配策略"><a href="#10-8-消费者组再均衡分区分配策略" class="headerlink" title="10.8 消费者组再均衡分区分配策略"></a>10.8 消费者组再均衡分区分配策略</h2><p>消费者组的意义何在？为了提高数据处理的并行度！</p><p>当以下事件发生时，kafka将会进行一次分区分配：</p><ul><li>同一个consumer group内新增或减少了消费者</li><li>订阅的主题新增分区</li><li>订阅的主题增加<br>  将分区的消费权从一个消费者移到另一个消费者称为再均衡(rebalance)，如何rebalance也涉及到分区分配策略。<br>  kafka内部存在两种的分区分配策略：range(默认)和round robin。<br>  (消费者组的分区分配策略&#x2F;消费者组的负载均衡策略&#x2F;消费者组的再均衡策略)</li></ul><h3 id="10-8-1-Range-Strategy"><a href="#10-8-1-Range-Strategy" class="headerlink" title="10.8.1 Range Strategy"></a>10.8.1 Range Strategy</h3><p><font size=3><b>先将消费者按照client.id字典排序，然后按topic逐个处理；<br>针对一个topic，将其partition总数&#x2F;消费者数 得到商n和余数m，则每个consumer至少分到n<br>个分区，且前m个consumer每人多分一个分区；<br></b></font></p><p><font size=3><b>举例说明1：假设有TOPIC_A有5个分区，由3个consumer(C1,C2,C3)来消费；<br>5&#x2F;3得到商1，余2，则每个消费者至少分1个分区，前两个消费者各多1个分区<br>C1:2个分区，C2:2个分区，C3:1个分区<br>接下来，就按照“区间”进行分配：<br>C1：TOPIC_A-0 TOPIC_A-1<br>C2：TOPIC_A-2 TOPIC_A_3<br>C3：TOPIC_A-4<br></b></font></p><p>举例说明2：假设TOPIC_A有5个分区，TOPIC_B有3个分区，由2个consumer(C1,C2)来消费</p><ul><li>先分配TOPIC_A<br>  5&#x2F;2得到商2，余1，则C1有3个分区，C2有2个分区，得到结果<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_A-2<br>  C2：TOPIC_A-3 TOPIC_A-4</li><li>再分配TOPIC_B<br>  3&#x2F;2得到商1，余1，则C1有2个分区，C2有1个分区，得到结果<br>  C1：TOPIC_B-0 TOPIC_B-1<br>  C2：TOPIC_B-2</li><li>最终分配结果：<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_A-2 TOPIC_B-0 TOPIC_B-1<br>  C2：TOPIC_A-3 TOPIC_A-4 TOPIC_B-2</li></ul><h3 id="10-8-2-Round-Robin-Strategy"><a href="#10-8-2-Round-Robin-Strategy" class="headerlink" title="10.8.2 Round-Robin Strategy"></a>10.8.2 Round-Robin Strategy</h3><p><font size=3><b>将所有主题分区组成TopicAndPartition列表，并对TopicAndPartition列表按照其hashCode排序,然后，以轮询的方式分配给各消费者<br></b></font></p><p>以上述“例2”来举例：</p><ul><li>先对TopicAndPartition的hashCode排序，假如排序结果如下：<br>  TOPIC_A-0 TOPIC_B-0 TOPIC_A-1 TOPIC_A-2 TOPIC_B-1 TOPIC_A-3 TOPIC_A-4 TOPIC_B-2</li><li>然后按轮询方式分配<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_B-1 TOPIC_A-4<br>  C2：TOPIC_B-0 TOPIC_A-2 TOPIC_A-3 TOPIC_B-2</li></ul><p>我们可以通过++partition.assignment.strategy++参数选择range或roundrobin。<br>partition.assignment.strategy参数默认的值是range。<br>partition.assignment.strategy&#x3D;org.apache.kafka.clients.consumer.RoundRobinAssignor<br>partition.assignment.strategy&#x3D;org.apache.kafka.clients.consumer.RangeAssignor</p><p>这个参数属于“消费者”参数！</p><h3 id="10-8-3-Sticky-Strategy"><a href="#10-8-3-Sticky-Strategy" class="headerlink" title="10.8.3 Sticky Strategy"></a>10.8.3 Sticky Strategy</h3><p>对应的类叫做：org.apache.kafka.clients.consumer.StickyAssignor<br>sticky策略的特点：</p><ul><li>要去达成最大化的均衡</li><li>尽可能保留各消费者原来分配的分区</li></ul><p>再均衡的过程中，还是会让各消费者先取消自身的分区，然后再重新分配（只不过是分配过程中会尽<br>量让原来属于谁的分区依然分配给谁)</p><h3 id="10-8-3-CooperativeSticky-Strategy"><a href="#10-8-3-CooperativeSticky-Strategy" class="headerlink" title="10.8.3 CooperativeSticky Strategy"></a>10.8.3 CooperativeSticky Strategy</h3><p>对应的类叫做：org.apache.kafka.clients.consumer.ConsumerPartitionAssignor<br>sticky策略的特点：</p><ul><li>逻辑与sticky策略一致</li><li>支持cooperative再均衡机制（再均衡的过程中，不会让所有消费者取消掉所有分区然后再进行重分配）</li></ul><h2 id="10-9-消费者组再均衡流程"><a href="#10-9-消费者组再均衡流程" class="headerlink" title="10.9 消费者组再均衡流程"></a>10.9 消费者组再均衡流程</h2><p><font size=3><b>消费组在消费数据的时候，有两个角色进行组内的各事务的协调：<br>角色1：Group Coordinator（组协调器）位于服务端（就是某个broker）<br>角色2：Group Leader（组长）位于消费端（就是消费组中的某个消费者）<br></b></font></p><h3 id="10-9-1-GroupCoordinator介绍"><a href="#10-9-1-GroupCoordinator介绍" class="headerlink" title="10.9.1 GroupCoordinator介绍"></a>10.9.1 GroupCoordinator介绍</h3><p><font size=3><b>每个消费组在服务端对应一个GroupCoordinator进行管理，GroupCoordinator是Kafka服务端中用<br>于管理消费组的组件。<br>消费者客户端中由ConsumerCoordinator组件负责与GroupCoordinator进行交互；<br>ConsumerCoordinator和GroupCoordinator最重要的职责就是负责执行消费者rebalance操作，包括前面提及的分区分配工作也是在rebalance期间完成的。<br></b></font></p><p>会触发rebalance的事件可能是如下任意一种：</p><ul><li>有新的消费者加入消费组。</li><li>有消费者宕机下线，消费者并不一定需要真正下线，例如遇到长时间的GC、网络延迟导致消<br>  费者长时间未向GroupCoordinator发送心跳等情况时，GroupCoordinator会认为消费者己下线。</li><li>有消费者主动退出消费组（发送LeaveGroupRequest请求）：比如客户端调用了unsubscrible()<br>  方法取消对某些主题的订阅。</li><li>消费组所对应的GroupCoorinator节点发生了变更。</li><li>消费组内所订阅的任一主题或者主题的分区数量发生变化。</li></ul><h3 id="10-9-2-再均衡流程"><a href="#10-9-2-再均衡流程" class="headerlink" title="10.9.2 再均衡流程"></a>10.9.2 再均衡流程</h3><p><font color=green size=4><b>阶段1：定位Group Coordinator<br></b></font></p><p>coordinator在我们组记偏移量的__consumer_offsets分区的leader所在broker上查找Group Coordinator的方式：</p><ul><li>先根据消费组groupid的hashcode值计算它应该所在__consumer_offsets中的分区编号；<br>  Utils.abc(gropId.hashCode)%groupMetadataTopicPartitionCount<br>  groupMetadataTopicPartitionCount为__consumer_offsets的分区总数，这个可以通过broker端参数<br>  offset.topic.num.partitions来配置，默认值是50；</li><li>找到对应的分区号后，再寻找此分区leader副本所在broker节点，则此节点即为自己的Grouping Coordinator；</li></ul><p><font color=green size=4><b>阶段2：加入组join the group<br></b></font></p><p>此阶段的重要操作之1：选举消费组的leader<br>private val members &#x3D; new mutable.HashMap[String, MemberMetadata]<br>var leaderid &#x3D; members.keys.head</p><p>消费组leader的选举，策略就是：随机！</p><p>此阶段的重要操作之2：选择分区分配策略<br>最终选举的分配策略基本上可以看作被各个消费者支持的最多的策略，具体的选举过程如下：<br>(1) 收集各个消费者支持的所有分配策略，组成候选集candidates<br>(2) 每个消费者从候选集candidates找出第一个自身支持的策略，为这个策略投上一票。<br>(3) 计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。</p><p>其实，此逻辑并不需要consumer来执行，而是由Group Coordinator来执行</p><p><font color=green size=4><b>阶段3：组信息同步SYNC group<br></b></font></p><p><font size=3><b>此阶段，主要是由消费组leader将分区分配方案，通过Group Coordinator来转发给组中各消费者<br></b></font></p><p><font color=green size=4><b>阶段4：心跳联系 HEART BEAT<br></b></font></p><p><font size=3><b>进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。<br>各消费者在消费数据的同时，保持与Group Coordinator的心跳通信；<br></b></font></p><p>消费者的心跳间隔时间由参数++heartbeat.interval.ms++指定，默认值为3000，即这个参数必须比++session.timeout.ms++参数设定的值要小；一般情况下heartbeat.interval.ms的配置值不能超过session.timeout.ms配置值的l&#x2F;3。这个参数可以调整得更低，以控制正常重新平衡的预期时间；</p><p>如果一个消费者发生崩溃，并停止读取消息，那么GroupCoordinator会等待一小段时间确认这个消费者死亡之后才会触发再均衡。在这一小段时间内，死掉的消费者并不会读取分区里的消息。<br>这个一小段时间由session.timeout.ms参数控制，该参数的配置值必须在broker端参数</p><h3 id="10-9-3-再均衡监听器"><a href="#10-9-3-再均衡监听器" class="headerlink" title="10.9.3 再均衡监听器"></a>10.9.3 再均衡监听器</h3><p><u>一个消费组中，一旦有消费者的增减发生，会触发消费者组的rebalance再均衡；</u><br>如果想控制消费者在发生再均衡时执行一些特定的工作，可以通过订阅主题时注册“再均衡监听器”来实现；</p><p>场景举例：在发生再均衡时，处理消费位移<br>如果A消费者消费掉的一批消息还没来得及提交offset，而它所负贵的分区在rebalance中转移给了B<br>消费者，则有可能发生数据的重复消费处理。此情形下，可以通过再均衡监听器做一定程度的补救；</p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">&quot;tpc_5&quot;</span>), <span class="keyword">new</span> <span class="title class_">ConsumerRebalanceListener</span>()&#123;</span><br><span class="line">        <span class="comment">//被取消旧分区后被调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">            <span class="comment">//store the current offset to db</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//分配到新的分区后被调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">            <span class="comment">//fetch the current offset from db</span></span><br><span class="line">I   &#125;</span><br><span class="line">&#125;)；</span><br></pre></td></tr></table></figure><h2 id="10-10-Kafka系统的CAP保证"><a href="#10-10-Kafka系统的CAP保证" class="headerlink" title="10.10 Kafka系统的CAP保证"></a>10.10 Kafka系统的CAP保证</h2><h3 id="10-10-1-分布式系统的CAP理论"><a href="#10-10-1-分布式系统的CAP理论" class="headerlink" title="10.10.1 分布式系统的CAP理论"></a>10.10.1 分布式系统的CAP理论</h3><p>CAP理论作为分布式系统的基础理论，它描述的是一个分布式系统在以下三个特性中：</p><ul><li>一致性(Consistency)</li><li>可用性(Availability)</li><li>分区容错性(Partition tolerance)</li></ul><p>最多满足其中的两个特性。也就是下图所描述的。分布式系统要么满足CA，要么CP，要么AP。无法同时满足CAP。</p><p>分区容错性：指的是分布式系统中的某个节点或者网络分区出现了故障的时候，整个系统仍然能对外提供满足一致性和可用性的服务。也就是说部分故障不影响整体使用。<br>事实上我们在设计分布式系统是都会考虑到bug，硬件，网络等各种原因造成的故障，所以即使部分节点或者网络出现故障，我们要求整个系统还是要继续使用的<br>(不继续使用，相当于只有一个分区，那么也就没有后续的一致性和可用性了)</p><p>可用性：一直可以正常的做读写操作。简单而言就是客户端一直可以正常访问并得到系统的正常响应。用户角度来看就是不会出现系统操作失败或者访问超时等问题。</p><p>一致性：在分布式系统完成某些操作后任何读操作，都应该获取到该写操作写入的那个最新的值。相当于要求分布式系统中的各节点时时刻刻保持数据的一致性。</p><p>Kafka作为一个商业级消息中间件，数据可靠性和可用性是优先考虑的重点，兼顾尽可能保证数据一致性；</p><h3 id="10-10-2-分区副本机制"><a href="#10-10-2-分区副本机制" class="headerlink" title="10.10.2 分区副本机制"></a>10.10.2 分区副本机制</h3><p><font size=3><b>kafka从0.8.0版本开始引入了分区副本：引入了数据冗余<br>也就是说每个分区可以人为的配置几个副本（创建主题的时候指定replication-factor，也可以在broker级别进行配置default.replication.factor）；<br>在众多的分区副本里面有一个副本是Leader，其余的副本是follower，所有的读写操作都是经过Leader<br>进行的，同时follower会定期地去leader上复制数据。当Leader挂了的时候，其中一个follower会<br>重新成为新的Leader。通过分区副本，引入了数据冗余，同时也提供了kafka的数据可靠性。<br>++Kaka的分区多副本架构是Kaka可靠性保证的核心，把消息写入多个副本可以使Kaka在发生<br>崩溃时仍能保证消息的持久性。++<br></b></font></p><h3 id="10-10-3-ISR同步副本列表"><a href="#10-10-3-ISR同步副本列表" class="headerlink" title="10.10.3 ISR同步副本列表"></a>10.10.3 ISR同步副本列表</h3><p><font size=3><b>ISR概念：（同步副本）。每个分区的leader会维护一个ISR列表，ISR列表里面就是follower副本的broker编号，只有跟得上Leader的follower副本才能加入到ISR里面，这个是通过<u>replica.lag.time.max.ms</u>&#x3D;l0000(默认值)参数配置的，<u>只有ISR里的成员才有被选为leader的可能。</u><br></b></font></p><h3 id="10-10-4-分区副本的数据一致性解决方案"><a href="#10-10-4-分区副本的数据一致性解决方案" class="headerlink" title="10.10.4 分区副本的数据一致性解决方案"></a>10.10.4 分区副本的数据一致性解决方案</h3><p><font size=3><b>kafka让分区多副本同步的基本手段是：follower副本定期向leader请求数据同步！<br>既然是定期同步，则leader和follower之间必然存在各种数据不一致的情景！<br></b></font></p><p>动态过程中的副本数据不一致，是很难解决的；<br>kafka先尝试着解决上述“消费者所见不一致”及“分区数据最终不一致”的问题；</p><p>解决方案的核心思想</p><ul><li>在动态不一致的过程中，维护一条步进式的“临时一致线”（既所谓的High Watermark);</li><li>高水位线HW&#x3D;ISR副本中最小LEO+1；</li><li>底层逻辑就是：offset &lt; HW的message，是各副本间一致的且安全的！</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114824.png" alt="Kafka图11"></p><p><font size=3><b>如上图所示：offset &lt; HW:3 的message，是所有副本都已经备份好的数据<br></b></font></p><p><font color=DeepPink size=3><b>解决“消费者所见不一致”（消费者只允许看到HW以下的message）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114562.png" alt="Kafka图12"></p><p><font color=DeepPink size=3><b>解决“分区副本数据最终不一致”（follower数据按HW截断）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114124.png" alt="Kafka图13"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114196.png" alt="Kafka图14"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114067.png" alt="Kafka图15"></p><h3 id="10-10-5-HW方案的天生缺陷"><a href="#10-10-5-HW方案的天生缺陷" class="headerlink" title="10.10.5 HW方案的天生缺陷"></a>10.10.5 HW方案的天生缺陷</h3><p>如前所述，看似HW解决了“分区数据最终不一致”的问题，以及“消费者所见不一致”的问题，但其实，这里面存在一个巨大的隐患，导致：</p><ul><li>“分区数据最终不一致”的问题依然存在</li><li>producer设置acks&#x3D;al后，依然有可能丢失数据的问题</li></ul><p>产生如上结果的根源是：++HW高水位线的更新，与数据同步的进度，存在迟滞！++</p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114646.png" alt="Kafka图16"></p><p>Step 1： leader和follower副本处于初始化值，follower副本发送fetch请求，由于leader副本没有数据，因此不会进行同步操作；</p><p>Step2： 生产者发送了消息m1到分区leader副本，写入该条消息后leader更新LEO&#x3D;1；</p><p>Step3： follower发送fetch请求，携带当前最新的offset&#x3D;0，leader处理fetch请求时，更新remote LEO&#x3D;0，对比LEO值最小为0，所以HW&#x3D;0，leader副本响应消息数据及leader HW&#x3D;0给follower，follower写入消息后，更新LEO值，同时对比leader HW值，取最小的作为新的HW值，此时follower HW&#x3D;0，这也意味着，follower HW是不会超过leader HW值的。</p><p>Step4： follower发送第二轮fetch请求，携带当前最新的offset&#x3D;l，leader处理fetch请求时，更新remote LEO&#x3D;l，对比LEO值最小为l，所以HW&#x3D;l，此时leader没有新的消息数据，所以直接返回leader HW&#x3D;1给follower，follower对比当前最新的LEO值与leader HW值，取最小的作为新的HW值，此时follower HW&#x3D;1。</p><p><font size=3><b>从以上步骤可看出，leader中保存的remote LEO值的更新（也即HW的更新)总是需要额外一轮<br>fetch RPC请求才能完成，这意味着在leader切换过程中，会存在数据丢失以及数据不一致的问题！<br></b></font></p><h3 id="10-10-6-HW会产生数据丢失和副本最终不一致问题"><a href="#10-10-6-HW会产生数据丢失和副本最终不一致问题" class="headerlink" title="10.10.6 HW会产生数据丢失和副本最终不一致问题"></a>10.10.6 HW会产生数据丢失和副本最终不一致问题</h3><p><font color=Crimson size=3><b>数据丢失的问题（即使produce设置acks&#x3D;all，依然会发生）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114586.png" alt="Kafka图17"></p><p><font color=red size=3><b>注意：leader中的HW值是在follower下一轮fetch RPC请求中完成更新的<br></b></font></p><p>如上图所示：</p><ul><li>状态起始：B为leader，A为follower；最新消息m2已同步，但B的HW比A的HW大1</li><li>A在此时崩溃（即follower没能通过下一轮请求来更新HW值）</li><li>A重启时，会自动将LEO值调整到之前的HW值，即会进行日志截断</li><li>B重启后，会从向A发送fetch请求，收到fetch响应后，拿到HW值，并更新本地HW值，这时B会做日志截断，因此，offsets&#x3D;1的消息被永久地删除了。</li></ul><p><font color=Crimson size=3><b>副本间数据最终不一致的问题（即使produce设置acks&#x3D;all，依然会发生）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114328.png" alt="Kafka图18"></p><p>如上图所示：</p><ul><li>状态起始：A为leader，B为follower；最新消息m2已同步，但B的HW比A的HW大1</li><li>A在此时崩溃（即follower没能通过下一轮请求来更新HW值）</li><li>B先重启，会自动将LEO值调整到之前的W值，即会进行日志截断，并在此刻接收了新的消息m3，HW随之上升为2</li><li>然后，A重启上线，会从向B发送fetch请求，收到fetch响应后，拿到HW值，并更新本<br>  地HW值，发现不需要截断，从而己经产生了“副本间数据最终不一致”！</li></ul><p><font  size=3><b>只要新一届leader在老leader重启上线前，接收了新的数据，就可能发生上图中的场景，根源也在于HW的更新落后于数据同步进度<br></b></font></p><h3 id="10-10-7-Leader-Epoch机制的引入"><a href="#10-10-7-Leader-Epoch机制的引入" class="headerlink" title="10.10.7 Leader-Epoch机制的引入"></a>10.10.7 Leader-Epoch机制的引入</h3><p><font size=3><b>为了解决HW更新时机是异步延迟的，而HW又是决定日志是否备份成功的标志，从而造成数据丢失和数据不一致的现象，Kafka引入了leader epoch机制；<br>在每个副本日志目录下都创建一个leader-epoch-checkpoint文件，用于保存leader的epoch信息；<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114771.png" alt="Kafka图19"></p><p><font size=3><b>它的格式为(epoch offset)，epoch指的是leader版本，它是一个单调递增的一个正整数值，每次leader变更，epoch版本都会+l，offset是每一代leader写入的第一条消息的位移值，比如：<br>(0,0)<br>(1,300)<br>以上第2个版本是从位移300开始写入消息，意味着第一个版本写入了0-299的消息。<br></b></font></p><p>leader epoch具体的工作机制</p><ul><li><p>当副本成为leader时：<br>  这时，如果此时生产者有新消息发送过来，会首先更新leader epoch以及LEO，并添加到<br>  leader–epoch-checkpoint文件中；</p></li><li><p>当副本变成follower时：<br>  发送LeaderEpochRequest请求给leader副本，该请求包括了follower中最新的epoch版本；<br>  leader返回给follower的响应中包含了一个LastOffset，如果follower last epoch&#x3D;leader last epoch(纪元相同)，则LastOffset&#x3D;leader LEO，否则取follower last epoch中最小的leader epoch的start offset值；</p></li></ul><p><font size=3><b>举个例子：假设follower last epoch&#x3D;1，此时leader有(1,20)(2,80)(3,120)，则LastOffset&#x3D;80；follower拿到LastOffset之后，会对比当前LEO值是否大于LastOffset，如果当前LEO大于LastOffset，则从LastOffset截断日志；<br>follower开始发送fetch请求给leader保持消息同步。<br></b></font></p><p><font color=OrangeRed size=3><b>解决数据丢失：<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132115476.png" alt="Kafka图20"></p><p><font size=3><b>如上图所示：<br>A重启之后，发送LeaderEpochRequest请求给B，由于B还没追加消息，此时epoch&#x3D;request epoch<br>&#x3D;0，因此返LastOffset&#x3D;leader LEO&#x3D;2给A<br>A拿到LastOffset之后，发现等于当前LEO值，故不用进行日志截断。就在这时B宕机了，A成为leader，在B启动回来后，会重复A的动作，同样不需要进行日志截断，数据没有丢失。<br></b></font></p><p><font color=OrangeRed size=3><b>解决数据最终不一致问题：<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132115693.png" alt="Kafka图21"></p><p>如上图所示：</p><ul><li>A和B同时宕机后，B先重启回来成为分区leader，这时候生产者发送了一条消息过来，leader<br>  epoch更新到1</li><li>此时A启动回来后，发送LeaderEpochRequest(follower epoch&#x3D;0)给B，B判断follower epoch<br>  不等于最新的epoch，于是找到大于follower epoch最小的epoch&#x3D;l，即LastOffset&#x3D;epoch start offset&#x3D;1</li><li>A拿到LastOffset后，判断小于当前LEO值，于是从LastOffset位置进行日志截断，接着开<br>  始发送fetch请求给B开始同步消息，避免了消息不一致&#x2F;离散的问题。</li></ul><h3 id="10-10-8-LEO-HW-LSO等相关术语速查"><a href="#10-10-8-LEO-HW-LSO等相关术语速查" class="headerlink" title="10.10.8 LEO&#x2F;HW&#x2F;LSO等相关术语速查"></a>10.10.8 LEO&#x2F;HW&#x2F;LSO等相关术语速查</h3><p><font size=3><b>LEO：(last end offset) 就是该副本中消息的最大偏移量的值+1；<br>HW：(high watermark) 各副本中LEO的最小值。这个值规定了消费者仅能消费HW之前的数据；<br>LW：(low watermark) 一个副本的log中，最小的消息偏移量；     <br>LS0：(last stable offset) 最后一个稳定的offset；<br>对未完成的事务而言，LSO的值等于事务<br>中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同HW相同；<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8-9 Kafka 系统架构</title>
      <link href="/post/b2c5a78a.html"/>
      <url>/post/b2c5a78a.html</url>
      
        <content type="html"><![CDATA[<h1 id="8-Kafka-系统架构"><a href="#8-Kafka-系统架构" class="headerlink" title="8. Kafka 系统架构"></a>8. Kafka 系统架构</h1><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051118.png" alt="Kafka图2"></p><p><font size=3><b>自我推导设计：</b></font></p><ul><li><font size=3><b>kafka是用来存数据的</b></font></li><li><font size=3><b>现实世界数据有分类，所以存储系统也应有数据分类管理功能，如mysql的表；kafka有topic</b></font></li><li><font size=3><b>如一个topic的数据全部交给一台server存储和管理，则读写吞吐量有限</b></font></li><li><font size=3><b>所以，一个topic的数据应该可以分成多个部分(partition)分别交给多台server存储和管理</b></font></li><li><font size=3><b>如果一台server宕机，这台server负责的partition将不可用，所以，一个partition应有多个副本</b></font></li><li><font size=3><b>一个partition有多个副本，则副本间的数据一致性难以保证，因此要有一个leader统领读写</b></font></li><li><font size=3><b>一个leader万一挂掉，则该partition又不可用，因此还要有leader的动态选举机制</b></font></li><li><font size=3><b>集群有哪些topic，topic有哪几个分区，server在线情况，等等元信息和状态信息需要在集群内部及客户端之间共享，则引入了zookeeper</b></font></li><li><font size=3><b>客户端在读取数据时，往往需要知道自己所读取的位置，因而要引入消息偏移量维护机制<br>  </b></font></li></ul><h2 id="8-1-broker服务器"><a href="#8-1-broker服务器" class="headerlink" title="8.1 broker服务器"></a>8.1 broker服务器</h2><p><font size=3><b>一台kafka服务器就是一个broker，一个kafka集群由多个broker组成。<br></b></font></p><h2 id="8-2-生产者producer"><a href="#8-2-生产者producer" class="headerlink" title="8.2 生产者producer"></a>8.2 生产者producer</h2><p><font size=3><b>消息生产者，就是向kafka broker发消息的客户端。<br></b></font></p><h2 id="8-3-消费者consumer"><a href="#8-3-消费者consumer" class="headerlink" title="8.3 消费者consumer"></a>8.3 消费者consumer</h2><p><font size=3><b>consumer：消费者，从kafka broker取消息的客户端。<br>consuemr group：消费组，单个或多个consumer可以组成一个消费组。<br></b></font></p><h2 id="8-4-主题Topic与分区Partition"><a href="#8-4-主题Topic与分区Partition" class="headerlink" title="8.4 主题Topic与分区Partition"></a>8.4 主题Topic与分区Partition</h2><p><font size=3><b>在 Kafka 中消息是以 Topic 为单位进行归类的，Topic 在逻辑上可以被认为是一个 Queue，Producer 生产的每一条消息都必须指定一个 Topic，然后 Consumer 会根据订阅的 Topic 到对应的 broker 上去拉取消息。</b></font></p><p><font size=3><b>为了提升整个集群的吞吐量，Topic 在物理上还可以细分多个分区，一个分区在磁盘上对应一个文件夹。由于一个分区只属于一个主题，很多时候也会被叫做主题分区(Topic-Partition)。<br></b></font></p><h2 id="8-5-分区副本replica"><a href="#8-5-分区副本replica" class="headerlink" title="8.5 分区副本replica"></a>8.5 分区副本replica</h2><p><font size=3><b>每个topic的每个partition都可以配置多个副本(replica)，以提高数据的可靠性；<br>每个partition的所有副本中，必有一个leader副本，其它的就是follow副本(observer副本)；<br>follow定期找leader同步最新的数据，对外提供服务只有leader；<br></b></font></p><h2 id="8-6-分区副本leader"><a href="#8-6-分区副本leader" class="headerlink" title="8.6 分区副本leader"></a>8.6 分区副本leader</h2><p><font size=3><b>partition replica中的一个角色，在一个partition的多个副本中，会存在一个副本角色为leader；<br>producer和consumer只能跟leader交互(读写数据)；<br></b></font></p><h2 id="8-7-分区follower"><a href="#8-7-分区follower" class="headerlink" title="8.7 分区follower"></a>8.7 分区follower</h2><p><font size=3><b>partition replica中的一个角色，它通过心跳通信不断从leader中拉取、复制数据(只负责备份)。如果leader所在节点宕机，follower中会选举出新的leader；<br></b></font></p><h2 id="8-8-消息偏移量offset"><a href="#8-8-消息偏移量offset" class="headerlink" title="8.8 消息偏移量offset"></a>8.8 消息偏移量offset</h2><p><font size=3><b>partition中每条消息都会被分配一个递增id(offset)，通过offset可以快速定位到消息的存储位置；kafka只保证按一个partition中的消息的顺序，不保证一个topic整体(多个partition间)的顺序。<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051824.jpeg" alt="Kafka图3"></p><h1 id="9-Kafka的数据存储结构"><a href="#9-Kafka的数据存储结构" class="headerlink" title="9. Kafka的数据存储结构"></a>9. Kafka的数据存储结构</h1><h1 id="9-1-Kafka的整体存储结构"><a href="#9-1-Kafka的整体存储结构" class="headerlink" title="9.1 Kafka的整体存储结构"></a>9.1 Kafka的整体存储结构</h1><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051828.jpg" alt="Kafka图5"></p><h1 id="9-2-物理存储目录结构"><a href="#9-2-物理存储目录结构" class="headerlink" title="9.2 物理存储目录结构"></a>9.2 物理存储目录结构</h1><ul><li><p><font size=3><b>存储目录 名称规范：topic名称-分区号 <br>  例如：t1-0、t1-1<br>  “t1”即为一个topic的名称； <br>  而”t1-0&#x2F;t1-1”则表明这个目录是t1这个topic的哪个partition<br>  </b></font></p></li><li><p><font size=3><b>数据文件 名称规范 <br>  生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低，kafka采取了分片和索引机制； <br>  每个partition的数据将分为多个segment存储   <br>  每个segment对应两个文件，“.index文件”和“.log文件” <br>  index和log文件以当前segment的第一条消息的offset命名。<br>  </b></font></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051462.jpeg" alt="Kafka图4"></p><p><font size=3><b>index索引文件中的数据为：消息offset -&gt; log文件中该消息的物理偏移量位置；</b></font></p><p><font size=3><b>Kakfa中的索引文件以稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引；每当写入一定量(由broker端参数log.index.interval.bytes指定，默认值为4096，即4KB)的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应的可以缩小或增加索引项的密度； <br>查询指定偏移量时，使用二分查找法来快速定位偏移量的位置。<br></b></font></p><h1 id="9-3-消息的message存储结构"><a href="#9-3-消息的message存储结构" class="headerlink" title="9.3 消息的message存储结构"></a>9.3 消息的message存储结构</h1><p><font size=3><b>在客户端编程代码中，消息的封装有两种：ProducerRecord、ConsumerRecord<br>简单来说，++kafka中的每个message由一对key-value构成；++<br>Kafka中的message格式经历了3个版本的变化：v0、v1、v2<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v0</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051939.jpg" alt="Kafka图6"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v1</td></tr></table><p><font size=3><b>各个字段的含义介绍如下：</b></font></p><ul><li><font size=3><b>crc：占用4个字节，主要用于校验消息的内容；</b></font></li><li><font size=3><b>magic：这个占用1个字节，主要用于标识Kafka版本。Kafka 0.10.x magic默认值为1</b></font></li><li><font size=3><b>attributes：占用1个字节，这里面存储了消息压缩使用的编码以及Timestamp类型。目前Kafka支持gzip、snappy以及lz4(0.8.2引入)三种压缩格式；后四位如果是0001则表示gzip压缩，如果是0010则是snappy压缩，如果是0011则是lz4压缩，如果是0000则表示没有使用压缩。第4个bit位如果为0，代表使用create time；如果为1代表append time；其余位(第5~8位)保留</b></font></li><li><font size=3><b>key length：占用4个字节，主要标识Key的内容的长度；</b></font></li><li><font size=3><b>key：占用N个字节，存储的是key的具体内容；</b></font></li><li><font size=3><b>value length：占用4个字节，主要标识value的内容的长度；</b></font></li><li><font size=3><b>value：value即是消息的真实内容，在Kafka中这个也叫做payload。<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051829.jpg" alt="Kafka图7"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v2</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051007.jpg" alt="Kafka图8"></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5-7 Kafka API开发</title>
      <link href="/post/fa7bdefa.html"/>
      <url>/post/fa7bdefa.html</url>
      
        <content type="html"><![CDATA[<h1 id="5-Kafka-生产者API"><a href="#5-Kafka-生产者API" class="headerlink" title="5. Kafka 生产者API"></a>5. Kafka 生产者API</h1><p><font size=3><b>一个正常的生产逻辑需要具备以下几个步骤： <br>（1）配置生产者参数及创建相应的生产者实例   <br>（2）构建待发送的消息       <br>（3）发送消息   <br>（4）关闭生产者实例<br></b></font></p><p><font size=3><b>首先引入maven依赖<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//泛型K：要发送的数据中的key</span></span><br><span class="line">        <span class="comment">//泛型V：要发送的数据中的value</span></span><br><span class="line">        <span class="comment">//隐含之意：kafka中的message，是key-value结构的（可以没有key）</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;doit01:9092,doit02:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//因为kafka底层的存储是没有类型维护机制的，用户所发的所有数据类型，都必须变成序列化后的byte[]</span></span><br><span class="line">        <span class="comment">//所以，kafka的producer需要一个针对用户要发送的数据类型的序列化工具</span></span><br><span class="line">        <span class="comment">//且这个序列化工具类，需要实现kafka所提供的序列工具接口，org.apache.kafka.common.serialization.Serializer</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 代码中进行客户端参数配置的另一种写法</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092,doit02:9092&quot;</span>);</span><br><span class="line">        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        props.setProperty(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造一个生产者客户端</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//将业务数据封装成客户端所能发送的封装格式</span></span><br><span class="line">            <span class="comment">//0-&gt;abc0</span></span><br><span class="line">            <span class="comment">//1-&gt;abc1</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; message = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;abcx&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;abc&quot;</span> + i);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//调用客户端去发送</span></span><br><span class="line">            <span class="comment">//数据的发送动作在producer的底层是异步线程去异步发送的</span></span><br><span class="line">            producer.send(message);</span><br><span class="line"></span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭客户端</span></span><br><span class="line">        <span class="comment">//producer.flush();</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font size=3><b>消息对象ProducerRecord，除了包含业务数据外，还包含了多个属性：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br></pre></td></tr></table></figure><h1 id="6-Kafka-消费者API"><a href="#6-Kafka-消费者API" class="headerlink" title="6. Kafka 消费者API"></a>6. Kafka 消费者API</h1><h2 id="6-1-Kafka-消费者API示例"><a href="#6-1-Kafka-消费者API示例" class="headerlink" title="6.1 Kafka 消费者API示例"></a>6.1 Kafka 消费者API示例</h2><p><font size=3><b>一个正常的消费逻辑需要具备以下几个步骤： <br>（1）配置消费者客户端参数及创建相应的消费者实例   <br>（2）订阅主题topic <br>（3）拉取消息并消费 <br>（4）定期向__consumer_offsets主题提交消费位移offset   <br>（5）关闭消费者实例<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构建一个properties来存放消费者客户端参数</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092&quot;</span>);</span><br><span class="line">        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//kafka的消费者，默认是从所属组之前所记录的偏移量开始消费，如果找不到之前的记录的偏移量，则从如下参数配置的策略来确定消费起始位移</span></span><br><span class="line">        <span class="comment">//可以选择：earliest（自动重置到每个分区的最前一条消息），latest（自动重置到每个分区的最新一条消息），none（没有重置策略）</span></span><br><span class="line">        props.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">&quot;latest&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置消费者所属的组id</span></span><br><span class="line">        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;d30-1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自动提交最新的消费位移，默认是开启的</span></span><br><span class="line">        props.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自动提交最新消费位移的时间间隔，默认值就是5000ms</span></span><br><span class="line">        props.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">&quot;5000&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造一个消费者客户端</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//订阅主题（可以是多个）</span></span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">&quot;abcx&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//显示指定消费起始偏移量</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">abcxP0</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;abcx&quot;</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">abcxP1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;abcx&quot;</span>, <span class="number">1</span>);</span><br><span class="line">        consumer.seek(abcxP0, <span class="number">10</span>);</span><br><span class="line">        consumer.seek(abcxP1, <span class="number">15</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//循环往复拉取数据</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">condition</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (condition) &#123;</span><br><span class="line">            <span class="comment">//客户端去拉取数据的时候，如果服务器没有数据响应，会保持连接等待服务端响应</span></span><br><span class="line">            <span class="comment">//poll中传入的超时时长参数，是指等待的最大时长</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMinutes(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                <span class="comment">//ConsumerRecord中，不光有用户的业务数据，还有kafka塞入的元数据</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> record.key();</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> record.value();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//本条数据所属的topic</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> record.topic();</span><br><span class="line">                <span class="comment">//本条数据所属的分区</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> record.partition();</span><br><span class="line">                <span class="comment">//本条数据的offset</span></span><br><span class="line">                <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> record.offset();</span><br><span class="line">                <span class="comment">//当前这条数据所在分区的leader的朝代纪元</span></span><br><span class="line">                Optional&lt;Integer&gt; leaderEpoch = record.leaderEpoch();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//在kafka的数据底层存储中，不光有用户的业务数据，还有大量元数据</span></span><br><span class="line">                <span class="comment">//timestamp就是其中只一，记录本条数据的时间戳</span></span><br><span class="line">                <span class="comment">//但是时间戳有两种类型，本条数据的创建时间（生产者），本条数据的追加时间（borker写入log文件的时间）</span></span><br><span class="line">                <span class="type">TimestampType</span> <span class="variable">timestampType</span> <span class="operator">=</span> record.timestampType();</span><br><span class="line">                <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> record.timestamp();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//数据头是生产者在写入数据时附加进去的（相当于用户自己自定义的元数据）</span></span><br><span class="line">                <span class="type">Headers</span> <span class="variable">headers</span> <span class="operator">=</span> record.headers();</span><br><span class="line"></span><br><span class="line">                System.out.println(String.format(<span class="string">&quot;数据key：%s, 数据value：%s, 数据所属的topic：%s, 数据所属的partition：%d,&quot;</span> +</span><br><span class="line">                                <span class="string">&quot;数据的offset：%d, 数据所属leader的纪元：%s, 数据时间戳类型：%s, 数据的时间戳：%d,&quot;</span>,</span><br><span class="line">                        key, value, topic, partition, offset, leaderEpoch.toString(), timestampType.name(), timestamp));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭客户端</span></span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-2-subscribe订阅主题"><a href="#6-2-subscribe订阅主题" class="headerlink" title="6.2 subscribe订阅主题"></a>6.2 subscribe订阅主题</h2><p><font size=3><b>subscribe有如下重载方法：</b></font></p><ul><li><font size=3><b>public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener)</b></font></li><li><font size=3><b>public void subscribe(Collection<String> topics)</b></font></li><li><font size=3><b>public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)</b></font></li><li><font size=3><b>public void subscribe(Pattern pattern)<br>  </b></font></li></ul><ol><li><font size=3><b>指定集合方式订阅主题： <br> consumer.subscribe(Arrays.asList(topic1));</b></font></li><li><font size=3><b>正则方式订阅主题<br> 如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅，在之后的过程中，如果有人又创建了新的主题，并且主题名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。<br> 正则表达式的方式订阅的示例如下<br> consumer.subscribe(Pattern.compile(“topic.*”));<br> 利用正则表达式订阅主题，可实现动态订阅。</b></font></li></ol><h2 id="6-3-assign订阅主题"><a href="#6-3-assign订阅主题" class="headerlink" title="6.3 assign订阅主题"></a>6.3 assign订阅主题</h2><p><font size=3><b>消费者不仅可以通过KafkaConsumer.subscribe()方法订阅主题，还可以直接订阅某些主题的指定分区；<br>在KafkaConsumer中提供了assign()方法来实现这些功能，此方法的具体定义如下:   <br>public void assign(Collection<TopicPartition> partitions) <br>这个方法只接受参数partitions，用来指定需要订阅的分区集合，示例如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;tpc_1&quot;</span>, <span class="number">0</span>), <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;tpc_2&quot;</span>, <span class="number">1</span>)));</span><br></pre></td></tr></table></figure><h2 id="6-4-subscribe和assign的区别"><a href="#6-4-subscribe和assign的区别" class="headerlink" title="6.4 subscribe和assign的区别"></a>6.4 subscribe和assign的区别</h2><ul><li><font color=red size=3><b>通过subscribe()方法订阅主题具有消费者自动再均衡功能；<br>  </b></font></li></ul><p><font size=3><b>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。<br></b></font></p><ul><li><font color=red size=3><b>assign()方法订阅分区时，是不具备消费者自动均衡功能的；<br>  </b></font></li></ul><p><font size=3><b>其实这一点从assign()方法参数可以看出端倪，两种类型subscribe()都有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。<br></b></font></p><h2 id="6-5-取消订阅"><a href="#6-5-取消订阅" class="headerlink" title="6.5 取消订阅"></a>6.5 取消订阅</h2><p><font size=3><b>既然有订阅，那么就有取消订阅。 <br>可以使用KafkaConsumer中的unsubscribe()方法取消主题的订阅，这个方法即可以取消通过subscribe(Collection)方式实现的订阅； <br>也可以取消通过subscribe(Pattern)方式实现的订阅，还可以取消通过assign(Collection)方式实现的订阅，示例如下：  <br>consumer.unsubscribe();   <br>如果将subscribe(Collection)或assign(Collection)集合参数设置为空集合，作用与unsubscribe()方法相同，如下示例中三行代码的效果相同：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe();</span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;());</span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure><h2 id="6-6-消息的消费模式"><a href="#6-6-消息的消费模式" class="headerlink" title="6.6 消息的消费模式"></a>6.6 消息的消费模式</h2><p><font size=3><b><u>Kafka中的消费是基于拉取模式的。</u></b></font><br><font size=3><b>消息的消费一般有两种模式：推送模式和拉取模式。推送模式是服务端主动将消息推送给消费者，而拉取模式消费者主动向服务端发起请求来拉取消息。</b></font></p><p><font size=3><b>Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复的调用poll()方法，poll()方法返回的是所订阅的主题（分区）上的一组消息。 <br>对于poll()方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空，如果订阅的所有分区中都没有可供消费的消息，那么poll()方法返回为空的信息集；</b></font></p><p><font size=3><b>poll()方法具体定义如下:   <br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)   <br>超过时间参数timeout，用来控制poll()方法的阻塞时间，在消费者的缓冲区里没有可用数据时会发生阻塞，如果消费者程序只用来单纯拉取并消费数据，则为了提高吞吐率，可以把timeout设置为Long.MAX_VALUE<br></b></font></p><h2 id="6-7-自动提交消费者偏移量"><a href="#6-7-自动提交消费者偏移量" class="headerlink" title="6.7 自动提交消费者偏移量"></a>6.7 自动提交消费者偏移量</h2><p><font size=3><b>Kafka中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数<u>enable.auto.commit</u>配置，默认值为true。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数<u>auto.commit.interval.ms</u>配置，默认值为5秒，此参数生效的前提是enable.auto.commit 参数为true。</b></font></p><p><font size=3><b>在默认的方式下，消费者每隔5秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。</b></font></p><p><font size=3><b>Kafka消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</b></font></p><ul><li><p><font size=3><b>重复消费 <br>  假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。</b></font></p></li><li><p><font size=3><b>丢失消息 <br>  按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看下图中的情形： <br>  拉取线程不断地拉取消息并存入本地缓存，比如在BlockingQueue中，另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第y+1次拉取，以及第m次位移提交的时候，也就是x+6之前的位移己经确认提交了，处理线程却还正在处理x+3的消息；此时如果处理线程发生了异常，待其恢复之后会从第m次位移提交处，也就是x+6的位置开始拉取消息，那么x+3至x+6之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。<br>  </b></font></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132042073.png" alt="Kafka图10"></p><h2 id="6-8-手动提交消费者偏移量-调用kafka-api"><a href="#6-8-手动提交消费者偏移量-调用kafka-api" class="headerlink" title="6.8 手动提交消费者偏移量(调用kafka api)"></a>6.8 手动提交消费者偏移量(调用kafka api)</h2><p><font size=3><b>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免；同时，自动位移提交也无法做到精确的位移管理。在Kafka中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。</b></font></p><p><font size=3><b>很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费；</b></font></p><p><font size=3><b>手动的提交方式可以让开发人员根据程序的逻辑在合适的时机进行位移提交。开启手动提交功能的前<br>提是消费者客户端参数++enable.auto.commit++配置为false，示例如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p><font size=3><b>手动提交可以细分为同步提交和异步提交，对应于KafkaConsumer中的commitSync()和commitAsync()两种类型的方法；<br></b></font></p><ul><li><font size=3><b>同步提交的方式 <br>  commitSync()方法的定义如下：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">    <span class="comment">//do something to process record.</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font size=3><b>对于采用commitSync()的无参方法，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用commitSync()的另一个有参方法，具体定义如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, offsetAndMetadata&gt; offsets)</span></span><br></pre></td></tr></table></figure><p><font size=3><b>示例代码如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt;r : records)(</span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">        <span class="comment">//do something to process record.</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">        consumer.commitsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">offsetAndMetadata</span> (offset+<span class="number">1</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=red size=3><b>提交的偏移量 &#x3D; 消费完的record的偏移量 + 1<br>因为，__consumer_offsets中记录的消费偏移量，代表的是，消费者下一次要读取的位置！！!<br></b></font></p><ul><li><font size=3><b>异步提交的方式 <br>  异步提交的方式(commitAsync())在执行的时候消费者线程不会被阻塞；可能在提交消费位移的结果还未返回之前就开始了新一次的拉取。异步提交可以让消费者的性能得到一定的增强。 <br>  commitAsync方法有一个不同的重载方法，具体定义如下：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, offsetCommitCallback callback)</span></span><br></pre></td></tr></table></figure><p><font size=3><b>示例代码如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset ()</span><br><span class="line">        <span class="comment">//do something to process record.</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">        consumer.commitAsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">offsetAndMetadata</span>(offset+<span class="number">1</span>)), <span class="keyword">new</span> <span class="title class_">offsetcommitcallback</span>() &#123;</span><br><span class="line">        <span class="meta">@override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onComplete</span><span class="params">(Map&lt;TopicPartition, offsetAndMetadata&gt; map, Exception e)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(e == <span class="literal">null</span> ) &#123;</span><br><span class="line">                System.out.printIn (map);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                System.out.println (<span class="string">&quot;error commit offset&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-9-消费者提交偏移量方式的总结"><a href="#6-9-消费者提交偏移量方式的总结" class="headerlink" title="6.9 消费者提交偏移量方式的总结"></a>6.9 消费者提交偏移量方式的总结</h2><p><font size=3><b>consumer的消费位移提交方式：</b></font></p><ol><li><font size=3><b>全自动 auto.offset..commit &#x3D; true -&gt; 定时提交到consumer_offsets</b></font></li><li><font size=3><b>半自动 auto.offset.commit &#x3D; false；然后手动触发提交consumer.commitSync()； -&gt; 提交到consumer_offsets</b></font></li><li><font size=3><b>全手动 auto.offset.commit &#x3D; false；写自己的代码去把消费位移保存到你自己的地方<br> mysql&#x2F;zk&#x2F;redis&#x2F; -&gt; 提交到自己所涉及的存储；初始化时也需要自己去从自定义存储中查询到消费位移</b></font></li></ol><h2 id="6-7-其它重要参数"><a href="#6-7-其它重要参数" class="headerlink" title="6.7 其它重要参数"></a>6.7 其它重要参数</h2><ul><li><font size=3><b>fetch.min.bytes&#x3D;1B    （一次拉取的最小字节数）</b></font></li><li><font size=3><b>fetch.max.bytes&#x3D;50M   （一次拉取的最大数据量）</b></font></li><li><font size=3><b>fetch.max.wait.ms&#x3D;500ms    (拉取时的最大等待时长</b></font>)</li><li><font size=3><b>max.partition.fetch.bytes&#x3D;1MB    （每个分区一次拉取的最大数据量）</b></font></li><li><font size=3><b>max.poll.records&#x3D;500    （一次拉取的最大条数）</b></font></li><li><font size=3><b>connections.max.idle.ms&#x3D;540000ms    （网络连接的最大闲置时长）</b></font></li><li><font size=3><b>request.timeout.ms&#x3D;30000ms    （一次请求等待响应的最大超时时长，consumer等待请求响应的最长时间）</b></font></li><li><font size=3><b>metadata.max.age.ms&#x3D;300000    （元数据在限定时间内没有进行更新，则会被强制更新）</b></font></li><li><font size=3><b>reconnect.backoff.ms&#x3D;50ms    （尝试重新连接指定主机之前的退避时间）</b></font></li><li><font size=3><b>retry.backoff.ms&#x3D;100ms    （尝试重新拉取数据的重试间隔）<br>  </b></font></li></ul><h1 id="7-topic管理-API示例"><a href="#7-topic管理-API示例" class="headerlink" title="7. topic管理 API示例"></a>7. topic管理 API示例</h1><p><font size=3><b>如果希望将管理类的功能集成到公司内部的系统中，打造集管理、监控、运营、告警为一体的生态平台，那么就需要以程序调用API方式去实现。 <br>工具类KafkaAdminClient可以用来管理broker、配置和ACL(Access Control List)，管理topic。<br></b></font></p><ul><li><font size=3><b>创建主题：CreateTopicsResult createTopics(Collection&lt;NewTopic&gt; newTopics)</b></font></li><li><font size=3><b>删除主题：DeleteTopicsResult deleteTopics(Collection&lt;String&gt; topics)</b></font></li><li><font size=3><b>列出所有可用的主题：ListTopicsResult listTopics()</b></font></li><li><font size=3><b>查看主题的信息：DescribeTopicsResult describeTopics(Collection&lt;String&gt; topicNames)</b></font></li><li><font size=3><b>查询配置信息：DescribeConfigsResult describeConfigs(Collection&lt;\ConfigResource&gt;<br>  resources)</b></font></li><li><font size=3><b>修改配置信息：AlterConfigsResult alterConfigs(Map&lt;ConfigResource,Config&gt; configs)</b></font></li><li><font size=3><b>增加分区：CreatePartitionsResult createPartitions(Map&lt;String,NewPartitions&gt; newPartitions)<br>  </b></font></li></ul><p><font size=3><b>构造一个KafkaAdminClient<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br></pre></td></tr></table></figure><h2 id="7-1-列出主题"><a href="#7-1-列出主题" class="headerlink" title="7.1 列出主题"></a>7.1 列出主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics();</span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get();</span><br><span class="line">System.out.printIn(topics);</span><br></pre></td></tr></table></figure><h2 id="7-2-查看主题信息"><a href="#7-2-查看主题信息" class="headerlink" title="7.2 查看主题信息"></a>7.2 查看主题信息</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList (<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>));</span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line">Set&lt;String&gt; ksets = res.keySet();</span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123;</span><br><span class="line">    System.out.println(res.get(k));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-3-创建主题"><a href="#7-3-创建主题" class="headerlink" title="7.3 创建主题"></a>7.3 创建主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092,doit02:9092,doit03:</span></span><br><span class="line"><span class="string">9092&quot;</span>)；</span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">3000</span>);</span><br><span class="line"><span class="comment">//创建admin client对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br><span class="line"><span class="comment">//由服务端controller自行分配分区及副本所在broker</span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>);</span><br><span class="line"><span class="comment">//手动指定分区及副本的broker分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"><span class="comment">//分区0，分配到broker0,broker1</span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>, Arrays.asList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line"><span class="comment">//分区1，分配到broker0,broker2</span></span><br><span class="line">replicaAssignments.put(<span class="number">1</span>, Arrays.asList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments);</span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3, tpc_4));</span><br><span class="line"></span><br><span class="line"><span class="comment">//从future中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    result.all().get()；</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure><h2 id="7-4-删除主题"><a href="#7-4-删除主题" class="headerlink" title="7.4 删除主题"></a>7.4 删除主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, </span><br><span class="line"><span class="string">&quot;tpc_1&quot;</span>)):</span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line">System.out.printIn(values);</span><br></pre></td></tr></table></figure><h2 id="7-5-其它管理"><a href="#7-5-其它管理" class="headerlink" title="7.5 其它管理"></a>7.5 其它管理</h2><p><font size=3><b>除了进行topic管理外，KafkaAdminClient也可以进行诸如动态参数管理，分区管理等各类管理操作；<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. Kafka 命令行工具</title>
      <link href="/post/79d4ebb8.html"/>
      <url>/post/79d4ebb8.html</url>
      
        <content type="html"><![CDATA[<h1 id="4-Kafka-命令行工具"><a href="#4-Kafka-命令行工具" class="headerlink" title="4. Kafka 命令行工具"></a>4. Kafka 命令行工具</h1><h2 id="4-1-topic管理操作：kafka-topics"><a href="#4-1-topic管理操作：kafka-topics" class="headerlink" title="4.1 topic管理操作：kafka-topics"></a>4.1 topic管理操作：kafka-topics</h2><h3 id="4-1-1-查看topic列表"><a href="#4-1-1-查看topic列表" class="headerlink" title="4.1.1 查看topic列表"></a>4.1.1 查看topic列表</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper doit01:2181</span><br></pre></td></tr></table></figure><h3 id="4-1-2-查看topic状态信息"><a href="#4-1-2-查看topic状态信息" class="headerlink" title="4.1.2 查看topic状态信息"></a>4.1.2 查看topic状态信息</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper doit01:2181 --describe --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132030431.png" alt="Kafka图1"></p><p><font size=3><b>从上面的结果中，可以看出，topic的分区数量，以及每个分区的副本数量，以及每个副本所在的broker节点，以及每个分区的leader副本所在borker节点，以及每个分区的ISR副本列表；<br>ISR：in synchronized replicas 同步副本（当然也包含leader自身replica.lag.time.max.ms&#x3D;10000默认值） <br>OSR：out of synchronized replicas 失去同步的副本（该副本上次请求leader同步数据距现在的时间间隔超出配置阈值）<br></b></font></p><h3 id="4-1-3-创建topic"><a href="#4-1-3-创建topic" class="headerlink" title="4.1.3 创建topic"></a>4.1.3 创建topic</h3><p><font size=3><b>(1)基本方式<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --create --replication-factor 3 --partitions 3 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><font size=3><b>参数解释：</b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic名称</span><br></pre></td></tr></table></figure><p><font size=3><b>(2)手动指定分配方案：分区数，副本数，存储位置<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --create --topic <span class="built_in">test</span> --replica-assignment 0:1:3,1:2:6</span><br></pre></td></tr></table></figure><p><font size=3><b>该topic，将有如下partition：<br>partition0：所在节点：broker0、borker1、borker3   <br>partition1：所在节点：borker1、borker2、borker6<br></b></font></p><h3 id="4-1-4-删除topic"><a href="#4-1-4-删除topic" class="headerlink" title="4.1.4 删除topic"></a>4.1.4 删除topic</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --delete --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><font size=3><b>删除topic，server.properties中需要一个参数处于启用状态：delete.topic.enable&#x3D;true<br>使用kafka-topics.sh脚本删除主题的行为本质上只是在ZooKeeper中的&#x2F;admin&#x2F;delete_topics路径下建一个与待删除主题同名的节点，以标记该主题为待删除的状态，然后由kafka控制器异步完成。<br></b></font></p><h3 id="4-1-5-增加分区数"><a href="#4-1-5-增加分区数" class="headerlink" title="4.1.5 增加分区数"></a>4.1.5 增加分区数</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --partitions 3</span><br></pre></td></tr></table></figure><p><font size=3><b>kafka只支持增加分区，不支持减少分区 <br>原因是：减少分区，代价太大(数据的转移，日志段拼接合并) <br>如果真的需要实现此功能，则完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去。<br></b></font></p><h3 id="4-1-6-动态配置topic参数"><a href="#4-1-6-动态配置topic参数" class="headerlink" title="4.1.6 动态配置topic参数"></a>4.1.6 动态配置topic参数</h3><p><font size=3><b>通过管理命令，可以为已创建的topic增加、修改、删除topic level参数<br>添加&#x2F;修改  指定topic的配置参数：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --connfig compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b>–config compression.type&#x3D;gzip 修改或添加参数配置<br>–add-config compression.type&#x3D;gzip 添加参数配置   <br>–delete-config compression.type 删除参数配置<br></b></font></p><h2 id="4-2-生产者：kafka-console-producer"><a href="#4-2-生产者：kafka-console-producer" class="headerlink" title="4.2 生产者：kafka-console-producer"></a>4.2 生产者：kafka-console-producer</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-producer.sh --broker-list doit01:9092 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><h2 id="4-3-消费者：kafka-console-consumer"><a href="#4-3-消费者：kafka-console-consumer" class="headerlink" title="4.3 消费者：kafka-console-consumer"></a>4.3 消费者：kafka-console-consumer</h2><p><font color=OrangeRed size=3><b>消费者在消费的时候，需要指定要订阅的主题，还可以指定消费的起始偏移量<br>消费的起始偏移量有3种策略： <br>earliest：从最早的消息开始消费 <br>lastes：从最新的消息开始消费 <br>指定offset（分区号，偏移量）：从指定的位置开始消费<br>kafka的topic中的消息，是有序号的（序号叫消费偏移量），而且消息的偏移量是在各个partition中独立维护的，在各个分区内，都是从0开始递增编号！<br></b></font></p><p><font size=3><b>(1)消费消息<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure><p><font size=3><b>(2)指定要消费的分区，和要消费的起始offset<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic <span class="built_in">test</span> --offset 2 --partition 0</span><br></pre></td></tr></table></figure><p><font size=3><b>(3）消费组 <br>消费组是kafka为了提高消费并行度的一种机制。 <br>消费组内的各个消费者之间，分担数据读取任务的最小单位是：partition<br>在kafka的底层逻辑中，任何一个消费者都有自己所属的组   <br>组和组之间，没有任何关系，大家都可以消费到目标topic的所有数据，但是组内的各个消费者，就只能读取到自己所分配到的partitions   <br>kafka中的消费组，可以动态增减消费者，而且消费组中的消费者数量发生任意变动，都会重新分配分区消费任务。<br></b></font></p><h2 id="4-4-消费位移的记录"><a href="#4-4-消费位移的记录" class="headerlink" title="4.4 消费位移的记录"></a>4.4 消费位移的记录</h2><p><font size=3><b>kafka的消费者，可以记录自己所消费到的消息偏移量，记录的这个偏移量就叫消费位移，记录这个消费到的位置，作用就在于消费者重启后可以接续上一次消费到位置来继续往后面消费。<br></b></font></p><p><font color=OrangeRed size=3><b>消费位移，是组内共享的！！！<br></b></font></p><p><font color=OrangeRed size=3><b>consumer去记录偏移量的时候，不是读到一条或一批数据就记录一次，而是周期性的去提交当前的位移<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic __consumer_offsets --formatter <span class="string">&quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot;</span></span><br></pre></td></tr></table></figure><p><font size=3><b>通过指定formatter工具类，来对__consumer_offsets主题中的数据进行解析；<br>如果需要获取某个特定consumer-group的消费偏移量信息，则需要计算该消费组的偏移量记录所在分区：Math.abs(groupID.hashCode())%numPartitions<br>__consumer_offsets的分区数为：50<br></b></font></p><h2 id="4-5-配置管理kafka-config"><a href="#4-5-配置管理kafka-config" class="headerlink" title="4.5 配置管理kafka-config"></a>4.5 配置管理kafka-config</h2><p><font size=3><b>kafka-config.sh脚本是专门用来对参数配置进行操作的，这里的操作是运行状态修改原有的配置，因此可以达到动态变更的目的； <br>动态配置的参数，会被存储在zookeeper上，因而是持久生效的 <br>kafka-configs.sh脚本包含：变更alter、查看describe这两种指令类型；<br>kafka-configs.sh支持主题、broker、用户和客户端这4个类型的配置。 <br>kafka-configs.sh脚本使用entity-type参数来指定操作配置的类型，并且使entity-name参数来指定配置的名称。<br></b></font></p><p><font size=3><b>比如查看topic的配置可以按如下方式执行：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --describe --entity-type topics --entity-name tpc_2</span><br></pre></td></tr></table></figure><p><font size=3><b>比如查看broker的动态配置可以按如下方式执行：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --describe --entity-type brokers --entity-name tpc_2</span><br></pre></td></tr></table></figure><p><font color=#0066FF size=3><b>entity-type和entity-name的对应关系<br></b></font></p><table><thead><tr><th><strong>entity-type的释义</strong></th><th><strong>entity-name的释义</strong></th></tr></thead><tbody><tr><td><strong>主题类型的配置，取值为topics</strong></td><td><strong>指定主题的名称</strong></td></tr><tr><td><strong>broker类型的配置，取值为brokers</strong></td><td><strong>指定brokerId值，即broker中broker.id参数配置的值</strong></td></tr><tr><td><strong>客户端类型的配置，取值为clients</strong></td><td><strong>指定clientId值，即KafkaProducer或KafkaConsumer的client.id参数配置的值</strong></td></tr><tr><td><strong>用户类型的配置，取值为users</strong></td><td><strong>指定用户名</strong></td></tr></tbody></table><p><font size=3><b>示例：添加topic级别参数<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --alter --entity-type topics --entity-name tpc22 --add-config cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p><font size=3><b>使用kafka-configs.sh脚本来变更(alter)配置时，会在ZooKeeper中创建一个命名形式为：&#x2F;config&#x2F;<entity-type>&#x2F;<entity name>的节点，并将变更的配置写入这个节点<br></b></font></p><h3 id="4-5-1-动态配置topic参数"><a href="#4-5-1-动态配置topic参数" class="headerlink" title="4.5.1 动态配置topic参数"></a>4.5.1 动态配置topic参数</h3><p><font size=3><b>通过管理命令，可以为已创建的topic增加、修改、删除topic level参数</b></font></p><ul><li><font size=3><b>添加&#x2F;修改  指定topic的配置参数：<br>  </b></font></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --connfig compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b>如果利用kafka-configs.sh脚本来对topic、producer、consumer、borker等进行参数动态配置</b></font></p><ul><li><font size=3><b>添加、修改配置参数<br>  </b></font></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit01:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b><br>-删除s配置参数<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit01:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1-3 Kafka 简介</title>
      <link href="/post/58eeb88f.html"/>
      <url>/post/58eeb88f.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Kafka-基本概念"><a href="#1-Kafka-基本概念" class="headerlink" title="1. Kafka 基本概念"></a>1. Kafka 基本概念</h1><h2 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h2><p><font size=3><b>Kafka最初是由LinkedIn即领英公司基于Scala和Java语言开发的分布式消息发布-订阅系统，现已捐献给Apache软件基金会。其具有高吞吐、低延迟的特性，许多大数据实时流式处理系统比如Storm、Spark、Flink等都能很好的与之集成。<br></b></font></p><p><font size=3><b>总的来讲，Kafka通常具有3重角色：</b></font></p><ul><li><font size=3><b>存储系统：通常消息队列会把消息持久化到磁盘，防止消息丢失，保证消息可靠性。Kafka的消息持久化机制和多副本机制使其能够作为通用数据存储系统使用。</b></font></li><li><font size=3><b>消息系统：Kafka和传统的消息队列比如RabbitMQ、RocketMQ、ActiveMQ类似，支持流量削峰、服务解耦、异步通信等核心功能。</b></font></li><li><font size=3><b>流处理平台：Kafka不仅能够与大多数流式计算框架完美整合，并且自身也提供了一个完整的流式处理库，即Kafka Streaming。Kafka Streaming提供了类似Flink中的窗口、聚合、变换、连接等功能。<br>  </b></font></li></ul><p><font size=3><b><u>一句话概括：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列(Message Queue)，在业界主要应用于大数据实时流式计算领域。</u><br></b></font></p><h2 id="1-2-kafka的特点"><a href="#1-2-kafka的特点" class="headerlink" title="1.2 kafka的特点"></a>1.2 kafka的特点</h2><ul><li><font size=3><b>高吞吐、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition，由多个consumer group对partition进行consume操作。</b></font></li><li><font size=3><b>可扩展性：kafka集群支持热扩展</b></font></li><li><font size=3><b>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</b></font></li><li><font size=3><b>容错性：允许集群中有节点失败(若副本数量为n，则允许n-1个节点失败)</b></font></li><li><font size=3><b>高并发：支持数千个客户端同时写入<br>  </b></font></li></ul><p><font size=3><b>Kafka在各种应用场景中，起到的作用可以归纳为这么几个术语：削峰填谷、解耦！在大数据流式计算场景领域中，kafka主要作为计算系统的前置缓存和输出结果缓存。<br></b></font></p><h1 id="2-安装部署"><a href="#2-安装部署" class="headerlink" title="2. 安装部署"></a>2. 安装部署</h1><ul><li>上传安装包</li><li>解压</li><li>修改配置文件<br>  (1)进入配置文件系统</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@doit01 apps]# cd kafka_2.12-2.3.1/config</span><br></pre></td></tr></table></figure><p>(2)编辑配置文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为依次增长的：0、1、2、3、4，集群中唯一id</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 数据存储的目录</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/opt/apps/data/kafkadata</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 底层存储的数据(日志)留存时长(默认7天)</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 底层存储的数据(日志)留存量(默认1G)</span></span><br><span class="line"><span class="attr">log.retention.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 指定zk集群地址</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">doit01:2181,doit02:2181,doit03:2181</span></span><br></pre></td></tr></table></figure><ul><li>分发安装包</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..3&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">scp -r kafka_2.11-2.2.2 linux0<span class="variable">$i</span>:<span class="variable">$PWD</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>安装包分发后，记得修改broker.id</p><ul><li>配置环境变量</li><li>启停集群(在各个节点上启动)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon /opt/apps/kafka_2.11-2.2.2/config/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止集群</span></span><br><span class="line">bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure><h1 id="3-Kafka运维监控"><a href="#3-Kafka运维监控" class="headerlink" title="3. Kafka运维监控"></a>3. Kafka运维监控</h1><p><font color=red size=3><b>详见 <a href="https://www.bilibili.com/video/BV1Xr4y1t7mQ?p=6&vd_source=dd05d982b6c8a7e631e7f07548a539b1">https://www.bilibili.com/video/BV1Xr4y1t7mQ?p=6&amp;vd_source=dd05d982b6c8a7e631e7f07548a539b1</a><br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Kafka</title>
      <link href="/post/8716a180.html"/>
      <url>/post/8716a180.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器伪分布式部署kafka"><a href="#云服务器伪分布式部署kafka" class="headerlink" title="云服务器伪分布式部署kafka"></a>云服务器伪分布式部署kafka</h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><p>本次安装的kafka使用了独立的zookeeper，所以需先搭建好zookeeper集群。</p><p><a href="https://yiyuyyds.cn/archives/centos7---zookeeper%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2">ZooKeeper伪分布式安装部署</a></p><p><a href="https://kafka.apache.org/downloads">Kafka下载地址</a></p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2.解压安装包"></a>2.解压安装包</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HOME=/home/software/kafka_2.12-3.0.0</span><br><span class="line">export PATH=$PATH:/$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="4-复制server-properties文件并改名"><a href="#4-复制server-properties文件并改名" class="headerlink" title="4.复制server.properties文件并改名"></a>4.复制server.properties文件并改名</h2><p>进入kafka的config路径下，复制server.properties配置文件，因为是伪分布式安装，一台机器上有3个kafka进程，所以这里复制3份。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp server.properties server_1.properties</span><br><span class="line">cp server.properties server_2.properties</span><br><span class="line">cp server.properties server_3.properties</span><br></pre></td></tr></table></figure><h2 id="5-分别输入如下配置"><a href="#5-分别输入如下配置" class="headerlink" title="5.分别输入如下配置"></a>5.分别输入如下配置</h2><h3 id="server-1-properties"><a href="#server-1-properties" class="headerlink" title="server_1.properties"></a>server_1.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_1</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9092</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h3 id="server-2-properties"><a href="#server-2-properties" class="headerlink" title="server_2.properties"></a>server_2.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_2</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9093</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9093</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h3 id="server-3-properties"><a href="#server-3-properties" class="headerlink" title="server_3.properties"></a>server_3.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_3</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9094</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9094</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6.启动"></a>6.启动</h2><p>进入kafka的bin目录下，运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon ../config/server_1.properties</span><br><span class="line">kafka-server-start.sh -daemon ../config/server_2.properties</span><br><span class="line">kafka-server-start.sh -daemon ../config/server_3.properties</span><br></pre></td></tr></table></figure><p>通过jps命令可以查看3个kafka进程说明部署成功。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通义万相AI绘画大模型体验</title>
      <link href="/post/139d3597.html"/>
      <url>/post/139d3597.html</url>
      
        <content type="html"><![CDATA[<h1 id="通义万相AI绘画大模型体验"><a href="#通义万相AI绘画大模型体验" class="headerlink" title="通义万相AI绘画大模型体验"></a>通义万相AI绘画大模型体验</h1><p>官网：<a href="https://wanxiang.aliyun.com/">https://wanxiang.aliyun.com</a></p><p>通义万相是<a href="https://baike.baidu.com/item/%E9%98%BF%E9%87%8C%E4%BA%91/297128?fromModule=lemma_inlink">阿里云</a>通义系列AI绘画创作大模型，该模型可辅助人类进行图片创作，于2023年7月7日正式上线。</p><p>前两天通过了通义万相的适用体验，一番试用下来，感触最深的便是，当今AI横行，唯有融入并去接受他们才能适应时代。</p><p>在我的使用场景下，一般是生成封面图，或者一些唯美超现实主义的图片。</p><p>作为阿里推出的一款大模型AI绘画，每日免费使用50次，一般的用户基本都能满足，这点让我极其舒适（偷笑😏），能够使用中文绘制。</p><p><img src="https://s3.bmp.ovh/imgs/2024/03/02/4d2787ff83cec98b.png" alt="AI生成图"></p><p>可选的绘制比例有<code>1:1</code>,<code>16:9</code>,<code>9:16</code></p><p>绘制时间一般在<code>1分钟以内</code>，算是比较快的了，可能和描述的多少有关。</p><p>有时候它也会无法绘制（会返还灵感值），猜测使用英文绘制会更好，也许中文支持并没有那么高。</p><p><img src="https://s3.bmp.ovh/imgs/2024/03/02/5f02cd1d21849720.png" alt="AI生生成图"></p><p>绘制后会产生4张图片，也能够继续选择风格继续生成。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Hadoop</title>
      <link href="/post/da2f3447.html"/>
      <url>/post/da2f3447.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器伪分布式部署Hadoop"><a href="#云服务器伪分布式部署Hadoop" class="headerlink" title="云服务器伪分布式部署Hadoop"></a>云服务器伪分布式部署Hadoop</h2><h2 id="1-hadoop安装包下载-下载地址"><a href="#1-hadoop安装包下载-下载地址" class="headerlink" title="1.hadoop安装包下载 下载地址"></a>1.hadoop安装包下载 <a href="https://downloads.apache.org/hadoop/common/">下载地址</a></h2><p>下载后缀名为.tar.gz的文件</p><h2 id="2-上传安装包到服务器并解压"><a href="#2-上传安装包到服务器并解压" class="headerlink" title="2.上传安装包到服务器并解压"></a>2.上传安装包到服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.2.2.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置ssh免密登录"><a href="#3-配置ssh免密登录" class="headerlink" title="3.配置ssh免密登录"></a>3.配置ssh免密登录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &#x27;&#x27; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="4-添加环境变量"><a href="#4-添加环境变量" class="headerlink" title="4.添加环境变量"></a>4.添加环境变量</h2><p>修改&#x2F;etc&#x2F;profile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>使环境变量立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="5-配置hadoop"><a href="#5-配置hadoop" class="headerlink" title="5.配置hadoop"></a>5.配置hadoop</h2><p>进入hadoop安装目录下的etc&#x2F;hadoop文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hadoop-3.2.2/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="5-1-配置hadoop-env-sh"><a href="#5-1-配置hadoop-env-sh" class="headerlink" title="5.1 配置hadoop-env.sh"></a>5.1 配置hadoop-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><h3 id="5-2-配置core-site-xml"><a href="#5-2-配置core-site-xml" class="headerlink" title="5.2 配置core-site.xml"></a>5.2 配置core-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.0.219:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置HDFS数据块和元数据保存的目录，一定要修改--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意，要在hadoop目录下创建tmp目录</p><h3 id="5-3-配置hdfs-site-xml"><a href="#5-3-配置hdfs-site-xml" class="headerlink" title="5.3 配置hdfs-site.xml"></a>5.3 配置hdfs-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp/nameNodeDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp/dataNodeDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 &#x2F;tmp&#x2F;hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p><h3 id="5-4-配置mapred-site-xml"><a href="#5-4-配置mapred-site-xml" class="headerlink" title="5.4 配置mapred-site.xml"></a>5.4 配置mapred-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史服务器端地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-5-配置yarn-site-xml"><a href="#5-5-配置yarn-site-xml" class="headerlink" title="5.5 配置yarn-site.xml"></a>5.5 配置yarn-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><p>yarn默认端口是8088，但是这里必须要修改yarn端口，原因是因为像yarn、redis这种8088和6379端口是近年来挖矿病毒的热门踩点端口，具体的可参考这篇文章<a href="https://segmentfault.com/a/1190000015264170">https://segmentfault.com/a/1190000015264170</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--配置Yarn的节点--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--NodeManager执行MR任务的方式是Shuffle洗牌--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--修改yarn端口为7776--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:7776<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2022-6-21-更新：以下步骤不需要了，直接跳过"><a href="#2022-6-21-更新：以下步骤不需要了，直接跳过" class="headerlink" title="2022&#x2F;6&#x2F;21 更新：以下步骤不需要了，直接跳过"></a>2022&#x2F;6&#x2F;21 更新：以下步骤不需要了，直接跳过</h2><p><del>### 5.5 sbin目录修改4个文件<br>在Hadoop安装目录下找到sbin文件夹<br>在里面修改四个文件<br>对于start-dfs.sh和stop-dfs.sh文件，在文件开头添加下列参数：</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/env bash</span></span><br><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure><p><del>对于start-yarn.sh和stop-yarn.sh文件，在文件开头添加下列参数：</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/env bash</span></span><br><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><h2 id="6-执行-NameNode-的格式化"><a href="#6-执行-NameNode-的格式化" class="headerlink" title="6.执行 NameNode 的格式化"></a>6.执行 NameNode 的格式化</h2><p>进入hadoop目录下的bin目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hadoop-3.2.2/bin/</span><br></pre></td></tr></table></figure><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>成功的话，会看到 “Storage directory XXX&#x2F;XXX&#x2F;XXX has been successfully formatted” 的提示。</p><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6.启动"></a>6.启动</h2><h3 id="6-1启动hdfs"><a href="#6-1启动hdfs" class="headerlink" title="6.1启动hdfs"></a>6.1启动hdfs</h3><p>进入hadoop目录下的bin目录，启动hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>查看java进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如有以下5个进程，说明hadoop伪分布部署成功<br>NameNode<br>SecondaryNameNode<br>ResourceManager<br>NodeManager<br>DataNode</p><p>访问服务器9870端口，出现hadoop的Web界面。<br>访问服务器7776端口，出现yarn界面。</p><h3 id="6-2启动历史服务器"><a href="#6-2启动历史服务器" class="headerlink" title="6.2启动历史服务器"></a>6.2启动历史服务器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>出现JobHistoryServer进程，访问服务器19888端口，出现hadoop的JobHistory界面。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
