<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Java设计模式 1.Singleton-单例模式</title>
      <link href="/post/256339cb.html"/>
      <url>/post/256339cb.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Singleton-单例模式"><a href="#1-Singleton-单例模式" class="headerlink" title="1.Singleton-单例模式"></a>1.Singleton-单例模式</h1><p><font color=OrangeRed size=4><b>单例模式有8中写法</b></font></p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第一种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 饿汉式</span></span><br><span class="line"><span class="comment"> * 类加载到内存后，就实例化一个单例，JVM保证线程安全</span></span><br><span class="line"><span class="comment"> * 简单实用，推荐使用</span></span><br><span class="line"><span class="comment"> * 唯一缺点，不管有没有用到，类加载时就完成实例化</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr01</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Mgr01</span> <span class="variable">INSTANCE</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Mgr01</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr01</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr01 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Mgr01</span> <span class="variable">m1</span> <span class="operator">=</span> Mgr01.getInstance();</span><br><span class="line">        <span class="type">Mgr01</span> <span class="variable">m2</span> <span class="operator">=</span> Mgr01.getInstance();</span><br><span class="line">        System.out.println(m1 == m2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第二种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 和01是一个意思</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr02</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Mgr02 INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        INSTANCE = <span class="keyword">new</span> <span class="title class_">Mgr02</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr02</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr02 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Mgr02</span> <span class="variable">m1</span> <span class="operator">=</span> Mgr02.getInstance();</span><br><span class="line">        <span class="type">Mgr02</span> <span class="variable">m2</span> <span class="operator">=</span> Mgr02.getInstance();</span><br><span class="line">        System.out.println(m1 == m2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第三种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * lazy loading</span></span><br><span class="line"><span class="comment"> * 也称懒汉式</span></span><br><span class="line"><span class="comment"> * 虽然达到了按需初始化的目的，但却带来线程不安全的问题</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr03</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Mgr03 INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr03</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr03 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (INSTANCE == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            INSTANCE = <span class="keyword">new</span> <span class="title class_">Mgr03</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//模拟线程不安全的问题</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr03.getInstance().hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第四种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 虽然达到了按需初始化的目的，但却带来线程不安全的问题</span></span><br><span class="line"><span class="comment"> * 可以通过synchronized解决，但也带来效率下降</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr04</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Mgr04 INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr04</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> Mgr04 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (INSTANCE == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            INSTANCE = <span class="keyword">new</span> <span class="title class_">Mgr04</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr04.getInstance().hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第五种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 虽然达到了按需初始化的目的，但却带来线程不安全的问题</span></span><br><span class="line"><span class="comment"> * 可以通过synchronized解决，但也带来效率下降</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr05</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Mgr05 INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr05</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr05 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (INSTANCE == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">//通过减小同步代码块的方式提高效率，但这种方式还是线程不安全的</span></span><br><span class="line">            <span class="keyword">synchronized</span> (Mgr05.class) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                INSTANCE = <span class="keyword">new</span> <span class="title class_">Mgr05</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr05.getInstance().hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第六种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr06</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Mgr06 INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr06</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr06 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (INSTANCE == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">//双重检查锁</span></span><br><span class="line">            <span class="keyword">synchronized</span> (Mgr06.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (INSTANCE == <span class="literal">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        Thread.sleep(<span class="number">1</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                    INSTANCE = <span class="keyword">new</span> <span class="title class_">Mgr06</span>();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr06.getInstance().hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=Red size=4>补充：private static volatile Mgr06 INSTANCE;</font></p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第七种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 静态内部类方式</span></span><br><span class="line"><span class="comment"> * JVM保证单例</span></span><br><span class="line"><span class="comment"> * 加载外部类时不会加载内部类，这样可以实现懒加载</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Mgr07</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">Mgr07</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Mgr07Holder</span> &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Mgr07</span> <span class="variable">INSTANCE</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Mgr07</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Mgr07 <span class="title function_">getInstance</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Mgr07Holder.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;m&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr07.getInstance().hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>第八种写法</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.singleton;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 不仅可以解决线程同步，还可以防止反序列化</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">Mgr08</span> &#123;</span><br><span class="line">    INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">m</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">Thread</span>(() -&gt;</span><br><span class="line">                    System.out.println(Mgr08.INSTANCE.hashCode())</span><br><span class="line">            ).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最长回文子串</title>
      <link href="/post/13c31df7.html"/>
      <url>/post/13c31df7.html</url>
      
        <content type="html"><![CDATA[<h1 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a>最长回文子串</h1><h2 id="1-力扣题目编号：5"><a href="#1-力扣题目编号：5" class="headerlink" title="1.力扣题目编号：5"></a>1.力扣题目编号：<a href="https://leetcode.cn/problems/longest-palindromic-substring/">5</a></h2><h2 id="2-解法一：动态规划"><a href="#2-解法一：动态规划" class="headerlink" title="2.解法一：动态规划"></a>2.解法一：动态规划</h2><p>如果用dp[i][j] 保存子串从i 到j是否是回文子串，那么在求dp[i][j] 的时候如果j-i&gt;&#x3D;2时，如果 dp[i][j] 为回文，那么dp[i+1][j-1],也一定为回文。否则dp[i][j]不为回文。如下图：</p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131424420.png" alt="最长回文子串 图-1"  /><p>因此可得到动态规划的递归方程：</p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131425498.png" alt="最长回文子串 图-2"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">longestPalindrome</span><span class="params">(String s)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(s.length() &lt; <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxStart</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxEnd</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxLen</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="type">boolean</span>[][] dp = <span class="keyword">new</span> <span class="title class_">boolean</span>[s.length()][s.length()];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> <span class="number">1</span>; right &lt; s.length(); right++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>; left &lt; right; left++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(s.charAt(left) == s.charAt(right) &amp;&amp; (right-left &lt;= <span class="number">2</span> || dp[left+<span class="number">1</span>][right-<span class="number">1</span>]))&#123;</span><br><span class="line">                    dp[left][right] = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">if</span>(right-left+<span class="number">1</span> &gt; maxLen)&#123;</span><br><span class="line">                        maxLen = right-left+<span class="number">1</span>;</span><br><span class="line">                        maxStart = left;</span><br><span class="line">                        maxEnd = right;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.substring(maxStart, maxEnd+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>三点说明：</p><ol><li>当子串的长度为1时肯定为回文子串，对应上面的 dp[left][right] &#x3D; true 。</li><li>当子串的长度为2且里面的两个元素相同时，则也是回文子串。对应上面的 dp[left][right] &#x3D;  s.charAt(left)&amp;&amp;(right-left &lt;&#x3D; 2)。</li><li>当串的长度大于2时，如串为aba时，要判断s.charAt(left) &#x3D;&#x3D; s.charAt(right) &amp;&amp; dp[left+1][right-1]，依赖子串。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分治法 快速排序</title>
      <link href="/post/67d9c1da.html"/>
      <url>/post/67d9c1da.html</url>
      
        <content type="html"><![CDATA[<h1 id="分治法-快速排序"><a href="#分治法-快速排序" class="headerlink" title="分治法 快速排序"></a>分治法 快速排序</h1><h2 id="1-力扣题目编号：912"><a href="#1-力扣题目编号：912" class="headerlink" title="1.力扣题目编号：912"></a>1.力扣题目编号：<a href="https://leetcode.cn/problems/sort-an-array/">912</a></h2><h2 id="2-算法思想"><a href="#2-算法思想" class="headerlink" title="2.算法思想"></a>2.算法思想</h2><p>快速排序是基于分治策略的一种排序算法，主要思想是先设置一个基准元素，通过划分将待排序的序列分成前后两部分，其中前一部分的数据都比基准元素要小，后一部分的数据都比基准元素要大，然后再递归调用，对两部分的序列分别进行快速排序，以此使整个序列达到有序。</p><h2 id="3-算法步骤"><a href="#3-算法步骤" class="headerlink" title="3.算法步骤"></a>3.算法步骤</h2><ol><li>对于待排序数组a[left, right]，以a[mid]为基准元素将a[left, right]分为a[left, mid-1]，a[mid]，a[mid+1, right]3部分，使a[left, mid-1]的所有元素小于等于a[mid]，a[mid+1, right]的所有元素大于等于a[mid]。</li><li>递归调用快排算法，分别对a[left, mid-1]和a[mid+1, right]进行排序。</li></ol><h2 id="4-完整代码"><a href="#4-完整代码" class="headerlink" title="4.完整代码"></a>4.完整代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MAX = <span class="number">1e3</span>+<span class="number">7</span>;</span><br><span class="line"><span class="type">int</span> a[MAX];</span><br><span class="line"><span class="type">int</span> N;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">partition</span><span class="params">(<span class="type">int</span> a[], <span class="type">int</span> left, <span class="type">int</span> right)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = left;</span><br><span class="line">    <span class="type">int</span> j = right+<span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> x = a[i];</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(a[++i] &lt; x &amp;&amp; i &lt; right);</span><br><span class="line">        <span class="keyword">while</span>(a[--j] &gt; x );</span><br><span class="line">        <span class="keyword">if</span>(i &gt;= j)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">swap</span>(a[i], a[j]); </span><br><span class="line">    &#125;</span><br><span class="line">    a[left] = a[j];</span><br><span class="line">    a[j] = x;</span><br><span class="line">    <span class="keyword">return</span> j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quickSort</span><span class="params">(<span class="type">int</span> a[], <span class="type">int</span> left, <span class="type">int</span> right)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(left &lt; right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = <span class="built_in">partition</span>(a, left, right);</span><br><span class="line">        <span class="built_in">quickSort</span>(a, left, mid<span class="number">-1</span>);</span><br><span class="line">        <span class="built_in">quickSort</span>(a, mid+<span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    cin &gt;&gt; N;</span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; num;</span><br><span class="line">        a[i] = num;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">quickSort</span>(a, <span class="number">0</span>, N<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;快速排序后：&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; a[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">15</span></span><br><span class="line"><span class="comment">12 38 1 3 89 100 2 7 29 28 43 78 28 11 4</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3-6. FlinkSql 表相关</title>
      <link href="/post/eb1d1d4b.html"/>
      <url>/post/eb1d1d4b.html</url>
      
        <content type="html"><![CDATA[<h1 id="3-FlinkSql-表的概念及类别"><a href="#3-FlinkSql-表的概念及类别" class="headerlink" title="3. FlinkSql 表的概念及类别"></a>3. FlinkSql 表的概念及类别</h1><h2 id="3-1-表的标识结构"><a href="#3-1-表的标识结构" class="headerlink" title="3.1 表的标识结构"></a>3.1 表的标识结构</h2><p><font size=3><b>每一个表的标识由3部分组成：</b></font></p><ul><li><font size=3><b>catalog name （常用于标识不同的“源”，比如hive catalog，inner catalog等）</b></font></li><li><font size=3><b>database name  （通常语义中的“库”）</b></font></li><li><font size=3><b>table name  （通常语义中的“表”）<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TableEnvironment</span> <span class="variable">tEnv</span> <span class="operator">=</span> ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">&quot;a_catalog&quot;</span>);</span><br><span class="line">tEnv.useDatabase(<span class="string">&quot;db1&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册在默认catalog的默认database中</span></span><br><span class="line">tEnv.createTemporaryView(<span class="string">&quot;a_view&quot;</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册在默认catalog的指定database中</span></span><br><span class="line">tEnv.createTemporaryView(<span class="string">&quot;db2.a_view&quot;</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册在指定catalog的指定database中</span></span><br><span class="line">tEnv.createTemporaryView(<span class="string">&quot;x_catalog.db3.a_view&quot;</span>, table);</span><br></pre></td></tr></table></figure><p><font size=3><b>一个FlinkSql程序在运行时，tableEnvironment通过持有一个map结构来记录所注册的catalog；<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">CatalogManager</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Logger</span> <span class="variable">LOG</span> <span class="operator">=</span> LoggerFactory.getLogger(CatalogManager.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, Catalog&gt; catalogs;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;ObjectIdentifier, CatalogBaseTable&gt; temporaryTables;</span><br></pre></td></tr></table></figure><h2 id="3-2-表与视图"><a href="#3-2-表与视图" class="headerlink" title="3.2 表与视图"></a>3.2 表与视图</h2><p><font size=3><b>FlinkSql中的表，可以是virtual的(view视图)和regular的(table常规表)</b></font></p><ul><li><font size=3><b>table描述了一个物理上的外部数据源，如文件、数据库表、kafka消息topic</b></font></li><li><font size=3><b>view则基于表创建，代表一个或多个表上的一段计算逻辑(就是对一段查询计划的逻辑封装)；</b></font></li></ul><p><font size=3><b><u>不管是table还是view，在Table API中得到的都是Table对象；</u><br></b></font></p><h2 id="3-2-临时与永久"><a href="#3-2-临时与永久" class="headerlink" title="3.2 临时与永久"></a>3.2 临时与永久</h2><ul><li><font size=3><b>临时表(视图)：创建时带temporary关键字(create temporary view、create temporary table)- - 永久表(视图)：创建时不带temporary关键字(create view、create table)<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sql定义方式</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;create view view_1 as select ... from projectedTable&quot;</span>)</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;create temporary view view_2 as select ... from projectedTable&quot;</span>)</span><br><span class="line"></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;create table (id int, ...) with (&#x27;connector&#x27; = &#x27;...&#x27;)&quot;</span>)</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;create temporary table (id int, ...) with (&#x27;connector&#x27; = &#x27;...&#x27;)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//table api方式</span></span><br><span class="line">tenv.createTable(<span class="string">&quot;t_1&quot;</span>, tableDescriptro);</span><br><span class="line">tenv.createTemporaryTable(<span class="string">&quot;t_1&quot;</span>, tableDescriptor);</span><br><span class="line"></span><br><span class="line">tenv.createTemporaryView(<span class="string">&quot;v_1&quot;</span>, dataStream, schema);</span><br><span class="line">tenv.createTemporaryView(<span class="string">&quot;v_1&quot;</span>, table);</span><br></pre></td></tr></table></figure><h1 id="4-FlinkSql-表定义概览"><a href="#4-FlinkSql-表定义概览" class="headerlink" title="4. FlinkSql 表定义概览"></a>4. FlinkSql 表定义概览</h1><h2 id="4-1-Table-API-Table创建概览"><a href="#4-1-Table-API-Table创建概览" class="headerlink" title="4.1 [Table API] Table创建概览"></a>4.1 [Table API] Table创建概览</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132014123.png" alt="FlinkSql图14"></p><p><font size=3><b>table对象获取方式解析：</b></font></p><ul><li><font size=3><b>从已注册的表</b></font></li><li><font size=3><b>从TableDescriptor(连接器&#x2F;format&#x2F;schema&#x2F;options)</b></font></li><li><font size=3><b>从DataStream</b></font></li><li><font size=3><b>从Table对象上的查询api生成</b></font></li><li><font size=3><b>从测试数据<br>  </b></font></li></ul><p><font size=3><b>涉及的核心参数</b></font></p><ul><li><font size=3><b>已注册的表名(catalog_name.database_name.object_name)</b></font></li><li><font size=3><b>TableDescriptor(表描述器，核心是connector连接器)</b></font></li><li><font size=3><b>DataStream(底层流)</b></font></li><li><font size=3><b>测试数据值<br>  </b></font></li></ul><h2 id="4-2-Table-API-Table创建示例"><a href="#4-2-Table-API-Table创建示例" class="headerlink" title="4.2 [Table API] Table创建示例"></a>4.2 [Table API] Table创建示例</h2><p><font size=3><b>通过已注册的表名生成Table对象<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">t1</span> <span class="operator">=</span> tenv.from(<span class="string">&quot;t1&quot;</span>);</span><br></pre></td></tr></table></figure><p><font size=3><b>通过DataStream生成Table对象</b></font></p><ul><li><font size=3><b>自动推断schema(反射手段)<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">t1</span> <span class="operator">=</span> tenv.fromDataStream(dataStream1);</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>手动定义schema<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> tenv.fromDataStream(dataStream2, Schema.newBuilder()</span><br><span class="line">        .column(<span class="string">&quot;f0&quot;</span>, DataTypes.STRUCTURED(</span><br><span class="line">                DataBean.class,</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;guid&quot;</span>, DataTypes.INT()),</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;uuid&quot;</span>, DataTypes.STRING()),</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;ts&quot;</span>, DataTypes.BIGINT())</span><br><span class="line">        )));</span><br></pre></td></tr></table></figure><p><font size=3><b>通过tableEnv的fromValues方法生成Table对象(快速测试用)<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> tenv.fromValues(</span><br><span class="line">        DataTypes.ROW(</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;id&quot;</span>, DataTypes.INT()),</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;name&quot;</span>, DataTypes.STRING()),</span><br><span class="line">                DataTypes.FIELD(<span class="string">&quot;number&quot;</span>, DataTypes.DOUBLE())</span><br><span class="line">        ),</span><br><span class="line">        Row.of(<span class="number">1</span>, <span class="string">&quot;zs&quot;</span>, <span class="number">18.2</span>),</span><br><span class="line">        Row.of(<span class="number">2</span>, <span class="string">&quot;ls&quot;</span>, <span class="number">28.2</span>),</span><br><span class="line">        Row.of(<span class="number">3</span>, <span class="string">&quot;cc&quot;</span>, <span class="number">16.2</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><font size=3><b>通过TableEnv上调用查询api，生成新的Table对象(本质上就是view)<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> table3.select($(<span class="string">&quot;guid&quot;</span>), $(<span class="string">&quot;uuid&quot;</span>));</span><br></pre></td></tr></table></figure><h2 id="4-3-Table-Sql-Table创建概览"><a href="#4-3-Table-Sql-Table创建概览" class="headerlink" title="4.3 [Table Sql] Table创建概览"></a>4.3 [Table Sql] Table创建概览</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132014728.png" alt="FlinkSql图15"></p><p><font size=3><b>注册sql表(视图)方式解析：</b></font></p><ul><li><font size=3><b>从已存在的DataStream注册</b></font></li><li><font size=3><b>从已存在的Table对象注册</b></font></li><li><font size=3><b>从TableDescriptor(连接器)注册</b></font></li><li><font size=3><b>执行Sql的DDL语句来注册<br>  </b></font></li></ul><h2 id="4-4-Table-Sql-Table创建示例"><a href="#4-4-Table-Sql-Table创建示例" class="headerlink" title="4.4 [Table Sql] Table创建示例"></a>4.4 [Table Sql] Table创建示例</h2><p><font size=3><b>将已存在的Table对象注册成sql视图<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tenv.createTemporaryView(<span class="string">&quot;t1&quot;</span>, table);</span><br></pre></td></tr></table></figure><p><font size=3><b>将DataStream注册成sql视图<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataBean</span> <span class="variable">bean1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DataBean</span>(<span class="number">1</span>, <span class="string">&quot;s1&quot;</span>, <span class="number">1000</span>);</span><br><span class="line">DataStreamSource&lt;DataBean&gt; dataStream1 = env.fromElements(bean1);</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.自动推断schema</span></span><br><span class="line">tenv.createTemporaryView(<span class="string">&quot;t1&quot;</span>, dataStream1);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.也可以收到指定schema</span></span><br><span class="line"><span class="type">Schema</span> <span class="variable">schema</span> <span class="operator">=</span> Schema.Builder.column...build();</span><br><span class="line">tenv.createTemporaryView(<span class="string">&quot;t1&quot;</span>, dataStream1, schema);</span><br><span class="line"></span><br><span class="line">tenv.executeSql(<span class="string">&quot;desc t1&quot;</span>);</span><br><span class="line">tenv.executeSql(<span class="string">&quot;select * from t1&quot;</span>);</span><br></pre></td></tr></table></figure><p><font size=3><b>通过connector注册sql表<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tenv.createTable(<span class="string">&quot;t1&quot;</span>, TableDescriptor.forConnector(<span class="string">&quot;filesystem&quot;</span>)</span><br><span class="line">        .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;file:///d:/a.txt&quot;</span>)</span><br><span class="line">        .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">        .schema(Schema.newBuilder()</span><br><span class="line">                .column(<span class="string">&quot;guid&quot;</span>, DataTypes.STRING())</span><br><span class="line">                .column(<span class="string">&quot;name&quot;</span>, DataTypes.STRING())</span><br><span class="line">                .column(<span class="string">&quot;age&quot;</span>, DataTypes.STRING())</span><br><span class="line">                .build())</span><br><span class="line">        .build());</span><br></pre></td></tr></table></figure><p><font size=3><b>通过sql DDl语句定义sql表<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tenv.executeSql(<span class="string">&quot;create Table age_info&quot;</span> +</span><br><span class="line">        <span class="string">&quot;(&quot;</span> +</span><br><span class="line">        <span class="string">&quot;id int,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;name string&quot;</span> +</span><br><span class="line">        <span class="string">&quot;gender, string&quot;</span> +</span><br><span class="line">        <span class="string">&quot;age int&quot;</span> +</span><br><span class="line">        <span class="string">&quot;primary key(id) not enforced&quot;</span> +</span><br><span class="line">        <span class="string">&quot;) with (&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;hostname&#x27; = &#x27;192.168.0.219&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;port&#x27; = &#x27;3306&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;username&#x27; = &#x27;root&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;password&#x27; = &#x27;123456&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;database-name&#x27; = &#x27;abc&#x27;,&quot;</span> +</span><br><span class="line">        <span class="string">&quot;&#x27;table-name&#x27; = &#x27;age-info&#x27;&quot;</span> +</span><br><span class="line">        <span class="string">&quot;)&quot;</span>);</span><br></pre></td></tr></table></figure><h1 id="6-FlinkSql-catalog详解"><a href="#6-FlinkSql-catalog详解" class="headerlink" title="6. FlinkSql catalog详解"></a>6. FlinkSql catalog详解</h1>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. FlinkSql 编程概览</title>
      <link href="/post/71bf0702.html"/>
      <url>/post/71bf0702.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-FlinkSql-编程概览"><a href="#2-FlinkSql-编程概览" class="headerlink" title="2. FlinkSql 编程概览"></a>2. FlinkSql 编程概览</h1><h2 id="2-1-FlinkSql程序结构"><a href="#2-1-FlinkSql程序结构" class="headerlink" title="2.1 FlinkSql程序结构"></a>2.1 FlinkSql程序结构</h2><p><font size=3><b>所需依赖<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>FlinkSql编程4步曲</td></tr></table><ul><li><font size=3><b>创建FlinkSql编程入口</b></font></li><li><font size=3><b>将数据定义成表(视图)</b></font></li><li><font size=3><b>执行sql语义的查询(sql语法或者tableAPI)</b></font></li><li><font size=3><b>将查询结果输出到目标表<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">EnvironmentSettings</span> <span class="variable">envSettrings</span> <span class="operator">=</span> EnvironmentSettings.inStreamingMode();</span><br><span class="line">        <span class="type">TableEnvironment</span> <span class="variable">tableEnv</span> <span class="operator">=</span> TableEnvironment.create(envSettrings);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把kafka中的一个topic数据，映射成一张FlinkSql表</span></span><br><span class="line">        tableEnv.executeSql(<span class="string">&quot;create table t_kafka                    &quot;</span></span><br><span class="line">                + <span class="string">&quot;(                                                    &quot;</span></span><br><span class="line">                + <span class="string">&quot;id int,                                             &quot;</span></span><br><span class="line">                + <span class="string">&quot;name string,                                        &quot;</span></span><br><span class="line">                + <span class="string">&quot;age int,                                            &quot;</span></span><br><span class="line">                + <span class="string">&quot;gender string                                       &quot;</span></span><br><span class="line">                + <span class="string">&quot;)                                                    &quot;</span></span><br><span class="line">                + <span class="string">&quot;with (                                               &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;connector&#x27; = &#x27;kafka&#x27;,                              &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;topic&#x27; = &#x27;first&#x27;,                                  &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;properties.bootstrap.servers&#x27; = &#x27;hadoop1:9092&#x27;,    &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;properties.group.id&#x27; = &#x27;g1&#x27;,                       &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;,            &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;format&#x27; = &#x27;json&#x27;,                                  &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;json.fail-on-missing-field&#x27; = &#x27;false&#x27;,             &quot;</span></span><br><span class="line">                + <span class="string">&quot;&#x27;json.ignore-parse-errors&#x27; = &#x27;true&#x27;                 &quot;</span></span><br><span class="line">                + <span class="string">&quot;)                                                    &quot;</span>);</span><br><span class="line"></span><br><span class="line">        tableEnv.executeSql(<span class="string">&quot;select gender,avg(age) as avg_age from t_kafka group by gender&quot;</span>).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-2-两种编程方式"><a href="#2-2-两种编程方式" class="headerlink" title="2.2 两种编程方式"></a>2.2 两种编程方式</h2><p><font size=3><b>FlinkSql提供两种编程方式：</b></font></p><ul><li><font size=3><b>Table API</b></font></li><li><font size=3><b>Table Sql</b></font></li></ul><p><font size=3><b>类比sparksql的dataframe api编程和sql编程；</b></font></p><h3 id="2-2-1-Table-API方式"><a href="#2-2-1-Table-API方式" class="headerlink" title="2.2.1 Table API方式"></a>2.2.1 Table API方式</h3><p><font size=3><b>Table API是一个与编程语言(java、python、scala)集成的查询API；与SQL不同，查询逻辑不是以字符串表达，而是在“宿主语言”中调用所提供的类、方法等；<br>复杂的运算可以通过调用多个方法组成，如：table.filter(…).groupBy(…).select(…).join(…).on(…)<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Table</span> <span class="variable">table2</span> <span class="operator">=</span> table.groupBy($(<span class="string">&quot;gender&quot;</span>))</span><br><span class="line">    .select($(<span class="string">&quot;gender&quot;</span>), $(<span class="string">&quot;age&quot;</span>).avg().as(<span class="string">&quot;avg_age&quot;</span>)）</span><br></pre></td></tr></table></figure><h3 id="2-2-2-Table-Sql方式"><a href="#2-2-2-Table-Sql方式" class="headerlink" title="2.2.2 Table Sql方式"></a>2.2.2 Table Sql方式</h3><p><font size=3><b>用“sql字符串”形式进行基于表的关系运算逻辑表达</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">table.executeSql(</span><br><span class="line"><span class="string">&quot;insert into RevenueChina&quot;</span> +</span><br><span class="line"><span class="string">&quot;select cId, cName, sum(revenue) asa revSum&quot;</span> +</span><br><span class="line"><span class="string">&quot;from Orders&quot;</span> +</span><br><span class="line"><span class="string">&quot;where cCountry = china&quot;</span> +</span><br><span class="line"><span class="string">&quot;group by cId, cName&quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="2-2-3-混搭方式"><a href="#2-2-3-混搭方式" class="headerlink" title="2.2.3 混搭方式"></a>2.2.3 混搭方式</h3><p><font size=3><b>表API和SQL查询可以很容易的混合，因为Table对象可以和sql表进行方便的互转：</b></font></p><ul><li><font size=3><b>可以让SQL查询返回Table对象，进而调用Table API</b></font></li><li><font size=3><b>可以用env.from(“sql表名”)引用sql表得到Table对象，进而调用Table API</b></font></li><li><font size=3><b>可以用env.createTemporaryView(“sql表名”, table对象)，将Table对象注册成sql表，进而用sql</b></font></li></ul><h2 id="2-3-Table-Environment"><a href="#2-3-Table-Environment" class="headerlink" title="2.3 Table Environment"></a>2.3 Table Environment</h2><p><font size=3><b>FlinkSql的编程，总是从一个入口环境TableEnvironment开始；<br>TableEnvironment主要功能如下：</b></font></p><ul><li><font size=3><b>注册catalog</b></font></li><li><font size=3><b>向catallog注册表</b></font></li><li><font size=3><b>加载可插拔模块(目前有hive modules，以用于扩展支持hive的语法、函数等)</b></font></li><li><font size=3><b>执行sql查询(sql解析，查询计划生成，job提交)</b></font></li><li><font size=3><b>注册用户自定义函数</b></font></li><li><font size=3><b>提供DataStream和Table之间的转换<br>  </b></font></li></ul><p><font size=3><b>创建方式1（直接创建TableEnvironment）<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">EnvironmentSettings</span> <span class="variable">envSettrings</span> <span class="operator">=</span> EnvironmentSettings</span><br><span class="line">.newInstance()</span><br><span class="line">.inStreamingMode()</span><br><span class="line"><span class="comment">//.inBatchMode()</span></span><br><span class="line">.build();</span><br><span class="line"><span class="type">TableEnvironment</span> <span class="variable">tEnv</span> <span class="operator">=</span> TableEnvironment.create(settings);</span><br></pre></td></tr></table></figure><p><font size=3><b>创建方式2（从StreamExecutionEnvironment创建，这样便于结合sql和stream编程）<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutrionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="type">StreamTableEnvironment</span> <span class="variable">tEnv</span> <span class="operator">=</span> StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. FlinkSql 快速认识</title>
      <link href="/post/10f738cc.html"/>
      <url>/post/10f738cc.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-FlinkSql-快速认识"><a href="#1-FlinkSql-快速认识" class="headerlink" title="1. FlinkSql 快速认识"></a>1. FlinkSql 快速认识</h1><h2 id="1-1-基本原理和架构"><a href="#1-1-基本原理和架构" class="headerlink" title="1.1 基本原理和架构"></a>1.1 基本原理和架构</h2><p><font size=3><b>Flink Sql是架构于Flink core之上用sql语义方便快捷的进行结构化数据处理的上层库；<br>(非常类似Spark sql和Spark core的关系)<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>整体架构和工作流程</td></tr></table><p><font size=3><b>核心工作原理如下：</b></font></p><ul><li><font size=3><b>将数据源(数据集)，绑定元数据(schema)后，注册成catalog中的表(table，view)；</b></font></li><li><font size=3><b>然后由用户通过tableAPI或者sql来表达计算逻辑</b></font></li><li><font size=3><b>由table-planner利用apache calcite对sql语义解析，绑定元数据得到逻辑执行计划</b></font></li><li><font size=3><b>再用Optimizer进行优化后，得到物理执行计划</b></font></li><li><font size=3><b>物理计划经过代码生成器后生成代码，得到Transformation Tree</b></font></li><li><font size=3><b>Transformation Tree转成JobGraph后提交到Flink集群执行<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007473.png" alt="FlinkSql图1"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>关于元数据管理</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007413.jpeg" alt="FlinkSql图2"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007819.jpeg" alt="FlinkSql图3"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>关于逻辑执行计划</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007454.jpeg" alt="FlinkSql图4"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>关于查询优化</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007769.png" alt="FlinkSql图5"></p><p><font size=3><b>Flink Sql中有两个优化器</b></font></p><ul><li><font size=3><b>RBO(基于规则的优化器)</b></font></li><li><font size=3><b>CBO(基于代价的优化器)<br>  </b></font></li></ul><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>RBO(基于规则的优化器)</td></tr></table><p><font size=3><b>遍历一系列规则(RelOptRule)，只要满足条件就对原来的计划节点(表达式)进行转换或调整位置，生成最终的执行计划。 <br>常见的规则包括：</b></font></p><ul><li><font size=3><b>分区裁剪(Partition Prune)、列裁剪</b></font></li><li><font size=3><b>谓词下推(Predicate Pushdown)、投影下推(Projection Pushdown)、聚合下推、limit下推、sort下推</b></font></li><li><font size=3><b>常量折叠(Constant Folding)</b></font></li><li><font size=3><b>子查询内联转join等<br>  </b></font></li></ul><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>RBO优化示意图</td></tr></table><p><font size=3><b>常量折叠<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007022.jpeg" alt="FlinkSql图6"></p><p><font size=3><b>谓词下推<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007687.png" alt="FlinkSql图7"></p><p><font size=3><b>投影下推<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007592.png" alt="FlinkSql图8"></p><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>CBO(基于代价的优化器)</td></tr></table><p><font size=3><b>会保留原有表达式，基于统计信息和代价模型，尝试探索生成等价关系表达式，最终取代价最小的执行计划。<br>CBO的实现有两种模型</b></font></p><ul><li><font size=3><b>Volcano模型</b></font></li><li><font size=3><b>Cascades模型</b></font></li></ul><p><font size=3><b>这两种模型思想很是相似，不同点在于Cascades模型一边遍历SQL逻辑树，一边优化，从而进一步裁剪掉一些执行计划。<br></b></font></p><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>CBO优化示意图</td></tr></table><p><font size=3><b>根据代价cost选择批处理join有方式(sortmergejoin、hashjoin、boradcasthashjoin)。<br>比如前文中的例子，在filter下推之后，在t2.id&lt;1000的情况下，由1百万数据量变为了1千条，计算cost之后，使用broadcasthashjoin最合适。<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007801.png" alt="FlinkSql图9"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>物理计划 --> Transformation Tree</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132007768.png" alt="FlinkSql图10"></p><h2 id="1-2-动态表特性"><a href="#1-2-动态表特性" class="headerlink" title="1.2 动态表特性"></a>1.2 动态表特性</h2><p><font size=3><b>与spark、hive等组件中的“表”的最大不同之处：Flink Sql中的表是动态表！<br>这是因为：</b></font></p><ul><li><font size=3><b>Flink对数据的核心抽象是“无界(或有界)”的数据流</b></font></li><li><font size=3><b>对数据处理过程的核心抽象是“流式持续处理”</b></font></li></ul><p><font size=3><b>因而，Flink Sql对“源表(动态表)”的计算及输出结果(结果表)，也是流式、动态、持续的；</b></font></p><ul><li><font size=3><b>数据源的数据是持续输入</b></font></li><li><font size=3><b>查询过程是持续计算</b></font></li><li><font size=3><b>查询结果是持续输出</b></font></li></ul><p><font size=3><b>如下图所示：</b></font></p><ul><li><font size=3><b>“源表clicks”是流式动态的</b></font></li><li><font size=3><b>“聚合查询的输出结果表”也是流式动态的<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132008544.png" alt="FlinkSql图11"></p><p><font size=3><b>这其中的动态，不仅体现在“数据追加”，对应输出结果表来说，“动态”还包含对“前序输出结果”的“撤回(删除)”、“更新”等模式；<br></b></font></p><p><font size=3><b>而Flink Sql如何将这种对于“前序输出的修正”表达给下游呢？<br>它的核心设计是在底层的数据流中为每条数据添加“ChangeMode(修正模式)”标记，而添加了这种ChangeMode标记的底层数据流，取名为changelogStream；<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132008458.png" alt="FlinkSql图12"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132008124.png" alt="FlinkSql图13"></p><p><font size=3><b>在Flink1.12之前，动态表所对应的底层stream，有3种：</b></font></p><ul><li><font size=3><b>Append-only stream</b></font></li><li><font size=3><b>Retract stream</b></font></li><li><font size=3><b>Upsert stream</b></font></li></ul><p><font size=3><b>现在，统称为changelog stream<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>14. Flink 程序分布式部署运行</title>
      <link href="/post/451f2901.html"/>
      <url>/post/451f2901.html</url>
      
        <content type="html"><![CDATA[<h1 id="14-Flink-程序分布式部署运行"><a href="#14-Flink-程序分布式部署运行" class="headerlink" title="14. Flink 程序分布式部署运行"></a>14. Flink 程序分布式部署运行</h1><h2 id="14-1-Job执行流程"><a href="#14-1-Job执行流程" class="headerlink" title="14.1 Job执行流程"></a>14.1 Job执行流程</h2><table><thead><tr><th>四层</th><th>说明</th><th>备注</th></tr></thead><tbody><tr><td>StreamGraph</td><td>代码生成的最初的图</td><td>表示程序的拓扑结构</td></tr><tr><td>JobGraph</td><td>将多个符合条件的节点，链接为一个节点</td><td>可减少数据在节点之间流动所需要的序列化传输消耗</td></tr><tr><td>ExecutionGraph</td><td>JobGraph的并行化版本</td><td>是调度层最核心的数据结构</td></tr><tr><td>PhysicalGraph</td><td>JobManager根据ExecutionGraph对Job进行调度后，在各个TaskManager上部署Task后形成的“图”</td><td>并不是一个具体的数据结构</td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131943903.jpg" alt="Flink图45"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131943426.jpg" alt="Flink图46"></p><h2 id="14-2-Flink集群运行时架构"><a href="#14-2-Flink集群运行时架构" class="headerlink" title="14.2 Flink集群运行时架构"></a>14.2 Flink集群运行时架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131944326.jpeg" alt="Flink图47"><br><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131944120.png" alt="Flink图4"></p><h2 id="14-3-Flink-standalone集群"><a href="#14-3-Flink-standalone集群" class="headerlink" title="14.3 Flink standalone集群"></a>14.3 Flink standalone集群</h2><p><font color=Red size=3><b>Flink程序中如果要访问到hdfs，则需要添加2个jar包到Flink的lib目录中<br>flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar<br>commons-cli-1.4.jar<br></b></font></p><p><font size=3><b>standalone集群模式的缺点：</b></font></p><ul><li><font size=3><b>资源利用弹性不够(资源总量是定死的，job退出后也不能立即回收资源)</b></font></li><li><font size=3><b>资源隔离度不够(所有job共享集群的资源)</b></font></li><li><font size=3><b>所有job共用一个jobManager，负载过大</b></font></li></ul><h2 id="14-4-Flink-on-yarn"><a href="#14-4-Flink-on-yarn" class="headerlink" title="14.4 Flink on yarn"></a>14.4 Flink on yarn</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>yarn模式运行时示意图</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131944579.jpeg" alt="Flink图48"></p><p><font size=3><b>flink on yarn，本质上就是去yarn集群上申请容器，来运行flink的jobManager + taskManager集群<br>flink的job，是flink集群内部的概念，它对yarn是不可见的<br></b></font></p><h2 id="14-5-Flink-on-yarn的三种模式"><a href="#14-5-Flink-on-yarn的三种模式" class="headerlink" title="14.5 Flink on yarn的三种模式"></a>14.5 Flink on yarn的三种模式</h2><p><font size=3><b>Flink can execution applications in one of three ways:</b></font></p><ul><li><p><font size=3><b>in Application Mode</b></font><br>  <font size=3><b>每个job独享一个集群，job退出则集群退出，用户类的main方法在集群上运行;</b></font></p></li><li><p><font size=3><b>in a Per-Job Mode</b></font><br>  <font size=3><b>每个job独享一个集群，job退出则集群退出，用户类的main方法在client端运行；</b></font><br>  <font size=3><b>(大job，运行时长很长，比较合适，因为每起一个job，都要去向yarn申请容器启动jobManager，taskManager，比较耗时)</b></font></p></li><li><p><font size=3><b>in Session Mode<br>  多个job共享同一个集群，job退出集群也不会退出，用户类的main方法在client端运行；</b></font><br>  <font size=3><b>(需要频繁提交大量小job的场景比较适用，因为每次提交一个新job的时候，不需要去向yarn注册应用)<br>  </b></font></p></li></ul><p><font size=3><b>The above modes differ in :</b></font></p><ul><li><font size=3><b>the cluster lifecycle and resource isolation guarantees</b></font></li><li><font size=3><b>whether the application’s main() method is executed on the client or on the cluster<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131944182.jpeg" alt="Flink图49"></p><h2 id="14-6-yarn-session模式提交"><a href="#14-6-yarn-session模式提交" class="headerlink" title="14.6 yarn session模式提交"></a>14.6 yarn session模式提交</h2><p><font size=3><b>Yarn-Session模式：所有作业共享集群资源，隔离性差，JM负载瓶颈，main方法在客户端执行，适合执行时间短，频繁执行的短任务，集群中的所有作业只有一个jobManager，另外，job被随机分配给TaskManager</b></font></p><p><font size=3><b>特点：</b></font><br><font size=3><b>Session-Cluster模式需要先启动集群，然后再提交作业，接着会向yarn申请一块空间后，资源永远保持不变，如果资源满了，下一个作业就无法提交，只能等到yarn中的一个作业执行完成后，释放了资源，下个作业才会正常提交。所有作业共享Dispatcher和ResourceManager；共享资源，适合规模小执行时间短的作业。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>基本操作命令</td></tr></table><p><font size=3><b>提交命令：bin&#x2F;yarn-session.sh -help<br>停止命令：yarn application -kill application_1550951854_002<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -jm 1024 -tm 1024 -s 2 -m yarn-cluster -nm hello -qu default</span><br><span class="line"></span><br><span class="line"><span class="comment"># -jm  --&gt; jobManager memory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -tm  --&gt; taskManager memory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -m yarn-cluster  --&gt; 集群模式(yarn集群模式)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -s  --&gt; 规定每个taskManager上的tasksolt数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -nm  --&gt; 自定义application名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -qu  --&gt; 指定要提交的yarn队列</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>13. Flink 容错配置及测试</title>
      <link href="/post/dd121195.html"/>
      <url>/post/dd121195.html</url>
      
        <content type="html"><![CDATA[<h1 id="13-Flink-容错配置及测试"><a href="#13-Flink-容错配置及测试" class="headerlink" title="13. Flink 容错配置及测试"></a>13. Flink 容错配置及测试</h1><h2 id="13-1-task级失败重启恢复状态"><a href="#13-1-task级失败重启恢复状态" class="headerlink" title="13.1 task级失败重启恢复状态"></a>13.1 task级失败重启恢复状态</h2><p><font color=OrangeRed size=3><b>Task级别的故障重启，是系统自动进行的</b></font></p><h3 id="13-1-1-内置的重启策略"><a href="#13-1-1-内置的重启策略" class="headerlink" title="13.1.1 内置的重启策略"></a>13.1.1 内置的重启策略</h3><p><font color=#0066FF size=3><b>1. Fixed Delay Restart Strategy</b></font><br><font size=3><b>固定延迟重启策略</b></font></p><ul><li><font size=3><b>会尝试一个给定的次数来重启Job</b></font></li><li><font size=3><b>如果超过最大的重启次数(默认Integer.MAX_VALUE次)，Job最终会失败</b></font></li><li><font size=3><b>在连续两次重启尝试之间，重启策略会等待一个固定时间<br>  </b></font></li></ul><p><font color=#0066FF size=3><b>2. Exponential Delay Restart Strategy</b></font><br><font size=3><b>本策略：故障越频繁，两次重启间的惩罚间隔就越长</b></font></p><ul><li><font size=3><b>initialBackoff  重启间隔惩罚时长的初始值：1s</b></font></li><li><font size=3><b>maxBackoff  重启间隔最大惩罚时长：60s</b></font></li><li><font size=3><b>backoffMultiplier  重启间隔时长的惩罚倍数：2(每多故障一次，重启延迟惩罚就在上一次的惩罚时长*倍数)</b></font></li><li><font size=3><b>resetBackoffThreshold  重置惩罚时长的平稳运行时长阈值(平稳运行达到这个阈值后，如果再故障，则故障重启延迟时间重置为初始值：1s)</b></font></li><li><font size=3><b>jitterFactor  取一个随机数，来加在重启时间点上，已让每次重启的时间点呈现一定随机性<br>  </b></font></li></ul><p><font color=#0066FF size=3><b>3. Failure Rate Restart Strategy</b></font><br><font size=3><b>失败率重启策略</b></font></p><ul><li><font size=3><b>在task故障后会尝试重启failover</b></font></li><li><font size=3><b>但是超过失败率后，Job会最终被认定失败</b></font></li><li><font size=3><b>在两个连续的重启尝试之间，重启策略会等待一个固定时间<br>  </b></font></li></ul><p><font color=#0066FF size=3><b>4. No Restart Strategy</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>代码示例</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * task失败自动重启策略配置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">RestartStrategies.<span class="type">RestartStrategyConfiguration</span> <span class="variable">restartStrategy</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//固定、延迟重启(参数1：故障重启最大次数，   参数2：两次重启之间的延迟间隔)</span></span><br><span class="line">restartStrategy = RestartStrategies.fixedDelayRestart(<span class="number">5</span>, <span class="number">2000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认的故障重启策略：不重启(只要有task失败，整个job就失败)</span></span><br><span class="line">restartStrategy = RestartStrategies.noRestart();</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 本策略：故障越频繁，两次重启间的惩罚间隔就越长</span></span><br><span class="line"><span class="comment"> * initialBackoff  重启间隔惩罚时长的初始值：1s</span></span><br><span class="line"><span class="comment"> * maxBackoff  重启间隔最大惩罚时长：60s</span></span><br><span class="line"><span class="comment"> * backoffMultiplier  重启间隔时长的惩罚倍数：2(每多故障一次，重启延迟惩罚就在上一次的惩罚时长*倍数)</span></span><br><span class="line"><span class="comment"> * resetBackoffThreshold  重置惩罚时长的平稳运行时长阈值(平稳运行达到这个阈值后，如果再故障，则故障重启延迟时间重置为初始值：1s)</span></span><br><span class="line"><span class="comment"> * jitterFactor  取一个随机数，来加在重启时间点上，已让每次重启的时间点呈现一定随机性</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">restartStrategy = RestartStrategies.exponentialDelayRestart(Time.seconds(<span class="number">1</span>), Time.seconds(<span class="number">60</span>), <span class="number">2.0</span>, Time.hours(<span class="number">1</span>), <span class="number">1.0</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * failureRate  在指定时长内的最大失败次数</span></span><br><span class="line"><span class="comment"> * failureInterval  指定的衡量时长</span></span><br><span class="line"><span class="comment"> * delayInterval  两次重启之间的时间间隔</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">restartStrategy = RestartStrategies.failureRateRestart(<span class="number">5</span>, Time.hours(<span class="number">1</span>), Time.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//常用于自定义RestartStrategy</span></span><br><span class="line"><span class="comment">//用户自定义了重启策略类，配置在了flink-conf.yaml文件中</span></span><br><span class="line">restartStrategy = RestartStrategies.fallBackRestart();</span><br><span class="line"></span><br><span class="line">env.setRestartStrategy(restartStrategy);</span><br></pre></td></tr></table></figure><h3 id="13-1-2-Failover-strategy策略"><a href="#13-1-2-Failover-strategy策略" class="headerlink" title="13.1.2 Failover-strategy策略"></a>13.1.2 Failover-strategy策略</h3><p><font color=OrangeRed size=3><b>jobmanager.execuion.failover-strategy: region</b></font></p><p><font size=3><b>本参数的含义是：当一个task故障，需要restart的时候，是restart整个job中所有task，还是只restart一部分task；<br></b></font></p><p><font color=#0066FF size=3><b>1. Restart All Failover Strategy</b></font><br><font size=3><b>重启所有task，来实现failover<br></b></font></p><p><font color=#0066FF size=3><b>2. Restart Pipelined Region Failover Strategy</b></font><br><font size=3><b>本策略将task划分成无关联的region</b></font><br><font size=3><b>当一个task失败时，该策略会计算出需要重启的最小region集，在一部分场景中，该策略相比Restart All会少重启一些task</b></font><br><font size=3><b><u>region是一个通过pipelined数据交换所连接的task集合；batch数据交换则意味着region的边界；</u></b></font></p><ul><li><font size=3><b>All data exchanges in a DataStream job or Streaming Table&#x2F;SQL job are pipelined.</b></font></li><li><font size=3><b>All data exchanges in a Batch Table&#x2F;SQL job are batched by default.</b></font></li><li><font size=3><b>The data exchanges types in a DataSet job are determined  by the ExecutionMode which can be set through ExecutionConfig<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>region重启必要性的计算策略：</td></tr></table><ul><li><font size=3><b>region包含需要重启的失败task</b></font></li><li><font size=3><b>结果分区被必须重启的region所依赖且不可用时，产出结果分区的region也需要被重启</b></font></li><li><font size=3><b>如果一个需要被重启的region，则消费它数据的region也需要被重启；(这是为了确保数据一致性，因为不确定的计算或分区，可能造成不同的分区)<br>  </b></font></li></ul><h2 id="13-2-cluster级失败重启恢复状态"><a href="#13-2-cluster级失败重启恢复状态" class="headerlink" title="13.2 cluster级失败重启恢复状态"></a>13.2 cluster级失败重启恢复状态</h2><h3 id="13-2-1-save-points概念"><a href="#13-2-1-save-points概念" class="headerlink" title="13.2.1 save points概念"></a>13.2.1 save points概念</h3><ul><li><font size=3><b>save points(保存点)是基于Flink检查点机制的完整快照备份机制，用来保存状态，可以在另一个集群或者另一个时间点从保存的状态中将作业恢复过来</b></font></li><li><font size=3><b>适用于应用升级、集群迁移、Flink集群版本更新、A&#x2F;B测试以及假定场景、暂停和重启、归档等场景</b></font></li><li><font size=3><b>保存点可以视为一个(算子ID -&gt; State)的Map，对于每一个有状态的算子，Key是算子ID，Value是算子的State<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>save points相关配置(flink-conf.yaml)</td></tr></table><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.savepoints.dir:</span> <span class="string">hdfs:///flink/savepoints</span></span><br></pre></td></tr></table></figure><h3 id="13-2-2-save-points操作命令"><a href="#13-2-2-save-points操作命令" class="headerlink" title="13.2.2 save points操作命令"></a>13.2.2 save points操作命令</h3><table><tr><td bgcolor=Gainsboro><font size=4><b>IDEA本地运行时savepoints恢复测试</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">conf.setString(<span class="string">&quot;execution.savepoint.path&quot;</span>, <span class="string">&quot;file:///D:/checkpoint/7ecbd4f9ahiagq15ag161ahk/chk-154&quot;</span>);</span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment(conf);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>集群运行时，手动触发savepoints</td></tr></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#触发一次savepoint</span></span><br><span class="line">$ bin/flink savepoint :jobId [:targetDirectory]</span><br><span class="line"></span><br><span class="line"><span class="comment">#为yarn模式集群触发savepoint</span></span><br><span class="line">$ bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br><span class="line"></span><br><span class="line"><span class="comment">#停止一个job集群并触发savepoint</span></span><br><span class="line">$ bin/flink stop --savepoint [:targetDirectory] :jobId</span><br><span class="line"></span><br><span class="line"><span class="comment">#从一个指定savepoint恢复启动job集群</span></span><br><span class="line">$ bin/flink run -s :savepointPath [:runArgs]</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除一个savepoint</span></span><br><span class="line">$ bin/flink savepoint -d :savepointPath</span><br></pre></td></tr></table></figure><h3 id="13-2-3-从save-points恢复时，快照数据重分配问题"><a href="#13-2-3-从save-points恢复时，快照数据重分配问题" class="headerlink" title="13.2.3 从save points恢复时，快照数据重分配问题"></a>13.2.3 从save points恢复时，快照数据重分配问题</h3><p><font size=3><b>Flink程序，允许在某次重启时，修改程序的JobGraph图(比如改变算子顺序，改变算子并行度等)，而且在修改了程序的JobGraph图后，依然可以加载之前的状态快照数据，只不过，可能需要对之前的状态快照数据，在新的JobGraph下进行数据重分配；</b></font></p><ul><li><font size=3><b>Operator state</b></font></li></ul><p><font size=3><b>快照数据在重分配时，可能会对用户程序的计算逻辑产生不可预料的影响<br>UnionListState用广播模式重分配；ListState用round-robin模式恢复；</b></font></p><ul><li><font size=3><b>Keyed state</b></font></li></ul><p><font size=3><b>快照数据在重分配时，因为程序处理数据时接收数据的规律和状态的分配规律完全一致，所以不会产生任何逻辑上的影响<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12. Flink sink端容错机制</title>
      <link href="/post/7323d95a.html"/>
      <url>/post/7323d95a.html</url>
      
        <content type="html"><![CDATA[<h1 id="12-Flink-sink端容错机制"><a href="#12-Flink-sink端容错机制" class="headerlink" title="12. Flink sink端容错机制"></a>12. Flink sink端容错机制</h1><h2 id="12-1-幂等写入方式"><a href="#12-1-幂等写入方式" class="headerlink" title="12.1 幂等写入方式"></a>12.1 幂等写入方式</h2><p><font size=3><b>Sink端主要的问题是，作业失败重启时，数据重放可能造成最终目标存储系统中被写入了重复数据； <br>如果目标存储系统支持幂等写入，且数据中有合适的key(主键)，则Flink的sink端完全可以利用目标系统的幂等写入特点，来实现数据的最终一致(精确一次)；</b></font></p><p><font size=3><b>只是，幂等写入的方式，能实现最终一致，但有可能存在过程中的不一致；<br></b></font></p><p><font color=Red size=3><b>注意：动态过程不一致，主要出现在”输出结果非确定”的计算场景中，如，输出guid，event_id，event_count，insert_time<br>则重复写入guid，event_id相同的两次数据时，第一次的值和后面覆盖的值，是发生了变化的<br></b></font></p><h2 id="12-2-两阶段事务写入"><a href="#12-2-两阶段事务写入" class="headerlink" title="12.2 两阶段事务写入"></a>12.2 两阶段事务写入</h2><h3 id="12-2-1-核心流程"><a href="#12-2-1-核心流程" class="headerlink" title="12.2.1 核心流程"></a>12.2.1 核心流程</h3><p><font size=3><b>Flink中的两阶段事务提交sink，主要是利用了上述的”checkpoint两阶段提交协议”和目标存储系统的事务支持机制(比如mysql等)；<br></b></font></p><p><font size=3><b>Flink中两阶段事务提交的核心过程如下：</b></font></p><ul><li><font size=3><b>Sink算子在一批数据处理过程中，先通过预提交事务开始对外输出数据</b></font></li><li><font size=3><b>待这批数据处理完成(即收到了checkpoint信号)后，向checkpoint coordinate上报自身checkpoint完成信息</b></font></li><li><font size=3><b>checkpoint coordinate收到所有算子任务的checkpoint完成信息后，再向各算子任务广播本次checkpoint完成信息</b></font></li><li><font size=3><b>两阶段事务提交算子收到checkpoint coordinate的回调信息时，执行事务commit操作<br>  </b></font></li></ul><h3 id="12-2-2-优缺点"><a href="#12-2-2-优缺点" class="headerlink" title="12.2.2 优缺点"></a>12.2.2 优缺点</h3><p><font size=3><b>要实现TwoPahseCommitSinkFunction对外部系统有如下要求：</b></font></p><ul><li><font size=3><b>外部系统必须提供事务支持或者能够可以Sink去模拟事务(BucketingSink的原子命名模拟保证了提交的原子性)，在事务commited之前不能对下游系统可见</b></font></li><li><font size=3><b>在快照间隔内事务不能timeout，否则无法以事务的方式提交输出</b></font></li><li><font size=3><b>事务必须在收到jobManager发送的global commited的消息后，才能commited，在fail recovery的时候，若恢复时间较长(载入大状态)，若事务关闭(事务timeout)，该数据才会丢失</b></font></li><li><font size=3><b>在fail recovery后，事务需要支持恢复之前的pending的事务，并进行提交。(一些外部系统能够使用transaction id去commit或者abort之前的事务)</b></font></li><li><font size=3><b>事务的提交必须是幂等的，因为在恢复时，会重新提交一遍pending transaction，因此需要对同一个事务的commit是幂等的</b></font></li></ul><p><font size=3><b>可以看到外部系统不但要支持事务，同时也要能支持根据事务id去恢复之前的事务。<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11. Flink checkpoint机制</title>
      <link href="/post/1a084bba.html"/>
      <url>/post/1a084bba.html</url>
      
        <content type="html"><![CDATA[<h1 id="11-Flink-checkpoint机制"><a href="#11-Flink-checkpoint机制" class="headerlink" title="11. Flink checkpoint机制"></a>11. Flink checkpoint机制</h1><h2 id="11-1-分布式checkpoint的难题"><a href="#11-1-分布式checkpoint的难题" class="headerlink" title="11.1 分布式checkpoint的难题"></a>11.1 分布式checkpoint的难题</h2><p><font size=3><b>由于Flink是一个分布式的系统，数据在流经系统中各个算子时，是有先后顺序的，换个角度来说就是：整个系统对一条数据的处理过程，并不是一个原子性的过程；</b></font></p><p><font size=3><b>这样一来，对系统中各算子的状态进行持久化(快照)，就成了一件棘手的事情；<br></b></font></p><p><font color=Green size=3><b>来看如下数据处理场景：</b></font></p><ul><li><font size=3><b>算子1：从kafka中读取数据，并在状态中记录消费位移</b></font></li><li><font size=3><b>算子2：对流入的整数进行累加，并输出累加结果</b></font></li><li><font size=3><b>算子3：对流入是整数进行累加，并输出累加结果<br>  </b></font></li></ul><table><tr><td bgcolor=SkyBlue><font size=4><b>先注意观察正常情况下，整个系统的各算子状态变化及最终输出结果</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928949.png" alt="Flink图38"></p><table><tr><td bgcolor=SkyBlue><font size=4><b>出于容错考虑，需要在某个时机对整个系统各个算子的状态数据进行快照持久化，如下</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928248.png" alt="Flink图37"></p><table><tr><td bgcolor=SkyBlue><font size=4><b>系统重启后加载快照数据，恢复各算子崩溃前的状态，但是会发现，处理结果相对正常时完全错误</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928233.png" alt="Flink图39"></p><p><font color=Green size=3><b>从最终结果来看，整个计算</b></font></p><ul><li><font color=Green size=3><b>丢失了数据2的结果</b></font></li><li><font color=Green size=3><b>数据3则因为内部状态的紊乱而产生了错误的结果<br>  </b></font></li></ul><p><font color=Red size=3><b>发生错误的根本原因是：简单粗暴的快照所得到的快照状态在各个算子间不统一(不是经过了相同数据的影响)<br></b></font></p><h2 id="11-2-Checkpoint的核心要点"><a href="#11-2-Checkpoint的核心要点" class="headerlink" title="11.2 Checkpoint的核心要点"></a>11.2 Checkpoint的核心要点</h2><p><font size=3><b>checkpoint是Flink内部对状态数据的快照机制； <br>Flink的checkpoint机制是源于Chandy-Lamport算法： <br>底层逻辑：通过插入序号单调递增的barrier，把无界数据流划分成逻辑上的数据批(段)，并提供段落标记(barrier)来为这段数据的处理，加持“事务(transaction)”特性；</b></font></p><ul><li><font size=3><b>每一段数据流要么被完整成功处理</b></font></li><li><font size=3><b>要么回滚一切不完整的影响(状态变化)<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928653.png" alt="Flink图40"></p><h2 id="11-3-Checkpoint的整体流程"><a href="#11-3-Checkpoint的整体流程" class="headerlink" title="11.3 Checkpoint的整体流程"></a>11.3 Checkpoint的整体流程</h2><ol><li><font color=OrangeRed size=3><b>JobMaster即CheckpointCoordinator会定期向每个source task发送命令start checkpoint(trigger checkpoint)；</b></font></li><li><font color=OrangeRed size=3><b>当source task收到trigger checkpoint指令后，产生barrier并通过广播的方式发送到下游，source task及其它所有task，收到barrier-n，会执行本地checkpoint-n，当checkpoint-n完成后，向JobMaster发送ack；</b></font></li><li><font color=OrangeRed size=3><b>当流图的所有节点都完成checkpoint n，JobMaster会收到所有节点的ack，那么就表示完成checkpoint-n；</b></font></li></ol><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928103.png" alt="Flink图41"></p><p><font size=3><b>说明：checkpoint机制的调用流程实质是2PC。JobMaster是协调者，所有operator task是执行者。start checkpoint是pre-commit的开始信号，而每个operator task的checkpoint是pre-commit过程，ack是执行者operator task反馈给协调者JobMaster，最后callback是commit。</b></font></p><p><font size=3><b>barrier是JobManager定期指派各个source算子插入， <br>每个算子做完了checkpoint-n，就会向JobManager应答， <br>JobManager收到所有算子对checkpoint-n的应答后，才认为这次checkpoint是成功的(完整完成的 global)<br>然后，JobManager确认checkpoint-n全局完成后，会向各个算子通报一次checkpoint-n完成。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>1. barrier会在数据流源头被注入并行数据流中</td></tr></table><p><font size=3><b>barrier-n所在的位置就是恢复时数据重新处理的起始位置。例如在kafka中，这个位置就是最后一个记录在分区内的消费位移(offset)，作业恢复时，会根据这个位置从这个偏移量向kafka请求数据，这个偏移量就是State中保存的内容之一。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>2. barrier接着向下游传递</td></tr></table><p><font size=3><b>当一个非数据源算子从所有的输入流中收到barrier-n时，该算子就会对自己的State保存快照，并向自己的下游广播发送barrier-n；<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>3. 一旦Sink算子接收到barrier，有两种情况：</td></tr></table><p><font size=3><b>(1) 如果是引擎内严格一次处理保证，当Sink算子已经收到了所有上游的barrier-n时，Sink算子对自己的State进行快照，然后通知检查点协调器(CheckpointCoordinator)。当所有的算子都向检查点协调器汇报成功之后，检查点协调器向所有的算子确认本次快照完成。</b></font></p><p><font size=3><b>(2) 如果是端到端严格一次处理保证，当Sink算子已经收到了所有上游的barrier-n时，Sink算子对自己的State进行快照，并预提交事务(两阶段提交的第一阶段)，再通知检查点协调器(CheckpointCoordinator)，检查点协调器向所有的算子确认本次快照完成，Sink算子提交事务(两阶段提交的第二阶段)，本次事务完成。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>snapshotting operator算子</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928409.jpg" alt="Flink图43"></p><ul><li><font size=3><b>For each parallelism stream data source，the offset&#x2F;position in the stream when the snapshot was started </b></font></li><li><font size=3><b>For each operator，a pointer to the state that was stored as part of the snapshot<br>  </b></font></li></ul><h2 id="11-4-对齐和非对齐checkpoint"><a href="#11-4-对齐和非对齐checkpoint" class="headerlink" title="11.4 对齐和非对齐checkpoint"></a>11.4 对齐和非对齐checkpoint</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>对齐的checkpoint</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928035.png" alt="Flink图42"></p><ul><li><font size=3><b>算子收到数字流barrier，字母流对应barrier尚未到达</b></font></li><li><font size=3><b>算子收到数字流barrier，会继续从数字流中接收数据，但这些流只能被搁置，记录不能被处理，而是放入缓存中，等待字母流barrier到达，在字母流到达前，1、2、3数据已经被缓存</b></font></li><li><font size=3><b>字母流到达，算子开始对齐State进行异步快照，并将barrier向下游广播，并不等待快照完毕</b></font></li><li><font size=3><b>算子做异步快照，首先处理缓存中积压数据，然后再从输入通道中获取数据<br>  </b></font></li></ul><p><font color=Red size=3><b>注意背压！<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>非对齐的checkpoint</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131928151.png" alt="Flink图44"></p><p><font size=3><b>barrier不对齐：就是指当还有其它流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据，等到所有流的barrier都到达后，就可以对该Operator做checkpoint了；<br>如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。<br></b></font></p><p><font color=Red size=3><b>非对齐的checkpoint不能保证精确一次<br></b></font></p><h2 id="11-5-Checkpoint相关参数和API"><a href="#11-5-Checkpoint相关参数和API" class="headerlink" title="11.5 Checkpoint相关参数和API"></a>11.5 Checkpoint相关参数和API</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要让Flink对托管状态数据进行容错，还需要开启快照机制</span></span><br><span class="line"><span class="comment">// 开启状态数据的checkpoint机制(快照的周期，快照的模式)</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 开启快照后，就需要指定快照数据的持久化存储位置</span></span><br><span class="line"><span class="comment">/*env.getCheckpointConfig().setCheckpointStorage(new URI(&quot;hdfs://114.116.0.219:9000/checkpoint&quot;));*/</span></span><br><span class="line"><span class="type">CheckpointConfig</span> <span class="variable">checkpointConfig</span> <span class="operator">=</span> env.getCheckpointConfig();</span><br><span class="line">checkpointConfig.setCheckpointStorage(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///d:/checkpoint/&quot;</span>));</span><br><span class="line">checkpointConfig.setAlignedCheckpointTimeout(Duration.ofMinutes(<span class="number">1000</span>)); <span class="comment">//设置ck对齐的超时时长</span></span><br><span class="line">checkpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); <span class="comment">//设置ck算法模式</span></span><br><span class="line">checkpointConfig.setCheckpointInterval(<span class="number">2000</span>); <span class="comment">//设置ck的间隔时长</span></span><br><span class="line"><span class="comment">//checkpointConfig.setCheckpointIdOfIgnoredInFlightData(5); //用于非对齐算法模式下，在job恢复时让各个算子自动抛弃掉ck-5的飞行数据</span></span><br><span class="line"><span class="comment">// job cancel调时</span></span><br><span class="line">checkpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line">checkpointConfig.setForceUnalignedCheckpoints(<span class="literal">false</span>); <span class="comment">//是否强制使用非对齐的ck模式</span></span><br><span class="line">checkpointConfig.setMaxConcurrentCheckpoints(<span class="number">5</span>); <span class="comment">//允许在系统中同时存在的飞行中(未完成的)的ck数</span></span><br><span class="line">checkpointConfig.setMinPauseBetweenCheckpoints(<span class="number">2000</span>); <span class="comment">//设置两次ck之间的最小时间间隔，用于防止checkpoint过多的占用算子的处理时间</span></span><br><span class="line">checkpointConfig.setCheckpointTimeout(<span class="number">3000</span>); <span class="comment">//一个算子在一次checkpoint执行过程中的总耗时时长超时上限</span></span><br><span class="line">checkpointConfig.setTolerableCheckpointFailureNumber(<span class="number">10</span>); <span class="comment">//允许的checkpoint失败最大次数</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10. Flink 容错机制概述</title>
      <link href="/post/d9ecd3f3.html"/>
      <url>/post/d9ecd3f3.html</url>
      
        <content type="html"><![CDATA[<h1 id="10-Flink-容错机制概述"><a href="#10-Flink-容错机制概述" class="headerlink" title="10. Flink 容错机制概述"></a>10. Flink 容错机制概述</h1><h2 id="10-1-为什么要容错机制"><a href="#10-1-为什么要容错机制" class="headerlink" title="10.1 为什么要容错机制"></a>10.1 为什么要容错机制</h2><p><font size=3><b>Flink是一个stateful(带状态)的数据处理系统，系统在处理数据的过程中，各算子所记录的状态会随着数据的处理而不断变化；<br>一旦系统崩溃，需要重启后能恢复出崩溃前的状态才能进行数据的接续处理，因此，必须要有一种机制能对系统内的各种状态进行持久化容错；<br></b></font></p><h2 id="10-2-Flink的Exactly-Once概述"><a href="#10-2-Flink的Exactly-Once概述" class="headerlink" title="10.2 Flink的Exactly-Once概述"></a>10.2 Flink的Exactly-Once概述</h2><h3 id="10-2-1-EOS基本概念"><a href="#10-2-1-EOS基本概念" class="headerlink" title="10.2.1 EOS基本概念"></a>10.2.1 EOS基本概念</h3><p><font size=3><b>Exactly-Once语义：指端到端的一致性，从数据读取、引擎计算、写入外部存储的整个过程中，即使机器或软件出现故障，都确保数据仅处理一次，不会重复也不会丢失。</b></font></p><p><font size=3><b>对于Flink程序来说，端到端EOS语义则包含source、state、sink三个环节的紧密配合<br></b></font></p><h3 id="10-2-2-Flink的精确一致核心要素"><a href="#10-2-2-Flink的精确一致核心要素" class="headerlink" title="10.2.2 Flink的精确一致核心要素"></a>10.2.2 Flink的精确一致核心要素</h3><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131917498.png" alt="Flink图36"></p><p><font size=3><b>要实现端到端的EOS保证，核心点在于：<br>一条(或者一批)数据，从注入系统、中间处理、到输出结果的整个流程中，要么每个环节都处理成功，要么失败回滚(回到从未处理过的状态)！</b></font></p><p><font size=3><b>Flink在目前的各类分布式计算引擎中，对EOS的支持是最完善的；</b></font><br><font size=3><b>在合理的数据源选择，合理的算子选择，合理的目标存储系统选择，合适的参数配置下，可以实现严格意义上的端到端EOS</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>source端的保证</td></tr></table><p><font size=3><b>数据从上游进入Flink，必须保证消息严格一次消费，同时Source端必须满足可重放(replay)，否则Flink计算层收到消息后未计算，却发生failure而重启，消息就会丢失。</b></font></p><p><font size=3><b>Flink的很多source算子都能为EOS提供保障，如kafka Source：</b></font></p><ul><li><font size=3><b>能够记录偏移量</b></font></li><li><font size=3><b>能够重放数据</b></font></li><li><font size=3><b>将偏移量记录在state中，与下游的其它算子的state一起，经由checkpoint机制实现了状态数据的快照统一<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>Flink算子状态的精确一次保证</td></tr></table><p><font size=3><b><del>利用checkpoint机制，把状态数据定期持久化存储下来，Flink程序一旦发生故障的时候，还可以选择状态点恢复，避免数据的重复，丢失。</del></p><ul><li><del>barrier对齐的checkpoint(可支持exactly-once)</del></li><li><del>barrier非对齐的checkpoint(只能实现at least once)</del><br>  </b></font></li></ul><p><font size=3><b>基于分布式快照算法：(Chandy-Lamport)，Flink实现了整个数据流中各算子的状态数据快照统一；<br>++即一次checkpoint后所持久化的各算子的状态数据，确保是经过了相同数据的影响++<br>这样一来，就能确保：</b></font></p><ul><li><font size=3><b>一条(或一批)数据要么是经过了完整正确处理</b></font></li><li><font size=3><b>如果这条(批)数据在中间任何过程失败，则重启恢复后，所有算子的state数据都能回到这条数据从未处理过时的状态<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>Sink端的保证</td></tr></table><p><del><font size=3><b>Flink将处理完的数据发送到Sink端时，通过两阶段提交协议，即TwoPhaseCommitSinkFunction函数。该SinkFunction提取并封装了两阶段提交协议中的公共逻辑，保证Flink发送Sink端时实现严格一次处理语义，同时，Sink端必须支持事务机制，能够支持数据回滚或者满足幂等性。<br>回滚机制：即当作业失败后，能够将部分写入的结果回滚到之前写入的状态。<br>幂等性：就是一个相同的操作，无论重复多少次，造成的结果和只操作一次相等。即当作业失败后，写入部分结果，但是当重新写入全部结果时，不会带来负面结果，重复写入不会带来错误结果。<br></b></font></del></p><p><font size=3><b>从前文所述的source端和内部state的容错机制来看，一批数据如果在sink端写出过程中失败(可能已经有一部分数据进入目标存储系统)，则重启后重放这批数据时有可能造成目标存储系统中出现数据重复，从而破坏EOS；</b></font></p><p><font size=3><b>对此，Flink也设计了相应机制来确保EOS</b></font></p><ul><li><font size=3><b>采用幂等写入方式</b></font></li><li><font size=3><b>采用两阶段提交(2PC，two phase)写入方式</b></font></li><li><font size=3><b>采用预写日志提交方式<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>Sink幂等性</td></tr></table><p><del><font size=3><b>满足幂等性写入特性的sink，可以支持端到端一致性；<br>但是在写入过程中可能会存在短暂不一致；<br></b></font></del></p><table><tr><td bgcolor=Gainsboro><font size=4><b>两阶段事务提交(2PC)</td></tr></table><p><del><font size=3><b>++两阶段事务支持++<br>此方式实现端到端一致性利用的是目标存储系统的事务机制；<br>sink会在checkpoint前对目标存储系统开启事务，并进行数据预提交；<br>等到checkpoint完成时，再对目标存储系统提交事务，从而物化结果；<br>如果在checkpoint前任务失败，则此前开启的事务及预提交数据，会被目标存储系统的事务机制回滚；<br>++预写日志两阶段提交++<br></b></font></del></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9. Flink 状态(state)及使用</title>
      <link href="/post/4f17df8e.html"/>
      <url>/post/4f17df8e.html</url>
      
        <content type="html"><![CDATA[<h1 id="9-Flink-状态-state-及使用"><a href="#9-Flink-状态-state-及使用" class="headerlink" title="9. Flink 状态(state)及使用"></a>9. Flink 状态(state)及使用</h1><h2 id="9-1-状态基本概念"><a href="#9-1-状态基本概念" class="headerlink" title="9.1 状态基本概念"></a>9.1 状态基本概念</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>什么是状态(raw状态)</td></tr></table><p><font size=3><b>在稍复杂的流式计算逻辑中，基本都需要记录和利用一些历史累积信息；<br>例如，对整数流进行全局累计求和的逻辑：</b></font></p><ol><li><font size=3><b>我们在算子中定义一个变量来记录累计到当前的总和；</b></font></li><li><font size=3><b>在一条新数据到达时，就将新数据累加到总和变量中；</b></font></li><li><font size=3><b>然后输出结果；<br> </b></font></li></ol><p><font size=3><b>由上可知：<u>状态就是用户在程序逻辑中用于记录信息的变量</u> （当然，依据不同的需求，状态数据可多可少，可简单可复杂！）</b></font></p><p><font size=3><b>如上所述，state只不过是用户编程时自定义的“变量”，跟Flink又有何关系呢？ <br><u>因为状态 (状态中记录的数据) 需要容错！！！</u>程序一旦在运行中突然失败，则用户自定义的状态所记录的数据会丢失，因而无法实现失败后重启的接续！<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>Flink托管状态</td></tr></table><p><font size=3><b>Flink提供了内置的状态数据管理机制(简称状态机制)；<br>Flink会进行状态数据管理：包括故障发生后的状态一致性维护、以及状态数据的高效存储和访问。</b></font></p><p><font size=3><b>用户借由Flink所提供的状态管理机制来托管自己的状态数据，则不用担心状态数据在程序失败及恢复时所引入的一系列问题，从而使得开发人员可以专注于应用程序的逻辑开发；<br></b></font></p><h2 id="9-2-状态后端"><a href="#9-2-状态后端" class="headerlink" title="9.2 状态后端"></a>9.2 状态后端</h2><h3 id="9-2-1-状态后端基本概念"><a href="#9-2-1-状态后端基本概念" class="headerlink" title="9.2.1 状态后端基本概念"></a>9.2.1 状态后端基本概念</h3><ul><li><font size=3><b>所谓状态后端，就是状态数据的存储管理系统，包含状态数据的本地存储、读写操作、TTL维护、快照生成、快照远端存储功能；</b></font></li><li><font size=3><b>状态后端是可插拔替换的，它对上层屏蔽了底层的差异，因为在更换状态后端时，用户的代码不需要做任何更改；<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131913111.jpeg" alt="Flink图34"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>可用的状态后端类型(Flink-1.13版本以后)</td></tr></table><ul><li><font size=3><b>HashMapStateBackend</b></font></li><li><font size=3><b>EmbeddedRocksDBStateBackend<br>  </b></font></li></ul><p><font size=3><b><del>老版本(Flink-1.12以前)</del> FsStateBackend  MemoryStateBackend  RocksdbStateBackend<br></b></font></p><p><font size=3><b>新版本中，FsStateBackend和MemoryStateBackend整合成了HashMapStateBackend，<br>而且HashMapStateBackend和EmbeddedRocksDBStateBackend所生成的快照文件也统一了格式，因而在job重新部署或者版本升级时，可以任意替换StateBackend<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>如需使用rocksdb-backend，需要引入依赖</td></tr></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>状态后端的配置</td></tr></table><ul><li><font size=3><b>可以在代码中配置<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(...);</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>可以在flink-conf.yaml中配置<br>  </b></font></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">hashmap</span></span><br><span class="line"></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://192.168.0.219/flink/checkpoints</span></span><br></pre></td></tr></table></figure><h3 id="9-2-2-两种状态后端的区别"><a href="#9-2-2-两种状态后端的区别" class="headerlink" title="9.2.2 两种状态后端的区别"></a>9.2.2 两种状态后端的区别</h3><p><font color=#0066FF size=3><b>HashMapStateBackend：</b></font></p><ul><li><font size=3><b>数据状态是以java对象形式存储在heap内存中；</b></font></li><li><font size=3><b>内存空间不够时，也会溢出一部分数据到本地磁盘文件；</b></font></li><li><font size=3><b>可以支撑大规模的状态数据；(只不过在状态数据规模超出内存空间时，读写效率就会明显降低)<br>  </b></font></li></ul><p><font color=#0066FF size=3><b>EmbeddedRocksDBStateBackend：</b></font></p><ul><li><font size=3><b>状态数据是交给rocksdb来管理；</b></font></li><li><font size=3><b>Rocksdb中的数据是以序列化的kv字节进行存储；</b></font></li><li><font size=3><b>Rocksdb中的数据，有内存缓存的部分，也有磁盘文件的部分；</b></font></li><li><font size=3><b>Rocksdb的磁盘文件数据读写速度相对还是比较快的，所有在支持超大规模状态数据时，数据的读写效率不会有太大的降低；<br>  </b></font></li></ul><p><font color=Red size=3><b>注意：上述2中状态后端，在生成checkpoint快照文件时，生成的文件格式是完全一致的；<br>所以，用户的Flink程序在更改状态后端后，重启时依然可以加载和恢复此前的快照文件数据；</b></font></p><h2 id="9-3-状态数据结构"><a href="#9-3-状态数据结构" class="headerlink" title="9.3 状态数据结构"></a>9.3 状态数据结构</h2><p><font color=Red size=3><b>不同状态数据结构API详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=72&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=72&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P82<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>ValueState</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>MapState</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>ListState</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>ReducingState</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>AggregatingState</td></tr></table><p><font size=3><b>算子状态(Operator state)只有List这1种数据结构，键控状态(Keyed state)5种数据结构都有。<br></b></font></p><h2 id="9-4-算子状态-Operator-state"><a href="#9-4-算子状态-Operator-state" class="headerlink" title="9.4 算子状态(Operator state)"></a>9.4 算子状态(Operator state)</h2><ul><li><font size=3><b>算子状态，是每个subtask自己持有一份独立的状态数据(但如果在失败恢复后，算子并行度发生变化，则状态将在新的subtask之间均匀分配)；</b></font></li><li><font size=3><b>算子函数实现CheckpointedFunction后，即可使用算子状态；</b></font></li><li><font size=3><b>算子状态，通常用于source算子中，其它场景下建议使用KeyedState(键控状态)；<br>  </b></font></li></ul><p><font size=3><b>算子状态，在逻辑上，由算子task下所有subtask共享；<br>如何理解：正常运行时，subtask自己读写自己的状态数据；而一旦job重启且带状态算子发生了并行度的变化，则之前的状态数据将在新的一批subtask间均匀分配<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_23_State_OperatorState_Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line">        env.setRuntimeMode(RuntimeExecutionMode.STREAMING);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 要让Flink对托管状态数据进行容错，还需要开启快照机制</span></span><br><span class="line">        <span class="comment">// 开启状态数据的checkpoint机制(快照的周期，快照的模式)</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">1000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        <span class="comment">// 开启快照后，就需要指定快照数据的持久化存储位置</span></span><br><span class="line">        <span class="comment">/*env.getCheckpointConfig().setCheckpointStorage(new URI(&quot;hdfs://114.116.0.219:9000/checkpoint&quot;));*/</span></span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///d:/checkpoint/&quot;</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启 task级别故障自动failover</span></span><br><span class="line">        <span class="comment">// env.setRestartStrategy(RestartStrategies.noRestart()); 默认是 不会自动failover，一个task故障了，整个job就失败了</span></span><br><span class="line">        <span class="comment">// 使用的策略是：固定重启上限和重启时间间隔</span></span><br><span class="line">        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(<span class="number">3</span>, <span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//a</span></span><br><span class="line">        DataStreamSource&lt;String&gt; source = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需要使用map算子来达到一个效果</span></span><br><span class="line">        <span class="comment">//每来一条数据(字符串)，输出 该条字符串拼接此前到达过的所有字符串</span></span><br><span class="line">        source.map(<span class="keyword">new</span> <span class="title class_">StateMapFunction</span>()).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 要使用Operator state，需要用户去实现CheckpointedFunction</span></span><br><span class="line"><span class="comment"> * 然后在其中的方法initializeState中，去拿到operator state存储器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StateMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String, String&gt;, CheckpointedFunction &#123;</span><br><span class="line"></span><br><span class="line">    ListState&lt;String&gt; listState;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 正常的MapFunction的处理逻辑方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将本条数据，插入到状态存储器中</span></span><br><span class="line">        listState.add(value);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 然后拼接历史以来的字符串</span></span><br><span class="line">        Iterable&lt;String&gt; strings = listState.get();</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        <span class="keyword">for</span> (String string : strings) &#123;</span><br><span class="line">            sb.append(string);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 系统对状态数据做快照(持久化)时会调用的方法，用户利用这个方法，在持久化前，对状态数据做一些操作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">snapshotState</span><span class="params">(FunctionSnapshotContext functionSnapshotContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 算子任务在启动之初，会调用下面的方法，来为用户进行状态数据初始化</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initializeState</span><span class="params">(FunctionInitializationContext functionInitializationContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从方法中提供的context中拿到一个算子状态存储器</span></span><br><span class="line">        <span class="type">OperatorStateStore</span> <span class="variable">operatorStateStore</span> <span class="operator">=</span> functionInitializationContext.getOperatorStateStore();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 算子状态存储器，只提供List数据结构来为用户存储数据</span></span><br><span class="line">        ListStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> <span class="title class_">ListStateDescriptor</span>&lt;&gt;(<span class="string">&quot;strings&quot;</span>, String.class); <span class="comment">// 定义一个状态存储结构描述器</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// getListState方法，在task失败后，task自动重启时，会帮用户加载最近一次的快照状态数据</span></span><br><span class="line">        <span class="comment">// 如果是job重启，则不会自动加载此前的快照状态数据，需要人工指定checkpoint数据</span></span><br><span class="line">        listState = operatorStateStore.getListState(stateDescriptor); <span class="comment">// 在状态存储器上调用get方法，得到具体结构的状态管理器</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * unionListState 和普通ListState区别：</span></span><br><span class="line"><span class="comment">         * unionListState的快照存储系统，在系统重启后，加载状态数据时，它的重分配模式为：广播模式，即重启后的每一个subTask都有一份完整的数据</span></span><br><span class="line"><span class="comment">         * ListState的快照存储数据，在系统重启后，加载状态数据时，它的重分配模式为：round-robin(轮询)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">// ListState&lt;String&gt; unionListState = operatorStateStore.getUnionListState(stateDescriptor);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="9-5-键控状态-Keyed-state"><a href="#9-5-键控状态-Keyed-state" class="headerlink" title="9.5 键控状态(Keyed state)"></a>9.5 键控状态(Keyed state)</h2><ul><li><font size=3><b>键控状态，只能应用于keyby后的处理算子中；</b></font></li><li><font size=3><b>算子为每一个key绑定一份独立的状态数据；<br>  </b></font></li></ul><p><font color=Red size=3><b>如果要使用Keyed state，需要使用RichFunction，因为要其中的getRuntimeContext方法拿到状态管理器。<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131913921.jpeg" alt="Flink图35"></p><p><font color=Red size=3><b>状态数据在重启后的重分配影响：<br>Operator state   会有影响<br>Keyed state      不会有影响<br></b></font></p><h2 id="9-6-广播状态-broadcast-state"><a href="#9-6-广播状态-broadcast-state" class="headerlink" title="9.6 广播状态(broadcast state)"></a>9.6 广播状态(broadcast state)</h2><p><font size=3><b>前面第3章讲过<br><a href="http://124.223.222.59:8090/archives/3flinkduo-liu-cao-zuo-api#toc-head-8">http://124.223.222.59:8090/archives/3flinkduo-liu-cao-zuo-api#toc-head-8</a> <br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=87&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=87&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P87<br></b></font></p><h2 id="9-7-状态的TTL管理-time-to-live"><a href="#9-7-状态的TTL管理-time-to-live" class="headerlink" title="9.7 状态的TTL管理(time to live)"></a>9.7 状态的TTL管理(time to live)</h2><h3 id="9-7-1-TTL基本概念"><a href="#9-7-1-TTL基本概念" class="headerlink" title="9.7.1 TTL基本概念"></a>9.7.1 TTL基本概念</h3><ul><li><font size=3><b>Flink可以对状态数据进行存活时长管理，即“新陈代谢”；</b></font></li><li><font size=3><b>淘汰的机制主要是基于存活时间；</b></font></li><li><font size=3><b>存活时间的计算可以在数据被读、写的时候重置；</b></font></li><li><font size=3><b>TTL存活管理粒度是到元素级的(如ListState中的每个元素，MapState中的每个entry)<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>代码示例</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取一个 单值 结构的状态存储器，并设置TTL参数</span></span><br><span class="line"><span class="type">StateTtlConfig</span> <span class="variable">ttlConfig</span> <span class="operator">=</span> StateTtlConfig.newBuilder(Time.milliseconds(<span class="number">5000</span>)) <span class="comment">//配置数据的存活时长为5s</span></span><br><span class="line">        .setTtl(Time.milliseconds(<span class="number">4000</span>)) <span class="comment">//配置数据的存活时长为4s</span></span><br><span class="line">        .updateTtlOnCreateAndWrite()  <span class="comment">//当创建和写入的时候，导致该条数据的ttl计时重置</span></span><br><span class="line">        .updateTtlOnReadAndWrite()    <span class="comment">//读、写都导致该条数据的ttl计时重置</span></span><br><span class="line">        <span class="comment">//设置状态的可见性</span></span><br><span class="line">        <span class="comment">//NeverReturnExpired：永远不返回已过期的数据</span></span><br><span class="line">        <span class="comment">//ReturnExpiredIfNotCleanedUp：如果还没有被清除则会返回已过期的数据</span></span><br><span class="line">        .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">        .setStateVisibility(StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp)</span><br><span class="line">        <span class="comment">//ttl计时的时间语义 设置为处理时间</span></span><br><span class="line">        .setTtlTimeCharacteristic(StateTtlConfig.TtlTimeCharacteristic.ProcessingTime)</span><br><span class="line">        .useProcessingTime()  <span class="comment">//ttl计时的时间语义 设置为处理时间</span></span><br><span class="line">        <span class="comment">/*.cleanupIncrementally(1000, true)*/</span> <span class="comment">//增量清理(每当一条状态数据被访问，就会检查这条状态数据的ttl是否超时，是就删除)</span></span><br><span class="line">        .cleanupFullSnapshot()  <span class="comment">//全量快照清理策略(在checkpoint的时候，保存到快照文件中的只包含未过期的状态数据，但是它不会清理算子本地的状态数据)</span></span><br><span class="line">        .cleanupInRocksdbCompactFilter(<span class="number">1000</span>)  <span class="comment">//也属于增量清理，不过只对RocksdbStateBackend有效，在rockdb的compact机制中添加过期数据过滤器，以在compact过程中清理掉过期状态数据</span></span><br><span class="line">        <span class="comment">/*.disableCleanupInBackground()*/</span></span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>注意，上面代码中有3种设置快照清理策略：cleanupIncrementally()、cleanupFullSnapshot()、cleanupInRocksdbCompactFilter()，如果都不设置，就会有一个问题，那就是刚开始运行时会发生一次清理，然后再也不会清理了，而如果显式的设置了其中一个策略，就不会有这个问题。<br>问题解释：当这3种策略都不设置时，本地状态数据的过期清理默认策略就是cleanupIncrementally(5, false)<br></b></font></p><h3 id="9-7-2-TTL相关参数和机制解析"><a href="#9-7-2-TTL相关参数和机制解析" class="headerlink" title="9.7.2 TTL相关参数和机制解析"></a>9.7.2 TTL相关参数和机制解析</h3><p><font size=3><b>TTL的相关配置参数及其内含的机制，全部封装在StateTtlConfig类<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> UpdateType updateType;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> StateVisibility stateVisibility;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TtlTimeCharacteristic ttlTimeCharacteristic;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Time ttl;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> CleanupStrategies cleanupStrategies;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="title function_">StateTtlConfig</span><span class="params">(</span></span><br><span class="line"><span class="params">        UpdateType updateType,</span></span><br><span class="line"><span class="params">        StateVisibility stateVisibility,</span></span><br><span class="line"><span class="params">        TtlTimeCharacteristic ttlTimeCharacteristic,</span></span><br><span class="line"><span class="params">        Time ttl,</span></span><br><span class="line"><span class="params">        CleanupStrategies cleanupStrategies)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.updateType = checkNotNull(updateType);</span><br><span class="line">    <span class="built_in">this</span>.stateVisibility = checkNotNull(stateVisibility);</span><br><span class="line">    <span class="built_in">this</span>.ttlTimeCharacteristic = checkNotNull(ttlTimeCharacteristic);</span><br><span class="line">    <span class="built_in">this</span>.ttl = checkNotNull(ttl);</span><br><span class="line">    <span class="built_in">this</span>.cleanupStrategies = cleanupStrategies;</span><br><span class="line">    checkArgument(ttl.toMilliseconds() &gt; <span class="number">0</span>, <span class="string">&quot;TTL is expected to be positive.&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>updateType：Ttl计时重启策略，可取值为：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">UpdateType</span> &#123;</span><br><span class="line">       <span class="comment">/** TTL is disabled. State does not expire. */</span></span><br><span class="line">       Disabled,</span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * Last access timestamp is initialised when state is created and updated on every write</span></span><br><span class="line"><span class="comment">        * operation.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line"><span class="comment">//计时从写入或更新时开始(重置)</span></span><br><span class="line">       OnCreateAndWrite,</span><br><span class="line">       <span class="comment">/** The same as &lt;code&gt;OnCreateAndWrite&lt;/code&gt; but also updated on read. */</span></span><br><span class="line"><span class="comment">//和上面一致，不过读取也会导致计时重置</span></span><br><span class="line">       OnReadAndWrite</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>stateVisibility：过期数据可见策略，可取值为：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">StateVisibility</span> &#123;</span><br><span class="line">       <span class="comment">/** Return expired user value if it is not cleaned up yet. */</span></span><br><span class="line"><span class="comment">//可以返回已过期尚未清理的数据</span></span><br><span class="line">       ReturnExpiredIfNotCleanedUp,</span><br><span class="line">       <span class="comment">/** Never return expired user value. */</span></span><br><span class="line"><span class="comment">//不返回已过期的数据</span></span><br><span class="line">       NeverReturnExpired</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>ttlTimeCharacteristic：Ttl计时的时间语义，目前只支持processing time语义<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">TtlTimeCharacteristic</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Processing time, see also &lt;code&gt;</span></span><br><span class="line"><span class="comment">     * org.apache.flink.streaming.api.TimeCharacteristic.ProcessingTime&lt;/code&gt;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ProcessingTime</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><font size=3><b>ttl：数据存活时长，一个简单的长整数参数<br>  </b></font></p></li><li><p><font size=3><b>cleanupStrategies：过期数据清理策略，目前支持的策略有：</b></font></p></li></ul><ol><li><font size=3><b>cleanupIncrementally <br> 增量清除<br>   每当访问状态时，都会驱动一次过期检查(算子注册了很多key的state，一次检查只针对其中一部分)<br>   算子持有一个包含所有key的迭代器，每次检查后，迭代器都会向前advance指定的key数量。 <br>   本策略只针对本地状态空间，且只用于HashMapStateBackend</b></font></li><li><font size=3><b>cleanupFullSnapshot <br> 在进行全量快照(checkpoint)时，清理掉过期数据； <br> 注意：只是在生成的checkpoint数据中不包含过期数据，在本地状态空间中，并没有做清理； <br> 本策略只针对快照生效；</b></font></li><li><font size=3><b>cleanupInRocksdbCompactFilter   <br> 只针对rocksdbStateBackend有效；     <br> 它是利用rocksdb的compact功能，在rocksdb进行compact时，清理掉过期数据；<br> 本策略针对本地状态空间，且只用于EmbeddedRocksDBStateBackend<br> </b></font></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8. Flink 窗口计算API</title>
      <link href="/post/18fe509b.html"/>
      <url>/post/18fe509b.html</url>
      
        <content type="html"><![CDATA[<h1 id="8-Flink-窗口计算API"><a href="#8-Flink-窗口计算API" class="headerlink" title="8. Flink 窗口计算API"></a>8. Flink 窗口计算API</h1><h2 id="8-1-窗口-window-概念"><a href="#8-1-窗口-window-概念" class="headerlink" title="8.1 窗口(window)概念"></a>8.1 窗口(window)概念</h2><p><font size=3><b>窗口，就是把无界的数据流，依据一定规则划分成一段一段的有界数据流；<br>既然划分成有界数据段，通常都是为了”聚合”；<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900703.png" alt="Flink图28"></p><p><font size=3><b>Keyed Window重要特性：任何一个窗口，都绑定在自己所属的key上，不同key的数据肯定不会划分到相同窗口中去！<br></b></font></p><h2 id="8-2-窗口细分类型"><a href="#8-2-窗口细分类型" class="headerlink" title="8.2 窗口细分类型"></a>8.2 窗口细分类型</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>滚动窗口</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900372.png" alt="Flink图29"></p><p><font color=OrangeRed size=3><b>滚动窗口，是滑动窗口的特例，可以用滑动窗口来表达。(窗口长度 &#x3D; 滑动步长)</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>滑动窗口</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900878.png" alt="Flink图30"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>会话窗口</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900486.png" alt="Flink图31"></p><p><font color=OrangeRed size=3><b>会话窗口的触发机制，当Flink检查到前后两条数据中间间隔的时长超过了你指定的间隔时长，那么后面的数据就会进入一个新的窗口，而前面的窗口就闭合触发计算了。<br></b></font></p><h2 id="8-3-窗口计算API模板"><a href="#8-3-窗口计算API模板" class="headerlink" title="8.3 窗口计算API模板"></a>8.3 窗口计算API模板</h2><p><font size=3><b>下面展示了 Flink 窗口在 keyed streams 和 non-keyed streams 上使用的基本结构。 我们可以看到，这两者唯一的区别仅在于：keyed streams 要调用 keyBy(…)后再调用 window(…) ， 而 non-keyed streams 只用直接调用 windowAll(…)。留意这个区别，它能帮我们更好地理解后面的内容。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>Keyed Windows</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  仅 keyed 窗口需要</span><br><span class="line">       .window(...)              &lt;-  必填项：<span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  可选项：<span class="string">&quot;trigger&quot;</span> (省略则使用默认 trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  可选项：<span class="string">&quot;evictor&quot;</span> (省略则不使用 evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  可选项：<span class="string">&quot;lateness&quot;</span> (省略则为 <span class="number">0</span>)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  可选项：<span class="string">&quot;output tag&quot;</span> (省略则不对迟到数据使用 side output)</span><br><span class="line">       .reduce/aggregate/apply()      &lt;-  必填项：<span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  可选项：<span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>Non-Keyed Windows</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  必填项：<span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  可选项：<span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  可选项：<span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  可选项：<span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  可选项：<span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/apply()      &lt;-  必填项：<span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  可选项：<span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>小乱序，利用watermark的容错时间来解决<br>中等乱序，利用窗口允许迟到机制[.allowedLateness(…)] （即使时间到了，触发计算了，但计算完后依然保留桶，目的是等待可能迟到的数据，重新计算一次）<br>大乱序，利用窗口中的迟到数据侧流输出机制[.sideOutputLateData(…)] (如果有迟到严重的数据，那用第二种方案也不会触发计算，因为桶不可能无限保留，那么这种迟到严重的数据，Flink也不会至于丢掉，Flink会把它输出到侧流中去，用户可以get侧流中的迟到严重的数据，至于拿到之后怎么办，就由用户自己搞了)<br></b></font></p><h2 id="8-4-窗口指派API"><a href="#8-4-窗口指派API" class="headerlink" title="8.4 窗口指派API"></a>8.4 窗口指派API</h2><p><font color=Red size=3><b>详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=72&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=72&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P73<br></b></font></p><h2 id="8-5-窗口聚合算子"><a href="#8-5-窗口聚合算子" class="headerlink" title="8.5 窗口聚合算子"></a>8.5 窗口聚合算子</h2><h3 id="8-5-1-两类窗口聚合算子的区别"><a href="#8-5-1-两类窗口聚合算子的区别" class="headerlink" title="8.5.1 两类窗口聚合算子的区别"></a>8.5.1 两类窗口聚合算子的区别</h3><table><tr><td bgcolor=Gainsboro><font size=4><b>窗口聚合算子，整体上分两类</td></tr></table><ul><li><font size=3><b>增量聚合算子，如min、max、minBy、maxBy、sum、reduce、aggregate</b></font></li><li><font size=3><b>全量聚合算子，如apply、process<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>两类聚合算子的底层区别</td></tr></table><ul><li><font size=3><b>增量聚合：一次取一条数据，用聚合函数对中间累加器更新；窗口触发时，取累加器输出结果；</b></font></li><li><font size=3><b>全量聚合：数据”攒”在状态容器中，窗口触发时，把整个窗口的数据交给聚合函数；<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900505.png" alt="Flink图32"></p><h3 id="8-5-2-各种聚合算子代码示例"><a href="#8-5-2-各种聚合算子代码示例" class="headerlink" title="8.5.2 各种聚合算子代码示例"></a>8.5.2 各种聚合算子代码示例</h3><p><font color=Red size=3><b>详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=72&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=72&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P69 - P72<br></b></font></p><h2 id="8-6-数据延迟处理"><a href="#8-6-数据延迟处理" class="headerlink" title="8.6 数据延迟处理"></a>8.6 数据延迟处理</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>延迟处理的方案</td></tr></table><ul><li><font size=3><b>小乱序，用watermark容错，(减慢时间的推进，让本已经迟到的数据被认为没有迟到)</b></font></li><li><font size=3><b>中等乱序，用allowedLateness（允许一定限度内的迟到，并对迟到数据重新触发窗口计算）</b></font></li><li><font size=3><b>大乱序，用sideOutputLateData (将超出allowedLateness的迟到数据输出到一个侧流中)<br>  </b></font></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>代码示例</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;String&gt; stream = watermarkedBeanStream</span><br><span class="line">        .windowAll(SlidingEventTimeWindows.of(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .allowedLateness(Time.milliseconds(<span class="number">2000</span>))  <span class="comment">//允许迟到2秒，默认是0</span></span><br><span class="line">        .sideOutputLateData(lateTag)    <span class="comment">//超过迟到最大允许时间的数据，收集到侧流</span></span><br><span class="line">        .apply();</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取侧流，做一些自己的补救</span></span><br><span class="line">stream.getSideOutput(lateTag).print();</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>注意正确理解延迟时间！<br>如: allowedLateness(2s) 表示：<br>如果watermark(此刻的事件时间)推进到了A窗口结束点后2s，还来A窗口的数据，就算迟到，不会再触发A窗口的计算，而是输出到侧流。<br></b></font></p><h2 id="8-7-窗口触发机制"><a href="#8-7-窗口触发机制" class="headerlink" title="8.7 窗口触发机制"></a>8.7 窗口触发机制</h2><p><font size=3><b>窗口计算的触发，是由Trigger类来决定；<br>Flink中为各类内置的WindowAssigner都设计了对应的默认Trigger；<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131900409.png" alt="Flink图33"></p><p><font size=3><b>一般情况下不需要自己去重写Trigger，除非有特别的需求；<br>Evictor是窗口触发前，或者触发后，对窗口中的数据移除的机制；<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7. Flink 事件时间语义中的watermark</title>
      <link href="/post/84b30968.html"/>
      <url>/post/84b30968.html</url>
      
        <content type="html"><![CDATA[<h1 id="7-Flink-事件时间语义中的watermark"><a href="#7-Flink-事件时间语义中的watermark" class="headerlink" title="7. Flink 事件时间语义中的watermark"></a>7. Flink 事件时间语义中的watermark</h1><h2 id="7-1-事件时间推进的尴尬"><a href="#7-1-事件时间推进的尴尬" class="headerlink" title="7.1 事件时间推进的尴尬"></a>7.1 事件时间推进的尴尬</h2><p><font size=3><b>由于在事件时间语义的世界观中，时间是由流入系统的数据(事件)而推进的；<br>而事件时间，并不能像处理时间那样，由宇宙客观规律以恒定速度，不可停滞地推进；<br>从而，在事件时间语义的世界观中，时间的推进，并不是一件显而易见的事情；<br></b></font></p><ul><li><font size=3><b>场景1：<br> </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131843433.png" alt="Flink图14"></p><p><font size=3><b>数据时间存在乱序的可能性，但时光不能倒流啊！<br></b></font></p><ul><li><font size=3><b>场景2：<br> </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131844800.png" alt="Flink图15"></p><p><font size=3><b>下游分区接受上游多个分区的数据，数据时间错落有致，那以谁为准？<br></b></font></p><h2 id="7-2-watermark推进时间"><a href="#7-2-watermark推进时间" class="headerlink" title="7.2 watermark推进时间"></a>7.2 watermark推进时间</h2><ul><li><font size=3><b>所谓watermark，就是在事件时间语义世界观中，用于单调递增向前推进时间的一种机制；</b></font></li><li><font size=3><b>它的核心机制是在数据流中周期性的插入一种时间戳单调递增的特殊数据元素(watermark)，来不可逆转的在整个数据流中进行时间的推进；</b></font></li><li><font size=3><b>watermark中的时间戳到了哪里，算子的时间就推进到了哪里；<br>  </b></font></li></ul><p><font color=OrangeRed size=3><b>Watermark是从某一个算子实例(源头)开始，根据数据中的事件时间，来周期性的产生，并插入到数据流中，持续不断的往下游传递，以推进整个计算链条上各个算子实例的时间！<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//watermark的生成周期(默认值即为200ms)</span></span><br><span class="line">env.getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>Watermark源码片段</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">Watermark</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Watermark</span><span class="params">(<span class="type">long</span> timestamp)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getTimestamp</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getFormattedTimestamp</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ((SimpleDateFormat)TS_FORMATTER.get()).format(<span class="keyword">new</span> <span class="title class_">Date</span>(<span class="built_in">this</span>.timestamp));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>watermark，本质上也是Flink中各算子间流转的一种数据，只不过与用户的数据不同，它是Flink内部自动产生并插入到数据流的；</b></font></li><li><font size=3><b>它本身所携带的信息很简单，就是一个时间戳！<br>  </b></font></li></ul><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>watermark产生源头示意图</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>初始状态</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131845786.png" alt="Flink图18"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>收到一条新数据后</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131845017.png" alt="Flink图19"></p><p><font size=3><b>简单说，就是在watermark产生的源头算子实例中，实例程序会用一个定时器，去<u>周期性</u>的检查截止到此刻所收到过的数据的<u>事件时间最大值</u>，如果超过了之前的最大值，则将这个最大值更新为最新的watermark，并向下游传递；<br></b></font></p><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>watermark往下游推进的示意图</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>初始状态</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131847396.png" alt="Flink图20"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>新的上游watermark即将到达</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131847386.png" alt="Flink图21"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>上游新的watermark最终产生的效果</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131847127.png" alt="Flink图22"></p><p><font size=3><b>一个下游算子实例，如果消费着多个上游算子实例：</b></font><br><font color=OrangeRed size=3><b>则选择”Min(上游各实例的最新watermark)”作为自己当前的watermark；</b></font><br><font size=3><b>并将自己最新的watermark往下游传播；</b></font></p><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>watermark从源头往下游推进完整示意图</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131852023.png" alt="Flink图16"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131852678.png" alt="Flink图24"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131852782.png" alt="Flink图23"></p><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>watermark推进事件时间与窗口计算的结合示例</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131852114.png" alt="Flink图17"></p><p><font color=Red size=3><b>Flink产生watermark时，下游算子的watermark是以上游watermark的最小值为准，这样就会产生延迟。延迟最大的程度是最慢的那个分区。<br>如果最慢的watermark数据丢失了，下游的watermark永远是之前的最小值，不再更新，那么这样的话，未来数据是不是永远不会输出?<br>为了解决这个问题，Flink对窗口算子有一个机制：watermark-idle-timeout，如果有一条分支，迟迟没有数据过来，超过指定时间，那么Flink会主动推进时间。<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131851163.png" alt="Flink图25"></p><p><font size=3><b>从这里可以看出，如果某一个上游实例watermark一直停滞，则会导致下游实例的watermark也一直停滞，从而延迟窗口计算的触发，并造成大量数据的积压。<br>对此，Flink提供了一个机制：设置时间watermark的idle超时(在源头设置)：如果某个分区超过idle时长没有收到数据，则会自主往前推进时间。</b></font></p><h2 id="7-3-内置watermark生成策略"><a href="#7-3-内置watermark生成策略" class="headerlink" title="7.3 内置watermark生成策略"></a>7.3 内置watermark生成策略</h2><p><font size=3><b>在Flink1.12以后，watermark默认是按固定频率周期性的产生；<br>此前有两种生成策略:</b></font></p><ul><li><font size=3><b>AssignerWithPeriodicWatermarks  周期性生成watermark</b></font></li><li><font size=3><b>AssignerWithPunctuatedWatermarks[<del>已过期</del>]  按指定标记性事件生成watermark<br>  </b></font></li></ul><p><font color=OrangeRed size=3><b>在Flink1.12后，watermark默认是按固定频率周期性的产生，这个产生wartermark的源点不一定是source，也可以选择计算逻辑中任何一个环节产生。</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>新版本API内置的Watermark策略</td></tr></table><ul><li><font size=3><b>紧跟最大事件时间的watermark生成策略(完全不容忍乱序，只要迟到，就丢弃)<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WatermarkStrategy.forMonotonousTimestamps();</span><br></pre></td></tr></table></figure><ul><li><font size=3><b>允许乱序的watermark生成策略(最大事件数据 - 容错时间)<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">10</span>));<span class="comment">//根据实际数据的最大乱序情况来设置</span></span><br></pre></td></tr></table></figure><ul><li><font size=3><b>自定义watermark生成策略<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WatermarkStrategy.forGenerator(<span class="keyword">new</span> <span class="title class_">WatermarkGenerator</span>()&#123;...&#125;);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>Monotonous策略的时间推进示意图</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131853531.png" alt="Flink图27"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>BoundedOutOfOrderness策略的时间推进示意图 (watermark = eventTime - 5)</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131853785.png" alt="Flink图27"></p><p><font color=Red size=3><b>如果让watermark直接紧跟收到的数据的最大事件时间，那么会有大量迟到数据被认为是过期的，所以应该让时间的推进比收到的数据的最大事件时间慢一点。比如这里的例子，它允许的最大乱序是5。但即使这样，还是不能彻底解决乱序问题，它只是起一个缓冲作用。<br></b></font></p><h2 id="7-4-设置watermark策略的模板代码"><a href="#7-4-设置watermark策略的模板代码" class="headerlink" title="7.4 设置watermark策略的模板代码"></a>7.4 设置watermark策略的模板代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_19_Watermark_API_Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1,e01,168673487846,pg01</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line">        <span class="comment">//给上面的source算子，添加watermark生成策略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//策略1：WatermarkStrategy.noWatermarks()  不生成watermark，禁用了事件时间的推进机制</span></span><br><span class="line">        <span class="comment">//策略2：WatermarkStrategy.forMonotonousTimestamps()  紧跟最大事件时间</span></span><br><span class="line">        <span class="comment">//策略3：WatermarkStrategy.forBoundedOutOfOrderness()  允许乱序的watermark生成策略</span></span><br><span class="line">        <span class="comment">//策略4：WatermarkStrategy.forGenerator()  自定义watermark生成算法</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构造一个watermark的生成策略对象(算法策略，及事件时间的抽取方法)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        WatermarkStrategy&lt;String&gt; watermarkStrategy = WatermarkStrategy</span><br><span class="line">                .&lt;String&gt;forBoundedOutOfOrderness(Duration.ofMillis(<span class="number">0</span>)) <span class="comment">//允许乱序的算法策略</span></span><br><span class="line">                .withTimestampAssigner(<span class="keyword">new</span> <span class="title class_">SerializableTimestampAssigner</span>&lt;String&gt;() &#123; <span class="comment">// 时间抽取方法</span></span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(String s, <span class="type">long</span> l)</span> &#123;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> Long.parseLong(s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">2</span>]);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .withIdleness(Duration.ofMillis(<span class="number">2000</span>)); <span class="comment">// 防止上游某些分区的水位线不推进导致下游的窗口一直不触发(这个分区很久都没数据)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//然后将构造好的watermark策略对象，分配给流(source算子)</span></span><br><span class="line">        stream.assignTimestampsAndWatermarks(watermarkStrategy);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-5-内置watermark的源码分析"><a href="#7-5-内置watermark的源码分析" class="headerlink" title="7.5 内置watermark的源码分析"></a>7.5 内置watermark的源码分析</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>WatermarkGenerator接口</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The &#123;<span class="doctag">@code</span> WatermarkGenerator&#125; generates watermarks either based on events or periodically (in a</span></span><br><span class="line"><span class="comment"> * fixed interval).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;&lt;b&gt;Note:&lt;/b&gt; This WatermarkGenerator subsumes the previous distinction between the &#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment"> * AssignerWithPunctuatedWatermarks&#125; and the &#123;<span class="doctag">@code</span> AssignerWithPeriodicWatermarks&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called for every event, allows the watermark generator to examine and remember the event</span></span><br><span class="line"><span class="comment">     * timestamps, or to emit a watermark based on the event itself.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">onEvent</span><span class="params">(T event, <span class="type">long</span> eventTimestamp, WatermarkOutput output)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called periodically, and might emit a new watermark, or not.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;The interval in which this method is called and Watermarks are generated depends on &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * ExecutionConfig#getAutoWatermarkInterval()&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>BoundedOutOfOrdernessWatermarks乱序水印实现</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A WatermarkGenerator for situations where records are out of order, but you can place an upper</span></span><br><span class="line"><span class="comment"> * bound on how far the events are out of order. An out-of-order bound B means that once an event</span></span><br><span class="line"><span class="comment"> * with timestamp T was encountered, no events older than &#123;<span class="doctag">@code</span> T - B&#125; will follow any more.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The watermarks are generated periodically. The delay introduced by this watermark strategy is</span></span><br><span class="line"><span class="comment"> * the periodic interval length, plus the out-of-orderness bound.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BoundedOutOfOrdernessWatermarks</span>&lt;T&gt; <span class="keyword">implements</span> <span class="title class_">WatermarkGenerator</span>&lt;T&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The maximum timestamp encountered so far. */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> maxTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The maximum out-of-orderness that this watermark generator assumes. */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> outOfOrdernessMillis;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a new watermark generator with the given out-of-orderness bound.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> maxOutOfOrderness The bound for the out-of-orderness of the event timestamps.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">BoundedOutOfOrdernessWatermarks</span><span class="params">(Duration maxOutOfOrderness)</span> &#123;</span><br><span class="line">        checkNotNull(maxOutOfOrderness, <span class="string">&quot;maxOutOfOrderness&quot;</span>);</span><br><span class="line">        checkArgument(!maxOutOfOrderness.isNegative(), <span class="string">&quot;maxOutOfOrderness cannot be negative&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">this</span>.outOfOrdernessMillis = maxOutOfOrderness.toMillis();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// start so that our lowest watermark would be Long.MIN_VALUE.</span></span><br><span class="line">        <span class="built_in">this</span>.maxTimestamp = Long.MIN_VALUE + outOfOrdernessMillis + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onEvent</span><span class="params">(T event, <span class="type">long</span> eventTimestamp, WatermarkOutput output)</span> &#123;</span><br><span class="line">        <span class="comment">//视当前的事件时间戳，更新(或不更新)maxTimestamp</span></span><br><span class="line">        maxTimestamp = Math.max(maxTimestamp, eventTimestamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> &#123;</span><br><span class="line">        <span class="comment">//以当前eventTimeStamp - 乱序延迟数 - 1，作为生成的watermark值</span></span><br><span class="line">        output.emitWatermark(<span class="keyword">new</span> <span class="title class_">Watermark</span>(maxTimestamp - outOfOrdernessMillis - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=64&spm_id_from=pageDriver&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=64&amp;spm_id_from=pageDriver&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P64<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6. Flink 时间语义</title>
      <link href="/post/2c1a1676.html"/>
      <url>/post/2c1a1676.html</url>
      
        <content type="html"><![CDATA[<h1 id="6-Flink-时间语义"><a href="#6-Flink-时间语义" class="headerlink" title="6. Flink 时间语义"></a>6. Flink 时间语义</h1><h2 id="6-1-三种时间概念"><a href="#6-1-三种时间概念" class="headerlink" title="6.1 三种时间概念"></a>6.1 三种时间概念</h2><p><b>在实时流式计算中，”时间”是一个能影响计算结果的非常重要因素！<br>试想场景：每隔1分钟计算一次最近10分钟的活跃用户量；  </b></p><p><b>假设此刻的时间是13:10，要计算的活跃用户量时间段为：[13:00, 13:10）  </b></p><p><b>有一条行为日志中记录的用户行为时间是12:59，但到达Flink的计算程序已经是13:02，那么。这个用户是否要纳入本次计算的结果中呢？看如何定义：</b></p><ol><li><b>如果时段[13:00, 13:10)定义的是用户行为的发生时间(数据中的业务时间)，则不应纳入；</b></li><li><b>如果时段[13:00, 13:10)定义的是计算时的时间，则应该纳入；</b></li></ol><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131838436.jpeg" alt="Flink图13"></p><p><font color=#0066FF size=3><b>Flink内部为了直观地统一计算时所用的时间标准，特制定了两种时间语义：</b></font></p><ul><li><font color=#0066FF size=3><b>processing time  处理时间</b></font></li><li><font color=#0066FF size=3><b>event time 事件时间</b></font></li><li><font color=#0066FF size=3><b>Ingestion time 注入时间<br>  </b></font></li></ul><p><font color=OrangeRed size=3><b>时间语义注意影响”窗口计算”；<br></b></font></p><h2 id="6-1-两种时间语义"><a href="#6-1-两种时间语义" class="headerlink" title="6.1 两种时间语义"></a>6.1 两种时间语义</h2><p><font size=3><b>时间语义，是Flink中用于时间推进和时间判断的机制；<br>时间推进和时间判断，以什么为标准，就产出了两种不同的时间语义；</b></font></p><ul><li><p><font size=3><b>以processing time为依据，则叫做处理时间语义</b></font></p></li><li><p><font size=3><b>以event time为依据，则叫做事件时间语义</b></font></p></li></ul><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>时间语义的设计意义</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">process(EventLog eventLog)&#123;</span><br><span class="line"><span class="type">Long</span> <span class="variable">eventTime</span> <span class="operator">=</span> eventLog.getTimestamp();</span><br><span class="line"><span class="type">Long</span> <span class="variable">processTime</span> <span class="operator">=</span> System.currentMillimise();</span><br><span class="line"><span class="comment">//用户完全可以自己根据需求中的时间定义来进行相应的计算</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font size=3><b>Flink为什么还要搞出一个”事件时间语义”:时间按数据中的业务时间戳来推进！ <br>主要是，实时流计算中，有大量跟时间相关的统计需求，比如:时间窗口计算，定时器等，而这些需求，如果都让用户像上面的代码那样自己去进行判断、处理，那么它觉得自己的API不够强大！<br>所以，Flink想在API的层面，将两类时间定义的计算需求进行API层面的统一，它才搞出这么一种”事件时间语义”，有了这种语义，那么，处理时间和事件时间都可以看成”时间” <br>用户在不同时间定义下，要进行一个定时动作，就不需要再像上面的代码那样去进行各种判断，而是一个统一的动作，到XXX时间，给我做个什么事！<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">process(EventLog eventlog, TimeStamp timestamp)&#123;</span><br><span class="line"><span class="comment">//不管需求是需要用到哪种时间来计算，用户代码只需要看到一个timestamp了</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>代码中的timestamp到底是事件时间，还是处理时间，取决于环境中设置的”时间语义”<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>处理时间(processing time)语义</td></tr></table><p><font size=3><b>Processing Time是指数据被Operator处理时所在机器的系统时间。<br>处理时间遵循客观世界中时间的特性：单调递增，恒定速度，永不停滞，永不回退；<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>事件时间(event time)语义</td></tr></table><p><font size=3><b>Event Time是指在数据本身的业务时间(如用户行为日志中的用户行为时间戳)；<br>Event Time语义中，时间的推进完全由流入Flink系统的数据来驱动；<br>++数据中的业务时间推进到哪，Flink就认为自己的时间推进到了哪++<br>它可能停滞，也可能速度不恒定，但也一定是单调递增不可回退！<br></b></font></p><h2 id="6-3-时间语义的设置"><a href="#6-3-时间语义的设置" class="headerlink" title="6.3 时间语义的设置"></a>6.3 时间语义的设置</h2><p><font size=3><b>1.12以前，Flink默认以processing time作为默认的时间语义。<br>1.12及以后，Flink默认以event time作为默认的时间语义。<br>在需要指定时间语义的相关操作(如时间窗口)时，可以通过显式的API来使用特定的时间语义；<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>新版API中指定时间语义</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">1</span>));</span><br><span class="line">keyedStream.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">1</span>));</span><br><span class="line">keyedStream.window(TumblingSlidingEventTimeWindows.of(Time.seconds(<span class="number">5</span>));</span><br><span class="line">keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">1</span>));</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>新版API中禁用时间语义</td></tr></table><p><font size=3><b>如果需要禁用event time机制，则可以通过设置watermark生成频率间隔来实现：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果设置为0，则禁用了watermark的生成，从而失去了event time语义</span></span><br><span class="line">ExecutionConfig.setAutoWatermarkInterval(<span class="type">long</span>);</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>提示：如果需要使用已经过期的ingestion time，可以通过设置恰当的watermark来实现。<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5. Flink 并行度相关API</title>
      <link href="/post/cd6587ef.html"/>
      <url>/post/cd6587ef.html</url>
      
        <content type="html"><![CDATA[<h1 id="5-Flink-并行度相关API"><a href="#5-Flink-并行度相关API" class="headerlink" title="5. Flink 并行度相关API"></a>5. Flink 并行度相关API</h1><h2 id="5-1-基础概念"><a href="#5-1-基础概念" class="headerlink" title="5.1 基础概念"></a>5.1 基础概念</h2><ul><li><strong>用户通过算子API所开发的代码，会被Flink任务提交客户端解析成jobGraph</strong></li><li><strong>然后，jobGraph提交到集群JobManager，转化为ExecutionGraph(并行化后的执行图)</strong></li><li><strong>然后，ExecutionGraph中的各个task会以多并行实例(subTask)部署到taskManager上执行；</strong></li><li><strong>subTask运行的位置时taskManager所提供的槽位(task slot)，槽位简单理解就是线程；</strong></li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>重要提示</td></tr></table><ul><li><strong>一个算子的逻辑，可以封装在一个独立的task中(可以有多个运行时实例：subTask)；</strong></li><li><strong>也可以把多个算子的逻辑chain在一起后封装在一个独立的task中(可以有多个运行时实例：subTask)</strong></li></ul><p><font color=Red size=3><b>同一个task的多个并行实例，不能放在同一个taskslot上运行。<br>一个taskslot，可以运行多个不同task的各自1个并行实例。<br>job中只要有一个task的并行度超过集群可用的总槽位数，这个job就会提交失败。<br>所谓的task，就是对算子的封装，用户写的这些算子是用来表达计算逻辑的，本身是不能被执行的，真正拿去执行的时候，Flink会从相应的算子中抽出相应的逻辑，然后封装在task(一个类)中，每一个算子都可以成为一个独立的task。<br>而task的一个并行实例，在Flink中就叫subTask。<br>如果一个算子(不是那种固定并行度的算子)没有指定并行度，就会使用环境中的默认并行度参数(cpu核数)<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131826825.png" alt="Flink图11"></p><h2 id="5-2-task与算子链-operator-chain"><a href="#5-2-task与算子链-operator-chain" class="headerlink" title="5.2 task与算子链(operator chain)"></a>5.2 task与算子链(operator chain)</h2><p><strong>把多个算子的逻辑，串行在一个Task中调用，这就叫做operatorChain(算子链)。</strong></p><p><strong>上下游算子，能否chain在一起，放在一个Task中，取决于如下3个条件：</strong></p><ul><li><strong>能oneToOne数据传输</strong></li><li><strong>并行度相同</strong></li><li><strong>属于相同的slotSharingGroup</strong></li></ul><p><strong>3个条件都满足，才能合并成一个task，否则不能合并成一个task；</strong></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131826170.jpeg" alt="Flink图12"></p><p><font size=3><b>当然，Flink提供了相关的API，来让用户可以根据自己的需求，进行灵活的算子链合并或拆分；<ul><li><strong>setParallelism  设置算子的并行度</strong></li><li><strong>slotSharingGroup  设置算子的槽位共享组</strong></li><li><strong>disableChaining  对算子禁用前后链合并</strong></li><li><strong>startNewChain  对算子开启新链(即禁用算子前链合并)</strong></li></ul><h2 id="5-3-分区partition算子"><a href="#5-3-分区partition算子" class="headerlink" title="5.3 分区partition算子"></a>5.3 分区partition算子</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>分区算子：用于指定上游task的各并行subtask与下游的subtask之间如何传输数据</td></tr></table><p><font size=3><b>Flink中，对于上下游subTask之间的数据传输控制，由ChannelSelector策略来控制，而且Flink内针对各种场景，开发了众多ChannelSelector的具体实现<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131826794.png" alt="ChannelSelector"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>设置数据传输策略时，不需要显式指定partitioner，而是调用封装好的算子即可</td></tr></table><table><thead><tr><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>dataStream.global();</td><td>全部发往第1个task</td></tr><tr><td>dataStream.broadcast();</td><td>广播</td></tr><tr><td>dataStream.forward();</td><td>上下游并行度一样时一对于发送</td></tr><tr><td>dataStream.shuffle();</td><td>随机均匀分配</td></tr><tr><td>dataStream.rebalance();</td><td>Round-Robin(轮流分配)</td></tr><tr><td>dataStream.recale();</td><td>Local Round-Robin(本地轮流分配)</td></tr><tr><td>dataStream.partitionCustom();</td><td>自定义单播</td></tr><tr><td>dataStream.keyBy(KeySelector);</td><td>根据数据的key的hashcode来进行hash分发</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. Flink 编程 process function</title>
      <link href="/post/dcdcb7ec.html"/>
      <url>/post/dcdcb7ec.html</url>
      
        <content type="html"><![CDATA[<h1 id="4-Flink-编程-process-function"><a href="#4-Flink-编程-process-function" class="headerlink" title="4. Flink 编程 process function"></a>4. Flink 编程 process function</h1><h2 id="4-1-process-function概述"><a href="#4-1-process-function概述" class="headerlink" title="4.1 process function概述"></a>4.1 process function概述</h2><p><font size=3><b>process function相对于前文所述的map、flatMap、filter算子来说，最大的区别就是其让开发人员对数据的处理逻辑拥有更大的自由度；<br>process function把数据及必要的上下文提供给开发人员，然后如何处理，如何返回，全部交给用户来控制；<br>在事件驱动型的应用中，使用最频繁的api往往就是process function；<br>在不同类型的DataStream上，(比如keyedStream、windowedStream、ConnectedStream等)，应用process function时，flink提供了大量不同类型的process function，让其针对不同的DataStream拥有更具针对性的功能。<br></b></font></p><p><font color=Red size=3><b>在不同类型的流上，调用process算子，需要传入的ProcessFunction也不同。<br>ProcessFunction继承RichFunction，可以使用生命周期方法。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>各种算子运算后生成的DataStream类型，及各种DataStream类型之间的相互转换关系</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131743457.jpeg" alt="Flink图10"></p><ol><li><strong>ProcessFunction</strong></li><li><strong>KeyedProcessFunction</strong></li><li><strong>ProcessWindowFunction</strong></li><li><strong>ProcessAllWindowFunction</strong></li><li><strong>CoProcessFunction</strong></li><li><strong>ProcessJoinFunction</strong></li><li><strong>BroadcastProcessFunction</strong></li><li><strong>KeyedBroadcastProcessFunction</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3. Flink 多流操作API</title>
      <link href="/post/e8ed85cc.html"/>
      <url>/post/e8ed85cc.html</url>
      
        <content type="html"><![CDATA[<h1 id="3-Flink-多流操作API"><a href="#3-Flink-多流操作API" class="headerlink" title="3. Flink 多流操作API"></a>3. Flink 多流操作API</h1><h2 id="3-1-split分流操作-已deprecated"><a href="#3-1-split分流操作-已deprecated" class="headerlink" title="3.1 split分流操作[已deprecated]"></a>3.1 split分流操作[已deprecated]</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>split拆分 (DataStream -> SplitStream)</td></tr></table><p><font size=3><b>该方法是将一个DataStream中的数据流打上不同的标签，逻辑的拆分成多个不同类型的流，返回一个新的SplitStream，本质上还是一个数据流，只不过是将流中的数据打上了不同的标签。</p><p>返回的SplitStream已经标记为过时，1.12版本已经删除了split和select方法，可以使用测流输出代替。<br></b></font></p><h2 id="3-2-分流操作-使用测流输出"><a href="#3-2-分流操作-使用测流输出" class="headerlink" title="3.2 分流操作[使用测流输出]"></a>3.2 分流操作[使用测流输出]</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_13_SideOutput_Demo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.setInteger(<span class="string">&quot;rest.port&quot;</span>, <span class="number">8822</span>);</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="string">&quot;file:///d:/ckpt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 需求：将事件进行分流</span></span><br><span class="line"><span class="comment">         * appLaunch事件分到一个流</span></span><br><span class="line"><span class="comment">         * putBack事件分到一个流</span></span><br><span class="line"><span class="comment">         * 其它事件保留在主流</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        SingleOutputStreamOperator&lt;EventLog&gt; processed = streamSource.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;EventLog, EventLog&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> eventLog 输入数据</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> context 上下文，它能提供“侧输出”功能</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> collector 主流输出收集器</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(EventLog eventLog, ProcessFunction&lt;EventLog, EventLog&gt;.Context context, Collector&lt;EventLog&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">if</span> (eventLog.getEventId().equals(<span class="string">&quot;appLaunch&quot;</span>)) &#123;</span><br><span class="line">                    context.output(<span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;EventLog&gt;(<span class="string">&quot;launch&quot;</span>, TypeInformation.of(EventLog.class)), eventLog);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;putBack&quot;</span>.equals(eventLog)) &#123;</span><br><span class="line">                    context.output(<span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;String&gt;(<span class="string">&quot;back&quot;</span>, TypeInformation.of(String.class)), JSON.toJSONString(eventLog));</span><br><span class="line">                &#125;</span><br><span class="line">                collector.collect(eventLog);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 launch测流数据</span></span><br><span class="line">        DataStream&lt;EventLog&gt; launchStream = processed.getSideOutput(<span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;EventLog&gt;(<span class="string">&quot;launch&quot;</span>, TypeInformation.of(EventLog.class)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 back测流数据</span></span><br><span class="line">        DataStream&lt;String&gt; backStream = processed.getSideOutput(<span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;String&gt;(<span class="string">&quot;back&quot;</span>, TypeInformation.of(String.class)));</span><br><span class="line"></span><br><span class="line">        launchStream.print();</span><br><span class="line">        backStream.print();</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-3-sideOutput测流输出"><a href="#3-3-sideOutput测流输出" class="headerlink" title="3.3 sideOutput测流输出"></a>3.3 sideOutput测流输出</h2><p><font size=3><b>以下function函数，支持将特定数据输出到测流中。</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>KeyedCoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> OutputTag&lt;String&gt; outputTag = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;String&gt;(<span class="string">&quot;side-output&quot;</span>)&#123;&#125;;</span><br><span class="line">SingleOutputStreamOperator&lt;Integer&gt; mainDataStream = input.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(Integer integer, ProcessFunction&lt;Integer, Integer&gt;.Context context, Collector&lt;Integer&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//输出到主流</span></span><br><span class="line">        collector.collect(integer);</span><br><span class="line">        <span class="comment">//输出到测流</span></span><br><span class="line">        context.output(outputTag, <span class="string">&quot;sideout-&quot;</span> + String.valueOf(integer));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">//通过&quot;主流&quot;获取侧流</span></span><br><span class="line">DataStream&lt;String&gt; sideOutputStream = mainDataStream.getSideOutput(outputTag);</span><br></pre></td></tr></table></figure><h2 id="3-4-connect连接操作"><a href="#3-4-connect连接操作" class="headerlink" title="3.4 connect连接操作"></a>3.4 connect连接操作</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>connect连接 (DataStream,DataStream -> ConnectedStreams)</td></tr></table><p><font size=3><b>connect翻译成中文意思为连接，可以将两个数据类型一样也可以不一样的DataStream连接成一个新的ConnectedStreams。需要注意的是，connect方法与union方法不同，虽然调用connect方法将两个流连接成一个新的ConnectedStreams，但是里面的两个流依然是相互独立的，++这个方法最大的好处是可以让两个流共享State状态++，状态相关的内容在后面章节介绍。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建两个DataStream</span></span><br><span class="line">DataStreamSource&lt;String&gt; word = env.fromElements(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">DataStreamSource&lt;Integer&gt; num = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"><span class="comment">//将两个DataStream连接到一起</span></span><br><span class="line">ConnectedStreams&lt;String, Integer&gt; connected = word.connect(num);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>coMap (ConnectedStreams -> DataStream)</td></tr></table><p><font size=3><b>对ConnectedStreams调用map方法时需要传入CoMapFunction函数；<br>该接口需要指定3个泛型：</p><ol><li>第一个输入DataStream的数据类型</li><li>第二个输入DataStream的数据类型</li><li>返回结果的数据类型</li></ol><p>该接口需要重写两个方法：</p><ol><li>map1方法，是对第1个流进行map的处理逻辑</li><li>map2方法，是对第2个流进行map的处理逻辑</li></ol><p>这两个方法必须是相同的返回类型。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将两个DataStream连接到一起</span></span><br><span class="line">ConnectedStreams&lt;String, Integer&gt; wordAndNum = word.connect(num);</span><br><span class="line"><span class="comment">//对ConnectedStream中两个流分别调用两不同逻辑的map方法</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; result = wordAndNum.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;String, Integer, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">map1</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">map2</span><span class="params">(Integer integer)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> String.valueOf(integer * <span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>coFlatMap (ConnectedStreams -> DataStream)</td></tr></table><p><font size=3><b>对ConnectedStreams调用flatMap方法。调用flatMap方法，传入的Function是CoFlatMapFunction；</p><p>该接口需要重写两个方法：</p><ol><li>flatMap1方法，是对第1个流进行flatMap的处理逻辑</li><li>flatMap2方法，是对第2个流进行flatMap的处理逻辑</li></ol><p>这两个方法必须是相同的返回类型。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建两个DataStream</span></span><br><span class="line">DataStreamSource&lt;String&gt; word = env.fromElements(<span class="string">&quot;a b c&quot;</span>, <span class="string">&quot;d e f&quot;</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; num = env.fromElements(<span class="string">&quot;1,2,3&quot;</span>, <span class="string">&quot;4,5,6&quot;</span>);</span><br><span class="line"><span class="comment">//将两个DataStream连接到一起</span></span><br><span class="line">ConnectedStreams&lt;String, String&gt; connected = word.connect(num);</span><br><span class="line"><span class="comment">//对ConnectedStream中两个流分别调用两不同逻辑的flatMap方法</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; result = connected.flatMap(<span class="keyword">new</span> <span class="title class_">CoFlatMapFunction</span>&lt;String, String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap1</span><span class="params">(String s, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String w : words) &#123;</span><br><span class="line">            collector.collect(w);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap2</span><span class="params">(String s, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] nums = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String n : nums) &#123;</span><br><span class="line">            collector.collect(n);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="3-5-union合并操作"><a href="#3-5-union合并操作" class="headerlink" title="3.5 union合并操作"></a>3.5 union合并操作</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>union合并 (DataStream * -> DataStream)</td></tr></table><p><font size=3><b>该方法可以将两个或者多个数据类型一致的DataStream合并成一个DataStream。DataStream<T> union(DataStream<T> … streams)可以看出DataStream的union方法的参数为可变参数，即可以合并两个或多个数据类型一致的DataStream。<br>下面的例子是使用fromElements生成两个DataStream，一个是奇数的，一个是偶数的，然后将两个DataStream合并成一个DataStream。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Integer&gt; odd = env.fromElements(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>);</span><br><span class="line">DataStreamSource&lt;Integer&gt; even = env.fromElements(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>);</span><br><span class="line">DataStream&lt;Integer&gt; result = odd.union(even);</span><br></pre></td></tr></table></figure><h2 id="3-6-coGroup协同分组"><a href="#3-6-coGroup协同分组" class="headerlink" title="3.6 coGroup协同分组"></a>3.6 coGroup协同分组</h2><p><font size=3><b>coGroup本质上是join算子的底层算子，功能类似。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; stream1 = env.fromElements(<span class="string">&quot;1,aa,m,18&quot;</span>, <span class="string">&quot;2,bb,m,28&quot;</span>, <span class="string">&quot;3,cc,f,38&quot;</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; stream2 = env.fromElements(<span class="string">&quot;1:aa:m:18&quot;</span>, <span class="string">&quot;2:bb:m:28&quot;</span>, <span class="string">&quot;3:cc:f:38&quot;</span>);</span><br><span class="line"></span><br><span class="line">                         <span class="comment">//左边数据流的哪个字段与右边数据流的哪个字段</span></span><br><span class="line">DataStream&lt;String&gt; coGroup = stream1.coGroup(stream2).where(<span class="keyword">new</span> <span class="title class_">KeySelector</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">getKey</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).equalTo(<span class="keyword">new</span> <span class="title class_">KeySelector</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">getKey</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .apply(<span class="keyword">new</span> <span class="title class_">CoGroupFunction</span>&lt;String, String, String&gt;() &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             *</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> iterable 左边迭代器，5s内左边数据流的一组</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> iterable1 右边迭代器，5s内右边数据流的一组</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> collector</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">coGroup</span><span class="params">(Iterable&lt;String&gt; iterable, Iterable&lt;String&gt; iterable1, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h2 id="3-7-join关联操作"><a href="#3-7-join关联操作" class="headerlink" title="3.7 join关联操作"></a>3.7 join关联操作</h2><p><font size=3><b>用于关联两个流(类似于sql中的join)，还需要指定join的条件，需要在窗口中进行关联后的逻辑计算。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>join算子的代码结构：</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stream.join(otherStream)</span><br><span class="line">      .where(KeySelector)</span><br><span class="line">      .equalTo(KeySelector)</span><br><span class="line">      .window(WindowAssigner)</span><br><span class="line">      .apply(JoinFunction)</span><br></pre></td></tr></table></figure><h2 id="3-8-broadcast广播"><a href="#3-8-broadcast广播" class="headerlink" title="3.8 broadcast广播"></a>3.8 broadcast广播</h2><p><font size=3><b>Broadcast State是Flink 1.5引入的新特性。<br>在开发过程中，如果遇到需要下发广播配置、规则等低吞吐事件流到下游所有task时，就可以使用Broadcast State特性。下游的task接收这些配置、规则并保存为Broadcast State，将这些配置应用到另一个数据流的计算中。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>API介绍</td></tr></table><p><font size=3><b>通常，我们首先会创建一个Keyed或Non-Keyed的DataStream，然后再创建一个BroadcastedStream，最后通过DataStream来连接(调用connect方法)到BroadcastedStream上，这样实现将Broadcast State广播到DataStream下游的每个Task中。如果DataStream是KeyedStream，则连接到BroadcastedStream后，添加处理ProcessFunction时需要使用KeyedBroadcastProcessFunction来实现，下面是KeyedBroadcastProcessFunction的API，代码如下所示：<br></b></font></p><p><font color=Red size=3><b>详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=41&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=41&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P41<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.9 Flink 扩展sink算子</title>
      <link href="/post/53c0ce4c.html"/>
      <url>/post/53c0ce4c.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-9-Flink-扩展sink算子"><a href="#2-9-Flink-扩展sink算子" class="headerlink" title="2.9 Flink 扩展sink算子"></a>2.9 Flink 扩展sink算子</h1><h2 id="2-9-1-KafkaSink"><a href="#2-9-1-KafkaSink" class="headerlink" title="2.9.1 KafkaSink"></a>2.9.1 KafkaSink</h2><p><font size=3><b>Flink可以和Kafka多个版本整合，比如0.11.x、1.x、2.x等；<br>从Flink1.9开始，使用的是kafka2.2的客户端。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>核心类</td></tr></table><ul><li><strong>KafkaStringSerializationSchema – 反序列化</strong></li><li><strong>FlinkKafkaProducer – 生产者（即sink）</strong></li></ul><p><font size=3><b>需要添加依赖：<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>新版本API</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_10_KafkaSinkOperator_Demo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.setInteger(<span class="string">&quot;rest.port&quot;</span>, <span class="number">8822</span>);</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//开启checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="string">&quot;file:///d:/ckpt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把数据写入kafka</span></span><br><span class="line">        <span class="comment">// 1. 构造一个kafka的sink算子</span></span><br><span class="line">        KafkaSink&lt;String&gt; kafkaSink = KafkaSink.&lt;String&gt;builder()</span><br><span class="line">                .setBootstrapServers(<span class="string">&quot;192.168.0.219:9092&quot;</span>)</span><br><span class="line">                .setRecordSerializer(KafkaRecordSerializationSchema.&lt;String&gt;builder()</span><br><span class="line">                        .setTopic(<span class="string">&quot;event-log&quot;</span>)</span><br><span class="line">                        .setValueSerializationSchema(<span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>())</span><br><span class="line">                        .build()</span><br><span class="line">                )</span><br><span class="line">                .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)</span><br><span class="line">                .setTransactionalIdPrefix(<span class="string">&quot;doitedu-&quot;</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.把数据流输出到构造好的sink算子</span></span><br><span class="line">        streamSource</span><br><span class="line">                .map(JSON::toJSONString)</span><br><span class="line">                .sinkTo(kafkaSink);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>KafkaSink是能结合Flink的Checkpoint机制，来支持端到端的一致性语义；<br>（底层，当然是利用了kafka producer的事务机制）</b></font></p><h2 id="2-9-2-JdbcSink"><a href="#2-9-2-JdbcSink" class="headerlink" title="2.9.2 JdbcSink"></a>2.9.2 JdbcSink</h2><p><font size=3><b>需要添加依赖：<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>非Exactly-Once的JdbcSink</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_11_JdbcSinkOperator_Demo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.setInteger(<span class="string">&quot;rest.port&quot;</span>, <span class="number">8822</span>);</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="string">&quot;file:///d:/ckpt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构造一个Jdbc的sink算子</span></span><br><span class="line">        <span class="comment">// 不保证Exactly-Once的一种</span></span><br><span class="line">        SinkFunction&lt;EventLog&gt; jdbcSink = JdbcSink.sink(</span><br><span class="line">                <span class="string">&quot;insert into t_eventlog values(?,?,?,?,?) on duplicate key update sessionId=?,eventId=?,ts=?,eventInfo=?&quot;</span>,</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">JdbcStatementBuilder</span>&lt;EventLog&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">accept</span><span class="params">(PreparedStatement preparedStatement, EventLog eventLog)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">                        preparedStatement.setLong(<span class="number">1</span>, eventLog.getGuid());</span><br><span class="line">                        preparedStatement.setString(<span class="number">2</span>, eventLog.getSessionId());</span><br><span class="line">                        preparedStatement.setString(<span class="number">3</span>, eventLog.getEventId());</span><br><span class="line">                        preparedStatement.setLong(<span class="number">4</span>, eventLog.getTimeStamp());</span><br><span class="line">                        preparedStatement.setString(<span class="number">5</span>, JSON.toJSONString(eventLog.getEventInfo()));</span><br><span class="line"></span><br><span class="line">                        preparedStatement.setString(<span class="number">6</span>, eventLog.getSessionId());</span><br><span class="line">                        preparedStatement.setString(<span class="number">7</span>, eventLog.getEventId());</span><br><span class="line">                        preparedStatement.setLong(<span class="number">8</span>, eventLog.getTimeStamp());</span><br><span class="line">                        preparedStatement.setString(<span class="number">9</span>, JSON.toJSONString(eventLog.getEventInfo()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                JdbcExecutionOptions.builder().withBatchIntervalMs(<span class="number">5</span>).withMaxRetries(<span class="number">2</span>).build(),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">JdbcConnectionOptions</span>.JdbcConnectionOptionsBuilder()</span><br><span class="line">                        .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">                        .withPassword(<span class="string">&quot;123&quot;</span>)</span><br><span class="line">                        .withUrl(<span class="string">&quot;jdbc:mysql://&quot;</span>)</span><br><span class="line">                        .build()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        streamSource.addSink(jdbcSink);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>Exactly-Once的JdbcSink</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_11_JdbcSinkOperator_Demo1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.setInteger(<span class="string">&quot;rest.port&quot;</span>, <span class="number">8822</span>);</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="string">&quot;file:///d:/ckpt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构造一个Jdbc的sink算子</span></span><br><span class="line"><span class="comment">         * 不保证Exactly-Once的一种</span></span><br><span class="line"><span class="comment">         * 底层是利用jdbc目标数据库的事务机制</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        SinkFunction&lt;EventLog&gt; eosJdbcsink = JdbcSink.exactlyOnceSink(</span><br><span class="line">                <span class="string">&quot;insert into t_eventlog values(?,?,?,?,?) on duplicate key update sessionId=?,eventId=?,ts=?,eventInfo=?&quot;</span>,</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">JdbcStatementBuilder</span>&lt;EventLog&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">accept</span><span class="params">(PreparedStatement preparedStatement, EventLog eventLog)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">                        preparedStatement.setLong(<span class="number">1</span>, eventLog.getGuid());</span><br><span class="line">                        preparedStatement.setString(<span class="number">2</span>, eventLog.getSessionId());</span><br><span class="line">                        preparedStatement.setString(<span class="number">3</span>, eventLog.getEventId());</span><br><span class="line">                        preparedStatement.setLong(<span class="number">4</span>, eventLog.getTimeStamp());</span><br><span class="line">                        preparedStatement.setString(<span class="number">5</span>, JSON.toJSONString(eventLog.getEventInfo()));</span><br><span class="line"></span><br><span class="line">                        preparedStatement.setString(<span class="number">6</span>, eventLog.getSessionId());</span><br><span class="line">                        preparedStatement.setString(<span class="number">7</span>, eventLog.getEventId());</span><br><span class="line">                        preparedStatement.setLong(<span class="number">8</span>, eventLog.getTimeStamp());</span><br><span class="line">                        preparedStatement.setString(<span class="number">9</span>, JSON.toJSONString(eventLog.getEventInfo()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                JdbcExecutionOptions.builder().withBatchSize(<span class="number">5</span>).withMaxRetries(<span class="number">2</span>).build(),</span><br><span class="line">                JdbcExactlyOnceOptions.builder().withTransactionPerConnection(<span class="literal">true</span>).build(),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SerializableSupplier</span>&lt;XADataSource&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> XADataSource <span class="title function_">get</span><span class="params">()</span> &#123;</span><br><span class="line">                        <span class="type">MysqlXADataSource</span> <span class="variable">xADataSource</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MysqlXADataSource</span>();</span><br><span class="line">                        xADataSource.setUser(<span class="string">&quot;root&quot;</span>);</span><br><span class="line">                        xADataSource.setUrl(<span class="string">&quot;jdbc:mysql://&quot;</span>);</span><br><span class="line">                        xADataSource.setPassword(<span class="string">&quot;123&quot;</span>);</span><br><span class="line">                        <span class="keyword">return</span> xADataSource;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        streamSource.addSink(eosJdbcsink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-9-3-RedisSink"><a href="#2-9-3-RedisSink" class="headerlink" title="2.9.3 RedisSink"></a>2.9.3 RedisSink</h2><p><font color=Red size=3><b>详见<br><a href="https://www.bilibili.com/video/BV1K44y1g7wA?p=33&vd_source=26668f0ed33317a00612f0d4c98799c9">https://www.bilibili.com/video/BV1K44y1g7wA?p=33&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</a><br>P33</b></font></p><p><font size=3><b>Redis是一个基于内存、性能极高的NoSQL数据库，数据还可以持久化到磁盘，读写速度快，适合存储key-value类型的数据。Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。Flink实时计算出的结果，需要快速的输出存储起来，要求写入的存储系统的速度要快，这个才不会造成数据积压。Redis就是一个非常不错的选择。<br></b></font></p><p><font size=3><b>需要添加依赖：<br></b></font></p><p><font color=Red size=3><b>注意，这个依赖，在Maven官方的仓库中是没有的，需要下载bahir的源码到本地，并进行编译和安装到maven本地仓库，然后才能按照下面的方式引入：</b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.8 Flink 基础sink算子</title>
      <link href="/post/34837dad.html"/>
      <url>/post/34837dad.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-8-Flink-基础sink算子"><a href="#2-8-Flink-基础sink算子" class="headerlink" title="2.8 Flink 基础sink算子"></a>2.8 Flink 基础sink算子</h1><h2 id="2-8-1-打印输出print"><a href="#2-8-1-打印输出print" class="headerlink" title="2.8.1 打印输出print"></a>2.8.1 打印输出print</h2><p><font size=3><b>打印是最简单的一个Sink，通常是用来做实验和测试时使用。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = keyed.sum(<span class="string">&quot;f1&quot;</span>);</span><br></pre></td></tr></table></figure><h2 id="2-8-2-文件Sink"><a href="#2-8-2-文件Sink" class="headerlink" title="2.8.2 文件Sink"></a>2.8.2 文件Sink</h2><table><tr><td bgcolor=orange><font size=4><b>以下writeAs... 方法均已被标记为deprecated</td></tr></table><table><tr><td bgcolor=Gainsboro><font size=4><b>writeAsText 以文本格式输出</td></tr></table><p><font size=3><b>该方法是将数据以csv格式写入到指定的目录中，目录中的文件名称是该Sink所在subtask的Index+1。可以额外指定一个参数writeMode，默认是WriteMode.NO_OVERWRITE。如果是WriteMode.OVERWRITE，会将以前的文件覆盖。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>writeAsCsv 以csv格式输出</td></tr></table><p><font size=3><b>该方法是将数据以csv格式写入到指定的目录中，本质上使用的是CsvOutputFormat格式写入的。该Sink并不是将数据实时写入到文件中，而是有一个BufferedOutputStream，默认缓存的大小为4096个字节，只有达到这个大小，才会flush到磁盘。另外程序在正常退出，调用Sink的close方法也会flush到磁盘。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>writeUsingOutputFormat 以指定的格式输出</td></tr></table><p><font size=3><b>该方法是将数据以指定的格式写入到指定目录中，该方法要传入一个OutputFormat接口的实现类。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>writeToSocket 输出到网络端口</td></tr></table><p><b>该方法是将数据输出到指定的Socket网络地址端口。输出之前，指定的网络端口服务必须已经启动。<br></b></p><h2 id="2-8-3-StreamFileSink"><a href="#2-8-3-StreamFileSink" class="headerlink" title="2.8.3 StreamFileSink"></a>2.8.3 StreamFileSink</h2><p><strong>该Sink不但可以将数据写入到各种文件系统中，而且整合了checkpoint机制来保证Exactly Once语义，还可以对文件进行分桶存储，还支持以列式存储的格式写入，功能更强大。</strong></p><p><strong>streamFileSink中输出的文件，其生命周期会经历3种状态：</strong></p><ul><li>In-Progress Files</li><li>Pending Files</li><li>Finished Files</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131736506.jpeg" alt="Flink图9"></p><p><font size=3><b>需要添加依赖：<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-parquet_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.parquet<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>parquet-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.11.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-files<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>代码示例</td></tr></table><p><font size=3><b>通过DefaultRollingPolicy这个工具类，指定文件滚动生成的策略。这里设置的文件滚动生成策略有两个，一个是距离上一次生成文件时间超过30秒，另一个是文件大小达到100mb.这两个条件只要满足其中一个即可滚动生成文件。然后StreamingFileSink.forRowFormat方法将文件输出目录，文件写入的编码传入，再调用withRollingPolicy关联上面的文件滚动生成策略，接着调用build方法构建好StreamingFileSink，最后将其作为参数传入到addSink方法中。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_08_SinkOperator_Demos</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.enableCheckpointing(<span class="number">2000</span>, CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">        env.getCheckpointConfig().setCheckpointStorage(<span class="string">&quot;file:///d:/ckpt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//输出到控制台</span></span><br><span class="line">        <span class="comment">//streamSource.print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//输出到文件</span></span><br><span class="line">        <span class="comment">//streamSource.writeAsText(&quot;d:/sink_test2&quot;, FileSystem.WriteMode.OVERWRITE);</span></span><br><span class="line">        streamSource.map(bean -&gt; Tuple5.of(bean.getEventId(), bean.getGuid(), bean.getEventInfo(), bean.getSessionId(), bean.getTimeStamp()))</span><br><span class="line">                .returns(<span class="keyword">new</span> <span class="title class_">TypeHint</span>&lt;Tuple5&lt;String, Long, Map&lt;String, String&gt;, String, Long&gt;&gt;()&#123;&#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 应用StreamFileSink算子，将数据输出到文件系统</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 1. 输出为 行格式</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//构造一个FileSink对象</span></span><br><span class="line">        FileSink&lt;String&gt; rowSink = FileSink</span><br><span class="line">                .forRowFormat(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;d:/filesink/&quot;</span>), <span class="keyword">new</span> <span class="title class_">SimpleStringEncoder</span>&lt;String&gt;(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">                <span class="comment">//文件的滚动策略 (间隔时长10s，或文件大小达到1M，就进行文件切换)</span></span><br><span class="line">                .withRollingPolicy(DefaultRollingPolicy.builder().withRolloverInterval(<span class="number">1000</span>).withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span>).build())</span><br><span class="line">                <span class="comment">//分桶的策略 (划分子文件夹的策略)</span></span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> <span class="title class_">DateTimeBucketAssigner</span>&lt;String&gt;())</span><br><span class="line">                .withBucketCheckInterval(<span class="number">5</span>)</span><br><span class="line">                <span class="comment">//输出文件的文件名相关配置</span></span><br><span class="line">                .withOutputFileConfig(OutputFileConfig.builder().withPartPrefix(<span class="string">&quot;doitedu&quot;</span>).withPartSuffix(<span class="string">&quot;.txt&quot;</span>).build())</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//然后添加到流进行输出</span></span><br><span class="line">        streamSource.map(JSON::toJSONString)</span><br><span class="line">                <span class="comment">//.addSink()/*SinkFunction的实现类对象，用addSink()来添加*/</span></span><br><span class="line">                .sinkTo(rowSink);<span class="comment">/*Sink的实现类对象，用sinkTo()来添加*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 2. 输出为 列格式</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//详见</span></span><br><span class="line">        <span class="comment">// https://www.bilibili.com/video/BV1K44y1g7wA?p=24&amp;vd_source=26668f0ed33317a00612f0d4c98799c9</span></span><br><span class="line">        <span class="comment">// P24</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.7 Flink 基础transformation算子</title>
      <link href="/post/1656c012.html"/>
      <url>/post/1656c012.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-7-Flink-基础transformation算子"><a href="#2-7-Flink-基础transformation算子" class="headerlink" title="2.7 Flink 基础transformation算子"></a>2.7 Flink 基础transformation算子</h1><h2 id="2-7-1-映射算子"><a href="#2-7-1-映射算子" class="headerlink" title="2.7.1 映射算子"></a>2.7.1 映射算子</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>map 映射（DataStream -> DataStream）</td></tr></table><p><font size=3><b>map(new MapFunction)<br>MapFunction : (x) -&gt; y [1条变1条]</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; words = env.fromElements(<span class="string">&quot;hadoop&quot;</span>, <span class="string">&quot;spark&quot;</span>, <span class="string">&quot;flink&quot;</span>, <span class="string">&quot;hbase&quot;</span>, <span class="string">&quot;flink&quot;</span>);</span><br><span class="line"><span class="comment">//在map方法中传入MapFunction实现类实例，重写map方法</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; upperWords = words.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">//将单词转为大写</span></span><br><span class="line">                <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">            &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">//调用Sink将数据打印在控制台</span></span><br><span class="line">upperWords.print();</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>flatMap 扁平化映射（DataStream -> DataStream）</td></tr></table><p><font size=3><b>flatMap(new FlatMapFunction)<br>FlatMapFunction : x -&gt; x1,x2,x3 [1条变多条，并展平]</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; words = source.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                String[] split = value.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>如果是flatMap方法时传入Lambda表达式，需要在调用flatMap方法后，再调用returns方法指定返回的数据类型，否则Flink无法自动推断出返回的数据类型，会出现异常。</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>project 投影（DataStream -> DataStream）</td></tr></table><p><font size=3><b>该算子只能对 Tuple 类型数据使用，project方法的功能类似sql中的”select”字段，该方法只有java API才有，scala API没有该方法</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple3&lt;String, String, Integer&gt;&gt; users = env.fromElements(Tuple3.of(<span class="string">&quot;佩奇&quot;</span>, <span class="string">&quot;女&quot;</span>, <span class="number">5</span>), Tuple3.of(<span class="string">&quot;乔治&quot;</span>, <span class="string">&quot;男&quot;</span>, <span class="number">3</span>));</span><br><span class="line"><span class="comment">//只要第0个和第2个字段</span></span><br><span class="line">SingleOutputStreamOperator&lt;Tuple&gt; result = users.project(<span class="number">0</span>, <span class="number">2</span>);</span><br></pre></td></tr></table></figure><h2 id="2-7-2-过滤算子"><a href="#2-7-2-过滤算子" class="headerlink" title="2.7.2 过滤算子"></a>2.7.2 过滤算子</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>filter 过滤（DataStream -> DataStream）</td></tr></table><p><font size=3><b>filter(new FilterFunction)<br>FilterFunction : x -&gt; true&#x2F;false</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Integer&gt; numbers = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>);</span><br><span class="line"><span class="comment">//过滤奇数，保留偶数</span></span><br><span class="line">SingleOutputStreamOperator&lt;Integer&gt; even = numbers.filter(<span class="keyword">new</span> <span class="title class_">FilterFunction</span>&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">filter</span><span class="params">(Integer integer)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> integer % <span class="number">2</span> == <span class="number">0</span>;</span><br><span class="line">           &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="2-7-3-分组算子"><a href="#2-7-3-分组算子" class="headerlink" title="2.7.3 分组算子"></a>2.7.3 分组算子</h2><table><tr><td bgcolor=Gainsboro><font size=4><b>keyBy 按key分组（DataStream -> KeyedStream）</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed = words.keyBy(<span class="keyword">new</span> <span class="title class_">KeySelector</span>&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> value.f0;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="2-7-4-滚动聚合算子"><a href="#2-7-4-滚动聚合算子" class="headerlink" title="2.7.4 滚动聚合算子"></a>2.7.4 滚动聚合算子</h2><ul><li>此处所说的滚动聚合算子，是多个聚合算子的统称，有 sum、min、minBy、max、maxBy</li><li>这些算子的底层逻辑都是维护一个聚合值，并使用每条流入的数据对聚合值进行滚动更新</li><li>这些算子都只能在 KeyedStream 上调用 (就是必须 keyBy 后调用)</li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>sum</td></tr></table><p><font size=3><b>该算子实现实时滚动相加的功能，即新输入的数据和历史数据进行相加。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>min、minBy</td></tr></table><p><font size=3><b>这两个算子都是求最小值，min和minBy的区别在于：</b></font></p><ul><li>min的返回值，除了最小值字段以外，其它字段是第一条输入数据的值</li><li>minBy的返回值，就是最小值字段所在的那条数据</li></ul><table><tr><td bgcolor=Gainsboro><font size=4><b>max、maxBy</td></tr></table><p><font size=3><b>这两个算子都是求最大值，用法和min、minBy一样<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>reduce 归约</td></tr></table><p><font size=3><b>它的滚动聚合逻辑没有写死，而是由用户通过ReduceFunction来传入<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>iterate 迭代 (DataStream -> IterativeStream -> DataStream)</td></tr></table>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.6 Flink 基础source算子</title>
      <link href="/post/32cb9a74.html"/>
      <url>/post/32cb9a74.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-6-Flink-基础source算子"><a href="#2-6-Flink-基础source算子" class="headerlink" title="2.6 Flink 基础source算子"></a>2.6 Flink 基础source算子</h1><p><font color=OrangeRed size=3><b>source 是用来获取外部数据的算子，按照获取数据的方式，可以分为：</b></font></p><ul><li><p>基于集合的Source</p></li><li><p>基于Socket网络端口的Source</p></li><li><p>基于文件的Source</p></li><li><p>第三方Connector Source</p></li><li><p>自定义Source</p></li></ul><p><font color=OrangeRed size=3><b>从并行的角度，source又可分为非并行的source和并行的source</b></font></p><ul><li>非并行source：并行度只能为1，即只有一个运行时实例，在读取大量数据时效率比较低，通常是用来做一些实验或测试，例如Socket Source。</li><li>并行的source：并行度可以是1到多个，在计算资源足够的前提下，并行度越大，效率越高，例如Kfaka Source。</li></ul><h2 id="2-6-1-基于集合的Source（测试用）"><a href="#2-6-1-基于集合的Source（测试用）" class="headerlink" title="2.6.1 基于集合的Source（测试用）"></a>2.6.1 基于集合的Source（测试用）</h2><p><font size=3><b>可将一个普通的Java集合、迭代器或者可变参数转换成一个分布式数据流DataStream</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>fromElements</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SourceOperator</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; fromElements = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        fromElements.map(x -&gt; x+<span class="number">10</span>).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>fromCollection</td></tr></table><p><font size=3><b>非并行的 Source，可以将一个Collection作为参数传入到该方法中，返回一个DataStreamSource</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; dataList = Arrays.asList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; fromCollection = env.fromCollection(dataList);</span><br><span class="line">fromCollection.map(String::toUpperCase).print();</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>fromParallelCollection</td></tr></table><p><font size=3><b>该方法是一个并行的 Source，该方法需要传入两个参数，第一个是实现 SplittableIterator 的实现类的迭代器，第二个是迭代器中数据的类型。</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;LongValue&gt; parallelCollection = env.fromParallelCollection(<span class="keyword">new</span> <span class="title class_">LongValueSequenceIterator</span>(<span class="number">1</span>, <span class="number">100</span>), TypeInformation.of(LongValue.class));</span><br><span class="line">parallelCollection.map(v -&gt; v.getValue()+<span class="number">100</span>).print();</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>generateSequence</td></tr></table><p><font size=3><b>并行的 Source（并行度也可以通过调用该方法后，再通过调用 setParallelism() 方法来设置），通过指定的起始值和结束值来生成数据序列流。</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//调用env的generateSequence生成并行的DataSource，输出的数字是1到100</span></span><br><span class="line">DataStreamSource&lt;Long&gt; sequence = env.generateSequence(<span class="number">1</span>, <span class="number">100</span>).setParallelism(<span class="number">3</span>);</span><br><span class="line">sequence.map(x -&gt; x - <span class="number">1</span>).print();</span><br></pre></td></tr></table></figure><h2 id="2-6-2-基于Socket的Source（测试用）"><a href="#2-6-2-基于Socket的Source（测试用）" class="headerlink" title="2.6.2 基于Socket的Source（测试用）"></a>2.6.2 基于Socket的Source（测试用）</h2><p><font size=3><b>非并行的 Source，通过Socket通信获取数据得到数据流。<br>该方法还有多个重载的方法，如<br>socketTextStream(String hostname, int port, String delimiter, long maxRetry)<br>可以指定分隔符和最大重新连接次数。<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; lines = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br></pre></td></tr></table></figure><p><font color=OrangeRed size=3><b>注意，socketSource是一个非并行source，如果使用socketTextStream读取数据，在启动Flink程序之前，必须先启动一个Socket服务，在Linux中输入nc -lk 9999</b></font></p><h2 id="2-6-3-基于文件的Source"><a href="#2-6-3-基于文件的Source" class="headerlink" title="2.6.3 基于文件的Source"></a>2.6.3 基于文件的Source</h2><p><font size=3><b>基于文件的Source，本质上就是使用指定的FileInputFormat格式读取数据，可以指定TextInputFormat，CsvInputFormat，BinaryInputFormat等格式。<br>底层都是ContinuousFileMonitoringFunction，这个类继承了RichSourceFunction，都是非并行的Source。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>readFile</td></tr></table><p><font size=3><b> readFile(FileInputFormat inputFormat, String filePath, FileProcessingMode watchType)方法可以指定读取文件的FileInputFormat格式，<br>参数FileProcessingMode，可取值：</b></font></p><ul><li>PROCESS_ONCE：只读取文件中的数据一次，读取完成后，程序退出。</li><li>PROCESS_CONTINUOUSLY：会一直监听指定的文件，文件的内容发生变化后，会将以前的内容和新的内容全部读取出来，进而造成数据重复读取。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">path</span> <span class="operator">=</span> <span class="string">&quot;file://a.txt&quot;</span>;</span><br><span class="line">DataStreamSource&lt;String&gt; lines = env.readFile(<span class="keyword">new</span> <span class="title class_">TextInputFormat</span>(<span class="literal">null</span>), path, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">2000</span>);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>readTextFile</td></tr></table><p><font size=3><b>readTextFile(String filePath) 可以从指定的目录或文件读取数据，默认使用的是 TextInputFormat 格式读取数据，还有一个重载的方法 readTextFile(String filePath, String charsetName) 可以传入读取文件指定的字符集，默认是UTF-8编码。该方法是一个有限的数据源，数据读完后，程序就会退出，不能一直运行，该方法底层调用的是readFile方法，FileProcessingMode为PROCESS_ONCE<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; lines = env.readTextFile(path);</span><br></pre></td></tr></table></figure><h2 id="2-6-4-基于Kfaka的Source-生产常用"><a href="#2-6-4-基于Kfaka的Source-生产常用" class="headerlink" title="2.6.4 基于Kfaka的Source (生产常用)"></a>2.6.4 基于Kfaka的Source (生产常用)</h2><p><font size=3><b>在实际生产环境中，为了保证flink可以高效地读取数据源中的数据，通常是跟一些分布式消息中间件结合使用，例如Kakfa。Kafka的特点是分布式、多副本、高可用、高吞吐、可以记录偏移量等。Flink和Kafka整合可以高效的读取数据，并且可以保证Exactly Once（精确一次性语义）。</b></font></p><p>首先在maven项目的pom.xml文件中导入Flink跟Kakfa整合的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor=Gainsboro><font size=4><b>老版本API</td></tr></table><p>核心类FlinkKafkaConsumer，需指定三个参数：</p><ul><li>topic 名称</li><li>反序列化 Schema，SimpleStringSchema 指定的是读取Kafka中的数据反序列化成String格式</li><li>Properties 实例</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>注意，目前这种方式无法保证 Exactly Once，Flink的Source消费完数据后，将偏移量定期的写入到Kafka的__consumer_offsets中，这种方式虽然可以记录偏移量，但是无法保证 Exactly Once</b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>新版本API</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">properties.setProperty(<span class="string">&quot;auto.offset.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"></span><br><span class="line">KafkaSource&lt;String&gt; kafkaSource = KafkaSource.&lt;String&gt;builder()</span><br><span class="line">                .setTopics(<span class="string">&quot;tp01&quot;</span>)</span><br><span class="line">                .setGroupId(<span class="string">&quot;gp01&quot;</span>)</span><br><span class="line">                .setBootstrapServers(<span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">                <span class="comment">//消费起始位移选择之前的所提交的偏移量，(如果没有，则重置为LASTEST)</span></span><br><span class="line">                .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.LATEST))</span><br><span class="line">                <span class="comment">//.setStartingOffsets(OffsetsInitializer.earliest())消费起始位移直接选择为最早</span></span><br><span class="line">                <span class="comment">//.setStartingOffsets(OffsetsInitializer.latest())消费起始位移直接选择为最新</span></span><br><span class="line">                <span class="comment">//.setStartingOffsets(OffsetsInitializer.offsets(Map&lt;TopicPartition, Long &gt;))消费起始位移选择为方法所传入的每个分区对应的起始偏移量</span></span><br><span class="line">                .setValueOnlyDeserializer(<span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>())</span><br><span class="line"></span><br><span class="line">                <span class="comment">//开启了kafka底层消费者的自动位移提交机制，它会把最新的消费者位移提交到kafka的consumer_offsets中</span></span><br><span class="line">                <span class="comment">//就算把自动位移提交机制开启，KafkaSource依然不依赖自动位移提交机制(宕机重启时，优先从flink自己的状态中去获取偏移量&lt;更可靠&gt;)</span></span><br><span class="line">                .setProperties(properties)</span><br><span class="line">                <span class="comment">//把本Source算子设置成 BOUNDED属性(有界流)，将来本Source去读取数据的时候，读到指定的位置，就停止读取并退出</span></span><br><span class="line">                <span class="comment">//常用于补数或者重跑某一段历史数据</span></span><br><span class="line">                <span class="comment">//.setBounded(OffsetsInitializer.committedOffsets())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment">//把本Source算子设置成 UNBOUNDED属性(无界流)，但并不会一直读数据，而是到达指定的位置就停止读取，但程序不退出</span></span><br><span class="line">                <span class="comment">//主要应用场景：需要从kakfa中读取某一段固定长度的数据，然后拿这一段数据去跟另外一个真正的无界流联合处理</span></span><br><span class="line">                .setUnbounded(OffsetsInitializer.latest())</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">//env.addSource();</span></span><br><span class="line">DataStreamSource&lt;String&gt; kafkaStreamSource = (DataStreamSource&lt;String&gt;) env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), <span class="string">&quot;kafkaSource&quot;</span>);</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>新版本API中，Flink会把Kafka消费者的消费位移记录在算子状态中，这样就实现了消费位移状态的容错，从而可以支持端到端的 Exactly Once</b></font></p><h2 id="2-6-5-自定义-Source"><a href="#2-6-5-自定义-Source" class="headerlink" title="2.6.5 自定义 Source"></a>2.6.5 自定义 Source</h2><p><font size=3><b>Flink的 DataStream API 可以让开发者根据实际需要，灵活的自定义Source，本质上就是定义一个类，实现 SourceFunction 或继承 RichParallelSourceFunction，实现 run 方法和 cancel 方法。<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>代码示例</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.flink.java.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomStringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.SourceFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomSourceFunction</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;EventLog&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> <span class="title class_">MySourceFunction</span>());</span><br><span class="line">        dataStreamSource.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySourceFunction</span> <span class="keyword">implements</span> <span class="title class_">SourceFunction</span>&lt;EventLog&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;EventLog&gt; sourceContext)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">EventLog</span> <span class="variable">eventLog</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">EventLog</span>();</span><br><span class="line">        String[] events = &#123;<span class="string">&quot;appLaunch&quot;</span>, <span class="string">&quot;pageLoad&quot;</span>, <span class="string">&quot;adShow&quot;</span>, <span class="string">&quot;adClick&quot;</span>, <span class="string">&quot;itemShare&quot;</span>, <span class="string">&quot;itemCollect&quot;</span>, <span class="string">&quot;wakeUp&quot;</span>, <span class="string">&quot;appClose&quot;</span>&#125;;</span><br><span class="line">        Map&lt;String, String&gt; eventInfoMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (flag) &#123;</span><br><span class="line">            eventLog.setGuid(RandomUtils.nextLong(<span class="number">1</span>, <span class="number">1000</span>));</span><br><span class="line">            eventLog.setSessionId(RandomStringUtils.randomAlphabetic(<span class="number">12</span>).toUpperCase());</span><br><span class="line">            eventLog.setTimeStamp(System.currentTimeMillis());</span><br><span class="line">            eventLog.setEventId(events[RandomUtils.nextInt(<span class="number">0</span>, events.length)]);</span><br><span class="line"></span><br><span class="line">            eventInfoMap.put(RandomStringUtils.randomAlphabetic(<span class="number">1</span>), RandomStringUtils.randomAlphabetic(<span class="number">2</span>));</span><br><span class="line">            eventLog.setEventInfo(eventInfoMap);</span><br><span class="line"></span><br><span class="line">            sourceContext.collect(eventLog);</span><br><span class="line"></span><br><span class="line">            eventInfoMap.clear();</span><br><span class="line"></span><br><span class="line">            Thread.sleep(RandomUtils.nextInt(<span class="number">500</span>, <span class="number">1000</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">        flag = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EventLog</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> guid;</span><br><span class="line">    <span class="keyword">private</span> String sessionId;</span><br><span class="line">    <span class="keyword">private</span> String eventId;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> timeStamp;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; eventInfo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Flink编程基础</title>
      <link href="/post/b8ee456f.html"/>
      <url>/post/b8ee456f.html</url>
      
        <content type="html"><![CDATA[<h1 id="2-Flink编程基础"><a href="#2-Flink编程基础" class="headerlink" title="2. Flink编程基础"></a>2. Flink编程基础</h1><h2 id="2-1-需要引入的基本依赖"><a href="#2-1-需要引入的基本依赖" class="headerlink" title="2.1 需要引入的基本依赖"></a>2.1 需要引入的基本依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如需使用scala API，则需替换<br>flink-java 为 flink-scala_2.12<br>flink-streaming-java_2.12 为 flink-streaming-scala_2.12</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-2-flink的DataStream抽象"><a href="#2-2-flink的DataStream抽象" class="headerlink" title="2.2 flink的DataStream抽象"></a>2.2 flink的DataStream抽象</h2><h2 id="2-3-flink编程模板"><a href="#2-3-flink编程模板" class="headerlink" title="2.3 flink编程模板"></a>2.3 flink编程模板</h2><p>一般的flink编程如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.flink.java.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.RuntimeExecutionMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">StreamBatchWordCount</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//流处理的编程环境入口</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">streamEnv</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        streamEnv.setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//按批计算模式执行</span></span><br><span class="line">        streamEnv.setRuntimeMode(RuntimeExecutionMode.BATCH);</span><br><span class="line">        <span class="comment">//streamEnv.setRuntimeMode(RuntimeExecutionMode.STREAMING);   //按流计算模式执行</span></span><br><span class="line">        <span class="comment">//streamEnv.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);   //flink自己决定</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//读文件，得到DataStream</span></span><br><span class="line">        DataStreamSource&lt;String&gt; streamSource = streamEnv.readTextFile(<span class="string">&quot;input&quot;</span>);</span><br><span class="line"></span><br><span class="line">        streamSource.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                String[] words = s.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                .keyBy(<span class="keyword">new</span> <span class="title class_">KeySelector</span>&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> String <span class="title function_">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        streamEnv.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用Lambda表达式编程如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.yiyuyyds.flink.java.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.KeyedStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> scala.tools.util.PathResolver;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountLambda</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//流式处理入口环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">envStream</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; streamSource = envStream.readTextFile(<span class="string">&quot;input&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将句子转为大写</span></span><br><span class="line">        <span class="comment">//streamSource.map(value -&gt; value.toUpperCase());</span></span><br><span class="line">        <span class="comment">//由于上面的lambda表达式，函数体只有一行代码，且其中的方法没有参数，则可以使用方法引用</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; upperCased = streamSource.map(String::toUpperCase);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordAndOne = upperCased.flatMap((String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) -&gt; &#123;</span><br><span class="line">            String[] words = s.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                <span class="comment">//.returns(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;);</span></span><br><span class="line">                <span class="comment">//.returns(TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;));</span></span><br><span class="line">                .returns(Types.TUPLE(Types.STRING, Types.INT));</span><br><span class="line"></span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordAndOne.keyBy(value -&gt; value.f0);</span><br><span class="line"></span><br><span class="line">        keyedStream.sum(<span class="number">1</span>).print();</span><br><span class="line"></span><br><span class="line">        envStream.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>使用Lambda编程风格需要注意，<br>函数的返回类型时Tuple2&lt;String, Integer&gt;，其中的Tuple2可以推断，但是Tuple2里面又有泛型&lt;String, Integer&gt;，在java编译后，泛型被擦除了，这行代码</b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br></pre></td></tr></table></figure><p><font color=Red size=3><b>里面的Tuple2的泛型信息丢失了，也就是说存在一个问题，在返回结果的数据类型中有泛型参数，编译之后，这些信息丢失了，为了解决这个问题，可以用flink的API（returns）来显示声明返回的类型。</b></font></p><h2 id="2-4-flink程序的并行概念"><a href="#2-4-flink程序的并行概念" class="headerlink" title="2.4 flink程序的并行概念"></a>2.4 flink程序的并行概念</h2><ul><li>flink 程序中，每一个算子都可以成为一个独立任务(tast)</li><li>flink 程序中，视上下游算子间数据分发规则、并行度、共享槽位设置，可组成算子链为一个task</li><li>每个任务在运行时都可拥有多个并行的运行实例(subTask)</li><li>且每个算子任务的并行度都可以在代码中显示设置</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131712016.png" alt="Flink图8"></p><h2 id="2-5-flink编程入口"><a href="#2-5-flink编程入口" class="headerlink" title="2.5 flink编程入口"></a>2.5 flink编程入口</h2><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>批处理计算入口</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> ExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>流式计算入口</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>流批一体入口</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//为env设置环境参数</span></span><br><span class="line"><span class="type">ExecutionConfig</span> <span class="variable">config</span> <span class="operator">=</span> env.getConfig();</span><br><span class="line"><span class="comment">//设置批处理模式</span></span><br><span class="line">config.setExecutionMode(ExecutionMode.BATCH);</span><br></pre></td></tr></table></figure><table><tr><td bgcolor=DeepSkyBlue><font size=4><b>开启web UI的入口</td></tr></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//显示声明为本地运行环境，且带WebUI</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        configuration.setInteger(<span class="string">&quot;rest.port&quot;</span>, <span class="number">8082</span>);</span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br></pre></td></tr></table></figure><p>还需要添加相关依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-runtime-web_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.14.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.Flink简介</title>
      <link href="/post/971239a4.html"/>
      <url>/post/971239a4.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Flink简介"><a href="#1-Flink简介" class="headerlink" title="1.Flink简介"></a>1.Flink简介</h1><h2 id="1-1-离线批计算与实时流计算"><a href="#1-1-离线批计算与实时流计算" class="headerlink" title="1.1 离线批计算与实时流计算"></a>1.1 离线批计算与实时流计算</h2><p><strong><font color=OrangeRed><br>批计算与流计算，本质上就是对有界流和无界流的计算。<br></font></strong></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131702169.png" alt="Flink图1"></p><center>图1</center><ul><li><strong><font color=#0066FF>批计算</font></strong><br>  针对有界流，由于在产出计算结果前可以看到整个(完整)数据集，因而如下计算都可实现，对数据排序，计算全局统计值，对输入数据的整体产出最终汇总聚合报表。</li><li><strong><font color=#0066FF>流计算</font></strong><br>  针对无界流，由于永远无法看到输入数据的整体(数据的输入永远无法结束)，只能每逢数据到达就进行计算，并输出“当时”的计算结果（因而计算结果也不会是一个一次性的结果，而是源源不断的无界的结果流）</li></ul><h2 id="1-2-Apache-Flink-是什么"><a href="#1-2-Apache-Flink-是什么" class="headerlink" title="1.2 Apache Flink 是什么?"></a>1.2 Apache Flink 是什么?</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131703655.png" alt="Flink图2"></p><center>图2</center><p><strong><font color=OrangeRed>Flink是一个以流为核心的高可用，高性能的分布式计算引擎。具备流批一体、高吞吐、低延迟、容错能力、大规模复杂计算等特点，在数据流上提供数据分发、通信等功能。</font></strong></p><ul><li><p><strong><font color=#0066FF>数据流</font></strong><br>  所有产生的数据都天然带有时间概念，把事件按照时间顺序排列起来，就形成了一个事件流，也被称作数据流。</p></li><li><p><strong><font color=#0066FF>流批一体</font></strong><br>  首先必须明白什么是有界数据和无界数据</p><ul><li><strong><font color=#0066FF>有界数据</font></strong><br>  有界数据就是在一个确定的时间范围内的数据流，有开始，有结束，一旦确定就不会再改变，一般批处理用来处理有界数据，如图1的bounded stream。</li><li><strong><font color=#0066FF>无界数据</font></strong><br>  无界数据就是持续产生的数据流，数据是无限的，有开始，无结束，一般流处理用来处理无界数据，如图1的unbounded stream。</li></ul><p>  Flink的设计思想是以流为核心，批是流的特例，擅长处理无界和有界数据，Flink提供精确的时间控制能力和有状态计算机制，可以轻松应对无界数据流，同时提供窗口处理有界数据流，所以被称为流批一体。</p></li><li><p><strong><font color=#0066FF>容错能力</font></strong><br>  在分布式系统中，硬件故障、进程异常、应用异常、网络故障等异常无处不在，Flink引擎必须保证故障发生后不仅可以重启应用程序，还要确保其内部状态保持一致，从最后一次正确的时间点重新出发。<br>  Flink提供集群级容错和应用级容错能力</p><ul><li><strong><font color=#0066FF>集群级容错</font></strong><br>  Flink与集群管理器紧密连接，如YARN、Kubernetes，当进程挂掉后，自动重启新进程接管之前的工作，同时具备高可用性，可消除所有单点故障。</li><li><strong><font color=#0066FF>应用级容错能力</font></strong><br>  Flink使用轻量级分布式快照，设计检查点(checkpoint)实现可靠容错。</li></ul><p>  Flink利用检查点特性，在框架层面提供 Exactly-once 语义，即端到端的一致性，确保数据仅处理一次，不会重复也不会丢失，即使出现故障，也能保证数据只写一次。</p></li></ul><h2 id="1-3-Flink的架构"><a href="#1-3-Flink的架构" class="headerlink" title="1.3 Flink的架构"></a>1.3 Flink的架构</h2><p><strong>Flink架构分为技术架构和运行架构两部分。</strong></p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>如下图为Flink技术架构</td></tr></table><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131705097.png" alt="Flink图3"/><p>Flink 作为流批一体的分布式计算引擎，必须提供面向开发人员的API层，同时还需要跟外部数据存储进行交互，需要连接器，作业开发、测试完毕后，需要提交集群执行，需要部署层，同时还需要运维人员能够管理和监控，还提供图计算、机器学习、SQL等，需要应用框架层。</p><table><tr><td bgcolor=DeepSkyBlue><font size=5><b>如下图为Flink运行架构</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131706211.png" alt="Flink图4"></p><p>Flink 集群采取 Master - Slave 架构，Master的角色为 JobManager，负责集群和作业管理，Slave的角色是 TaskManager，负责执行计算任务，同时，Flink 提供客户端 Client 来管理集群和提交任务，JobManager 和 TaskManager 是集群的进程。</p><p><strong>（1）Client</strong></p><p>Flink 客户端是Flink 提供的 CLI 命令行工具，用来提交 Flink 作业到 Flink 集群，在客户端中负责 StreamGraph (流图)和 Job Graph (作业图)的构建。</p><p><strong>（2）JobManager</strong></p><p>JobManager 根据并行度将 Flink 客户端提交的Flink 应用分解为子任务，从资源管理器 ResourceManager 申请所需的计算资源，资源具备之后，开始分发任务到 TaskManager 执行 Task，并负责应用容错、跟踪作业的执行状态、发现异常后恢复作业等。</p><p><strong>（3）TaskManager</strong></p><p>TaskManager 接收 JobManager 分发的子任务，根据自身的资源情况管理子任务的启动、停止、销毁、异常恢复等生命周期阶段。Flink程序中必须有一个TaskManager。</p><h2 id="1-4-Flink的特性"><a href="#1-4-Flink的特性" class="headerlink" title="1.4 Flink的特性"></a>1.4 Flink的特性</h2><table><tr><td bgcolor=Gainsboro><font size=5><b>适用于几乎所有流式数据处理场景</td></tr></table><ul><li><strong>事件驱动型应用</strong></li><li><strong>流、批数据分析</strong></li><li><strong>数据管道及ETL</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131710215.jpeg" alt="Flink图5"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>自带状态管理机制</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131708285.png" alt="Flink图7"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>强大的准确性保证</td></tr></table><ul><li><strong>Exactly-once 状态一致性</strong></li><li><strong>事件时间处理</strong></li><li><strong>专业的迟到数据处理</strong></li></ul><table><tr><td bgcolor=Gainsboro><font size=5><b>灵活丰富的多层API</td></tr></table><ul><li><strong>流、批数据之上的SQL查询</strong></li><li><strong>流、批数据之上的TableAPI</strong></li><li><strong>DataStream流处理API、DataSet批处理API</strong></li><li><strong>精细可控的processFunction</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131712730.png" alt="Flink图6"></p><table><tr><td bgcolor=Gainsboro><font size=5><b>规模弹性扩展</td></tr></table><ul><li><strong>可扩展的分布式架构</strong></li><li><strong>支持超大状态管理</strong></li><li><strong>增量checkpoint机制</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304131712016.png" alt="Flink图8"></p><center><font color=OrangeRed size=4><b>算子粒度的独立并行度灵活配置(槽位资源可扩展、算子任务实例可扩展）</font></center><table><tr><td bgcolor=Gainsboro><font size=5><b>强大的运维能力</td></tr></table><ul><li><strong>弹性实施部署机制</strong></li><li><strong>高可用机制</strong></li><li><strong>保存点恢复机制</strong></li></ul><table><tr><td bgcolor=Gainsboro><font size=5><b>优秀的性能</font></td></tr></table><ul><li><strong>低延迟</strong></li><li><strong>高吞吐</strong></li><li><strong>内存计算</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Flink</title>
      <link href="/post/9aba4271.html"/>
      <url>/post/9aba4271.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Flink"><a href="#云服务器安装Flink" class="headerlink" title="云服务器安装Flink"></a>云服务器安装Flink</h1><h2 id="1-Flink下载地址"><a href="#1-Flink下载地址" class="headerlink" title="1.Flink下载地址"></a>1.Flink<a href="https://flink.apache.org/downloads.html">下载地址</a></h2><h2 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf flink-1.13.2-bin-scala_2.11.tgz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export FLINK_HOME=/home/software/flink-1.13.2</span><br><span class="line">export PATH=$PATH:/$FLINK_HOME/bin</span><br><span class="line">export HADOOP_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br></pre></td></tr></table></figure><h2 id="4-配置conf文件"><a href="#4-配置conf文件" class="headerlink" title="4.配置conf文件"></a>4.配置conf文件</h2><p>修改conf目录下的flink-conf.yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim flink-conf.yaml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.rpc.address:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.219</span></span><br><span class="line"></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"></span><br><span class="line"><span class="attr">state.backend.fs.checkpointdir:</span> <span class="string">hdfs://192.168.0.219:9000/flink-checkpoints</span></span><br><span class="line"><span class="attr">state.savepoints.dir:</span> <span class="string">hdfs://192.168.0.219:9000/flink-savepoints</span></span><br><span class="line"></span><br><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.219</span><span class="string">:2181</span></span><br><span class="line"><span class="attr">high-availability.storageDir:</span> <span class="string">hdfs://192.168.0.219:9000/flink/ha/</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.client.acl:</span> <span class="string">open</span></span><br></pre></td></tr></table></figure><p>修改masters的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">114.116.24.98:8082</span><br></pre></td></tr></table></figure><p>修改workers的IP地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">114.116.24.98</span><br></pre></td></tr></table></figure><p>修改conf下zoo.cfg配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/home/software/flink-1.13.2/tmp/zookeeper</span><br><span class="line">server.1=localhost:2887:3887</span><br></pre></td></tr></table></figure><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>进入bin目录下，启动Flink</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><p>jps查看进程出现TaskManagerRunner和StandaloneSessionClusterEntrypoint</p><p>在浏览器输入<a href="http://114.116.24.98:8082可进入Flink的Web页面">http://114.116.24.98:8082可进入Flink的Web页面</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Spark</title>
      <link href="/post/41b91bba.html"/>
      <url>/post/41b91bba.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器伪分布式部署Spark"><a href="#云服务器伪分布式部署Spark" class="headerlink" title="云服务器伪分布式部署Spark"></a>云服务器伪分布式部署Spark</h1><h2 id="1-下载Spark-下载地址"><a href="#1-下载Spark-下载地址" class="headerlink" title="1.下载Spark 下载地址"></a>1.下载Spark <a href="https://spark.apache.org/downloads.html">下载地址</a></h2><p>安装spark前需要安装scala （<a href="https://www.scala-lang.org/download/">scala下载地址</a>）</p><h2 id="2-上传至服务器并解压"><a href="#2-上传至服务器并解压" class="headerlink" title="2.上传至服务器并解压"></a>2.上传至服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.1.2-bin-hadoop3.2.tgz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">这里没有添加sbin目录，因为会和hadoop的sbin目录冲突</span></span><br><span class="line">export SPARK_HOME=/home/software/spark-3.1.2-bin-hadoop3.2</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><h2 id="4-配置conf"><a href="#4-配置conf" class="headerlink" title="4.配置conf"></a>4.配置conf</h2><h3 id="4-1-配置spark-env-sh"><a href="#4-1-配置spark-env-sh" class="headerlink" title="4.1 配置spark-env.sh"></a>4.1 配置spark-env.sh</h3><p>进入conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/spark-3.1.2-bin-hadoop3.2/conf/</span><br></pre></td></tr></table></figure><p>复制spark-env.sh.template，重命名为spark-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/home/software/hadoop-3.2.2/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/home/software/hadoop-3.2.2/bin/hadoop classpath)</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/home/software/scala-2.12.4</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/software/spark-3.1.2-bin-hadoop3.2</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=192.168.0.219</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="comment">#spark master节点的网页端口（默认是8080）可自行设置</span></span><br><span class="line"><span class="comment">#export SPARK_MASTER_WEBUI_PORT=8080</span></span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_CORES=2</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_INSTANCES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_MEMORY=2G</span><br><span class="line"></span><br><span class="line"><span class="comment">#spark worker节点的网页端口（默认是8081）可自行设置</span></span><br><span class="line"><span class="comment">#export SPARK_WORKER_WEBUI_PORT=8081</span></span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_CORES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_MEMORY=1G</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$HADOOP_HOME</span>/lib/native</span><br></pre></td></tr></table></figure><p>内容说明：</p><table><thead><tr><th>变量名</th><th align="left">说明</th></tr></thead><tbody><tr><td>JAVA_HOME</td><td align="left">jdk的安装目录</td></tr><tr><td>HADOOP_HOME</td><td align="left">hadoop的安装目录</td></tr><tr><td>HADOOP_CONF_DIR</td><td align="left">hadoop的配置文件存放目录</td></tr><tr><td>SCALA_HOME</td><td align="left">scala的安装目录</td></tr><tr><td>SPARK_HOME</td><td align="left">spark的安装目录</td></tr><tr><td>SPARK_MASTER_IP</td><td align="left">spark主节点绑定的ip地址</td></tr><tr><td>SPARK_MASTER_PORT</td><td align="left">spark主节点绑定的端口号</td></tr><tr><td>SPARK_MASTER_WEBUI_PORT</td><td align="left">spark master节点的网页端口（默认是8088）</td></tr><tr><td>SPARK_WORKER_CORES</td><td align="left">worker使用的cpu核心数</td></tr><tr><td>SPARK_WORKER_INSTANCES</td><td align="left">最多能够同时启动的EXECUTOR的实例个数</td></tr><tr><td>SPARK_WORKER_MEMORY</td><td align="left">worker分配的内存数量</td></tr><tr><td>SPARK_WORKER_WEBUI_PORT</td><td align="left">worker的网页查看绑定的端口号（默认是8081）</td></tr><tr><td>SPARK_EXECUTOR_CORES</td><td align="left">每个executor分配的cpu核心数</td></tr><tr><td>SPARK_EXECUTOR_MEMORY</td><td align="left">每个executor分配的内存数</td></tr><tr><td>LD_LIBRARY_PATH</td><td align="left">指定查找共享库</td></tr></tbody></table><h3 id="4-2-配置workers"><a href="#4-2-配置workers" class="headerlink" title="4.2 配置workers"></a>4.2 配置workers</h3><p>复制workers.template，重命名为workers</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp workers.template workers</span><br></pre></td></tr></table></figure><p>添加ip地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.219</span><br></pre></td></tr></table></figure><h3 id="4-3-配置spark-defaults-conf"><a href="#4-3-配置spark-defaults-conf" class="headerlink" title="4.3 配置spark-defaults.conf"></a>4.3 配置spark-defaults.conf</h3><p>复制spark-defaults.conf.template，重命名为spark-defaults.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://192.168.0.219:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://192.168.0.219:9000/sparkLogs</span><br><span class="line">spark.history.fs.logDirectory    hdfs://192.168.0.219:9000/sparkLogs</span><br></pre></td></tr></table></figure><p>内容说明：</p><table><thead><tr><th align="left">变量名</th><th>说明</th></tr></thead><tbody><tr><td align="left">spark.master</td><td>spark主节点所在机器及端口，默认写法是spark:&#x2F;&#x2F;</td></tr><tr><td align="left">spark.eventLog.enabled</td><td>是否打开任务日志功能，默认为false</td></tr><tr><td align="left">spark.eventLog.dir</td><td>任务日志默认存放位置，配置为一个HDFS路径即可</td></tr><tr><td align="left">spark.history.fs.logDirectory</td><td>存放历史应用日志文件的目录</td></tr></tbody></table><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>进入spark的sbin目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/spark-3.1.2-bin-hadoop3.2/sbin/</span><br></pre></td></tr></table></figure><p>启动spark</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>因为这里没有配置spark&#x2F;sbin目录的环境变量 所以需要cd到spark的sbin目录下再进行启动（没配置此目录的环境变量是因为spark的启动文件 start-all.sh与hadoop的启动文件名重名，配了会发生冲突，解决办法可以将两个文件中的其中一个重命名即可，这里读者就没有进行相关的操作了，是直接全路径指定执行启动的）</p><p>查看java进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如有Master，Worker两个进程则说明伪分布式部署成功！</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成HBase</title>
      <link href="/post/e6cd08d.html"/>
      <url>/post/e6cd08d.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成HBase"><a href="#Hue集成HBase" class="headerlink" title="Hue集成HBase"></a>Hue集成HBase</h1><h2 id="1-修改hbase-site-xml"><a href="#1-修改hbase-site-xml" class="headerlink" title="1.修改hbase-site.xml"></a>1.修改hbase-site.xml</h2><p>增加如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--指定regionserver thrift端口号 默认9090 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1124<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-启动HBase，thrift服务"><a href="#2-启动HBase，thrift服务" class="headerlink" title="2.启动HBase，thrift服务"></a>2.启动HBase，thrift服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br><span class="line"></span><br><span class="line">hbase-daemon.sh start thrift -p 1124</span><br></pre></td></tr></table></figure><h2 id="3-修改hue-ini"><a href="#3-修改hue-ini" class="headerlink" title="3.修改hue.ini"></a>3.修改hue.ini</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[hbase]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">hbase_clusters</span>=(HBase|<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">1124</span>)</span><br><span class="line"></span><br><span class="line"><span class="attr">hbase_conf_dir</span>=/home/software/hbase-<span class="number">2.3</span>.<span class="number">6</span>/conf</span><br></pre></td></tr></table></figure><h2 id="4-启动Hue"><a href="#4-启动Hue" class="headerlink" title="4.启动Hue"></a>4.启动Hue</h2><p>进入hue根目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成MySQL</title>
      <link href="/post/2349cfef.html"/>
      <url>/post/2349cfef.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成MySQL"><a href="#Hue集成MySQL" class="headerlink" title="Hue集成MySQL"></a>Hue集成MySQL</h1><h2 id="1-添加MySQL方言"><a href="#1-添加MySQL方言" class="headerlink" title="1.添加MySQL方言"></a>1.添加MySQL方言</h2><p>进入hue根目录，添加到Hue Python虚拟环境中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/env/bin/pip install mysqlclient</span><br></pre></td></tr></table></figure><h2 id="2-修改hue-ini"><a href="#2-修改hue-ini" class="headerlink" title="2.修改hue.ini"></a>2.修改hue.ini</h2><p>需要把mysql的注释去掉，大概在1821行。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[notebook]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[interpreters]]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[mysql]]]</span></span><br><span class="line"><span class="attr">name</span> = MySQL</span><br><span class="line"><span class="attr">interface</span>=sqlalchemy</span><br><span class="line"><span class="attr">options</span>=<span class="string">&#x27;&#123;&quot;url&quot;: &quot;mysql://root:123456@192.168.0.219:3306/&quot;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[hive]]]</span></span><br><span class="line"><span class="attr">name</span>=Hive</span><br><span class="line"><span class="attr">interface</span>=hiveserver2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[databases]]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[[mysql]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nice_name</span>=<span class="string">&quot;My SQL DB&quot;</span></span><br><span class="line"><span class="attr">engine</span>=mysql</span><br><span class="line"><span class="attr">host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="attr">port</span>=<span class="number">3306</span></span><br><span class="line"><span class="attr">user</span>=root</span><br><span class="line"><span class="attr">password</span>=<span class="number">123456</span></span><br></pre></td></tr></table></figure><h2 id="2-重启Hue"><a href="#2-重启Hue" class="headerlink" title="2.重启Hue"></a>2.重启Hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue集成Hive</title>
      <link href="/post/7d7d578d.html"/>
      <url>/post/7d7d578d.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hue集成Hive"><a href="#Hue集成Hive" class="headerlink" title="Hue集成Hive"></a>Hue集成Hive</h1><h2 id="1-时间同步"><a href="#1-时间同步" class="headerlink" title="1.时间同步"></a>1.时间同步</h2><p>安装ntp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp</span><br></pre></td></tr></table></figure><p>与上海电信服务器同步时间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate api.ntp.bz</span><br></pre></td></tr></table></figure><h2 id="2-修改hive-site-xml"><a href="#2-修改hive-site-xml" class="headerlink" title="2.修改hive-site.xml"></a>2.修改hive-site.xml</h2><p>进入hive的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/conf</span><br></pre></td></tr></table></figure><p>增加hive-site.xml配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hiveserver2运行绑定host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hiveserver2运行绑定端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>11240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 远程模式部署metastore服务地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.0.219:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-启动Hadoop"><a href="#2-启动Hadoop" class="headerlink" title="2.启动Hadoop"></a>2.启动Hadoop</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动任务历史服务器</span></span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><h2 id="3-修改hue-ini"><a href="#3-修改hue-ini" class="headerlink" title="3.修改hue.ini"></a>3.修改hue.ini</h2><p>进入hue的conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue/desktop/conf</span><br></pre></td></tr></table></figure><p><font color=#dd0000>修改hue.ini</font></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[beeswax]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">hive_server_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="comment">#hiveserver2的thrift端口号</span></span><br><span class="line"><span class="attr">hive_server_port</span>=<span class="number">11240</span></span><br><span class="line"><span class="attr">hive_conf_dir</span>=/home/software/apache-hive-<span class="number">3.1</span>.<span class="number">2</span>-bin/conf</span><br><span class="line"></span><br><span class="line"><span class="attr">server_conn_timeout</span>=<span class="number">120</span></span><br><span class="line"></span><br><span class="line"><span class="attr">auth_username</span>=root</span><br><span class="line"><span class="attr">auth_password</span>=<span class="number">123456</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[metastore]</span></span><br><span class="line"><span class="comment">#允许使用hive创建数据库表等操作</span></span><br><span class="line"><span class="attr">enable_new_create_table</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="4-启动hiveserver2和metastore服务"><a href="#4-启动hiveserver2和metastore服务" class="headerlink" title="4.启动hiveserver2和metastore服务"></a>4.启动hiveserver2和metastore服务</h2><p><font color=#dd0000>启动metastore</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="built_in">nohup</span> hive --service metastore &amp;</span></span><br></pre></td></tr></table></figure><p><font color=#dd0000>启动hiveserver2</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">或者</span></span><br><span class="line">nohup hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h2 id="5-启动hue"><a href="#5-启动hue" class="headerlink" title="5.启动hue"></a>5.启动hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Hue并集成Hadoop</title>
      <link href="/post/254593a4.html"/>
      <url>/post/254593a4.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器安装Hue"><a href="#云服务器安装Hue" class="headerlink" title="云服务器安装Hue"></a>云服务器安装Hue</h2><h2 id="1-源码下载"><a href="#1-源码下载" class="headerlink" title="1.源码下载"></a>1.源码下载</h2><p>Hue官网并没有提供二进制安装包，我们需要自行编译。<br>源码文件下载地址<a href="https://github.com/cloudera/hue/releases">https://github.com/cloudera/hue/releases</a></p><h2 id="1-上传解压"><a href="#1-上传解压" class="headerlink" title="1.上传解压"></a>1.上传解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;sourceSoftware&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hue-release-4.7.1.tar.gz  -C /home/software/sourceSoftware/</span><br></pre></td></tr></table></figure><h2 id="2-安装依赖包"><a href="#2-安装依赖包" class="headerlink" title="2.安装依赖包"></a>2.安装依赖包</h2><p>在编译之前必须安装各种依赖软件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel gmp-devel rsync openssl-devel</span><br></pre></td></tr></table></figure><h2 id="3-源码编译"><a href="#3-源码编译" class="headerlink" title="3.源码编译"></a>3.源码编译</h2><p>进入源码文件解压根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/sourceSoftware/hue-release-4.7.1</span><br></pre></td></tr></table></figure><p>执行以下命令进行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PREFIX=/home/software/hue-4.7.1 make install</span><br></pre></td></tr></table></figure><p>编译完成后在&#x2F;home&#x2F;software&#x2F;hue-4.7.1目录下就会有hue软件包</p><h2 id="4-配置环境变量"><a href="#4-配置环境变量" class="headerlink" title="4.配置环境变量"></a>4.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HUE_HOME=/home/software/hue-4.7.1/hue</span><br><span class="line">export PATH=$PATH:/$HUE_HOME/build/env/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="5-修改配置"><a href="#5-修改配置" class="headerlink" title="5.修改配置"></a>5.修改配置</h2><p>在 core-site.xml 中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- HUE --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- #设置Hadoop集群的代理用户 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- #设置Hadoop集群的代理用户组 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 hdfs-site.xml 中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- datanode 通信是否使用域名,默认为false，改为true --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether datanodes should use datanode hostnames when</span><br><span class="line">        connecting to other datanodes for data transfer.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在yarn-site.xml中增加配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--打开HDFS上日志记录功能--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--在HDFS上聚合的日志最长保留多少秒。3天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>259200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在mapred-site.xml中增加配置，否则运行mapreduce任务会报错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">               &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">       &lt;property&gt;</span><br><span class="line">               &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">               &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">       &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>增加 httpfs-site.xml 文件，加入配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- HUE --&gt;</span>        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span>        </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="6-配置hue-ini"><a href="#6-配置hue-ini" class="headerlink" title="6.配置hue.ini"></a>6.配置hue.ini</h2><p>进入hue的desktop&#x2F;conf目录，里面有一个pseudo-distributed.ini.tmpl文件，复制一份改名为hue.ini</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/software/hue-4.7.1/hue/desktop/conf</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp pseudo-distributed.ini.tmpl  hue.ini</span><br></pre></td></tr></table></figure><p>修改hue.ini</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[desktop]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">secret_key</span>=jFE93j<span class="comment">;2[290-eiw.KEiwN2s3[&#x27;d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span></span><br><span class="line"></span><br><span class="line"><span class="attr">http_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"><span class="attr">http_port</span>=<span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">time_zone</span>=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="attr">server_user</span>=root</span><br><span class="line"><span class="attr">server_group</span>=root</span><br><span class="line"></span><br><span class="line"><span class="attr">default_user</span>=root</span><br><span class="line"><span class="attr">default_hdfs_superuser</span>=root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[database]]</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">engine</span>=mysql</span><br><span class="line">    <span class="attr">host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line">    <span class="attr">port</span>=<span class="number">3306</span></span><br><span class="line">    <span class="attr">user</span>=root</span><br><span class="line">    <span class="attr">password</span>=<span class="number">123456</span></span><br><span class="line">    <span class="attr">name</span>=hue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[hadoop]</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[hdfs_clusters]]</span></span><br><span class="line"><span class="section">[[[default]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fs_defaultfs</span>=hdfs://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">9000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">webhdfs_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">9870</span>/webhdfs/v1</span><br><span class="line"></span><br><span class="line"><span class="attr">hadoop_conf_dir</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span>/etc/hadoop</span><br><span class="line"><span class="attr">hadoop_hdfs_home</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span></span><br><span class="line"><span class="attr">hadoop_bin</span>=/home/software/hadoop-<span class="number">3.2</span>.<span class="number">2</span>/bin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[[yarn_clusters]]</span></span><br><span class="line"><span class="section">[[[default]]]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_host</span>=<span class="number">192.168</span>.<span class="number">0.219</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_port</span>=<span class="number">8032</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resourcemanager_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">7776</span></span><br><span class="line"><span class="attr">proxy_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">7776</span></span><br><span class="line"><span class="attr">history_server_api_url</span>=http://<span class="number">192.168</span>.<span class="number">0.219</span>:<span class="number">19888</span></span><br></pre></td></tr></table></figure><p>在mysql中创建hue数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database hue <span class="keyword">default</span> <span class="type">character</span> <span class="keyword">set</span> utf8 <span class="keyword">default</span> <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure><p>去hue&#x2F;build&#x2F;env&#x2F;bin目录下，执行以下命令，在mysql的hue数据库里看到对应的hue元数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">新建数据库并初始化</span></span><br><span class="line">cd hue/build/env/bin</span><br><span class="line">./hue syncdb</span><br><span class="line">./hue migrate</span><br></pre></td></tr></table></figure><h2 id="7-启动hue"><a href="#7-启动hue" class="headerlink" title="7.启动hue"></a>7.启动hue</h2><p>进入hue根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hue-4.7.1/hue</span><br></pre></td></tr></table></figure><p>这里必须登录zyw用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su zyw</span><br></pre></td></tr></table></figure><p>启动hue并挂在到后台</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./build/env/bin/supervisor &amp;</span><br></pre></td></tr></table></figure><p>启动之后，在Web浏览器访问8000端口即可进入hue界面。<br><strong>登录用户：root<br>密码：123456</strong></p>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase问题解决</title>
      <link href="/post/7542b90f.html"/>
      <url>/post/7542b90f.html</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase问题解决"><a href="#Hbase问题解决" class="headerlink" title="Hbase问题解决"></a>Hbase问题解决</h1><p><strong><font color=Red><br>报错：关闭HBase时无法找到Master：no hbase master found<br></font></strong></p><p><font color=OrangeRed size=5><b>原因</b></font></p><p>此时可以大体确定报错原因，系统找不到HBase的pid文件，pid文件里面是HBase的进程号，找不到进程号系统就没有办法去结束这个进程。</p><p>HBase的pid文件默认存放路径为 &#x2F;tmp 路径，可以进去看一下有没有和HBase相关的文件。肯定没有，因为很有可能被操作系统删掉了。</p><p><font color=#0066FF size=5><b>解决方案</b></font></p><p>修改pid文件存放路径，</p><p>进入 hbase 的 conf 目录，找到 hbase-env.sh 进行修改</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_PID_DIR=/home/software/hbase-2.3.6/pids</span><br></pre></td></tr></table></figure><p>之后重新启动即可。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署HBase</title>
      <link href="/post/f2089145.html"/>
      <url>/post/f2089145.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器伪分布式部署HBase"><a href="#云服务器伪分布式部署HBase" class="headerlink" title="云服务器伪分布式部署HBase"></a>云服务器伪分布式部署HBase</h2><h2 id="1-解压tar包"><a href="#1-解压tar包" class="headerlink" title="1.解压tar包"></a>1.解压tar包</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hbase-2.3.6-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="2-配置conf"><a href="#2-配置conf" class="headerlink" title="2.配置conf"></a>2.配置conf</h2><h3 id="2-1-hbase-env-sh"><a href="#2-1-hbase-env-sh" class="headerlink" title="2.1 hbase-env.sh"></a>2.1 hbase-env.sh</h3><p>进入conf目录，在hbase-env.sh文件中添加如下，这里我不用HBase自带的Zookeeper，下面HBASE_MANAGES_ZK设置为fales，使用独立配置的Zookeeper。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><h3 id="2-2-hbase-env-sh"><a href="#2-2-hbase-env-sh" class="headerlink" title="2.2 hbase-env.sh"></a>2.2 hbase-env.sh</h3><p>在hbase-site.xml文件中添加如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.0.219:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!--配置zk本地数据存放目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hbase-2.3.6/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-regionservers"><a href="#2-2-regionservers" class="headerlink" title="2.2 regionservers"></a>2.2 regionservers</h3><p>在regionservers文件中添加服务器ip地址</p><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/home/software/hbase-2.3.6</span><br><span class="line">export PATH=$PATH:/$HBASE_HOME/bin</span><br></pre></td></tr></table></figure><h2 id="3-启动"><a href="#3-启动" class="headerlink" title="3.启动"></a>3.启动</h2><p>注意，在启动HBase之前，先启动Zookeeper，然后启动HBase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>jps进程出现HMaster，HRegionServer则说明启动成功，在浏览器输入IP地址和端口号16010，出现HMaster的Web界面。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Azkaban</title>
      <link href="/post/4995146.html"/>
      <url>/post/4995146.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Azkaban"><a href="#云服务器安装Azkaban" class="headerlink" title="云服务器安装Azkaban"></a>云服务器安装Azkaban</h1><h2 id="1-源码编译"><a href="#1-源码编译" class="headerlink" title="1.源码编译"></a>1.源码编译</h2><p>Azkaban官方并没有提供二进制安装包，需要我们自行编译。<br><a href="https://azkaban.github.io/">源码文件下载</a></p><h3 id="1-1编译环境"><a href="#1-1编译环境" class="headerlink" title="1.1编译环境"></a>1.1编译环境</h3><p>编译之前需要提前安装好Maven，Ant，Node等软件，还有git和gcc-c++环境。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum  install -y git</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum  install -y gcc-c++</span><br></pre></td></tr></table></figure><h3 id="1-1下载源码解压"><a href="#1-1下载源码解压" class="headerlink" title="1.1下载源码解压"></a>1.1下载源码解压</h3><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-4.0.0.tar.gz -C /home/software/</span><br></pre></td></tr></table></figure><h3 id="1-2源码文件编辑"><a href="#1-2源码文件编辑" class="headerlink" title="1.2源码文件编辑"></a>1.2源码文件编辑</h3><p>在编译之前对文件进行修改，避免编译过程出现问题。</p><h3 id="1-2-1修改build-gradle文件"><a href="#1-2-1修改build-gradle文件" class="headerlink" title="1.2.1修改build.gradle文件"></a>1.2.1修改build.gradle文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim build.gradle</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">repositories</span> &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/central&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/gradle-plugin&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/public&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/repository/google&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://maven.aliyun.com/nexus/content/groups/public/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://plugins.gradle.org/m2/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>具体参考<a href="https://blog.csdn.net/chenxi5404/article/details/120512109">https://blog.csdn.net/chenxi5404/article/details/120512109</a></p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">allprojects</span> &#123;</span><br><span class="line">  apply plugin: <span class="string">&#x27;jacoco&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">repositories</span> &#123;</span><br><span class="line">    mavenCentral()</span><br><span class="line">    mavenLocal()</span><br><span class="line"><span class="comment">//  need this for rest.li/pegasus 28.* artifacts until they are in Maven Central:</span></span><br><span class="line">    maven &#123;</span><br><span class="line">      url <span class="string">&#x27;https://linkedin.jfrog.io/artifactory/open-source/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体参考<a href="https://blog.csdn.net/NKDark0214/article/details/122601181">https://blog.csdn.net/NKDark0214/article/details/122601181</a></p><h3 id="1-2-2修改gradle-wrapper-properties文件"><a href="#1-2-2修改gradle-wrapper-properties文件" class="headerlink" title="1.2.2修改gradle-wrapper.properties文件"></a>1.2.2修改gradle-wrapper.properties文件</h3><p>Azkaban使用的是gradle进行构建的，我这里直接下载一个gradle安装包，使用的版本为4.6的。<br>把下载好的安装包上传到服务器上并移动到Azkaban的源码目录中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp gradle-4.6-bin.zip /home/software/azkaban-4.0.0/gradle/wrapper</span><br></pre></td></tr></table></figure><p>修改这个目录的gradle-wrapper.properties配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim gradle-wrapper.properties</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">distributionUrl</span>=<span class="string">gradle-4.6-bin.zip</span></span><br></pre></td></tr></table></figure><h3 id="1-3编译"><a href="#1-3编译" class="headerlink" title="1.3编译"></a>1.3编译</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure><p>出现BUILD SUCCESSFUL表示编译成功。<br>然后在<br>azkaban-exec-server&#x2F;build&#x2F;distributions&#x2F;<br>azkaban-web-server&#x2F;build&#x2F;distributions&#x2F;<br>azkaban-db&#x2F;build&#x2F;distributions&#x2F;<br>这些目录下会有相关安装包。</p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2.解压安装包"></a>2.解压安装包</h2><p>创建azkaban安装目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/software/azkaban</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-db-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br><span class="line">tar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br><span class="line">tar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz  -C /home/software/azkaban</span><br></pre></td></tr></table></figure><h2 id="3-MySQL初始化"><a href="#3-MySQL初始化" class="headerlink" title="3.MySQL初始化"></a>3.MySQL初始化</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-db-0.1.0-SNAPSHOT</span><br></pre></td></tr></table></figure><p>创建azkaban数据库，并加载初始化sql脚本。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database azkaban;</span><br><span class="line"></span><br><span class="line">use azkaban;</span><br><span class="line"></span><br><span class="line">source <span class="operator">/</span>home<span class="operator">/</span>software<span class="operator">/</span>azkaban<span class="operator">/</span>azkaban<span class="operator">-</span>db<span class="number">-0.1</span><span class="number">.0</span><span class="operator">-</span>SNAPSHOT<span class="operator">/</span><span class="keyword">create</span><span class="operator">-</span><span class="keyword">all</span><span class="operator">-</span><span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span><span class="operator">-</span>SNAPSHOT.sql</span><br></pre></td></tr></table></figure><h2 id="4-web-server服务器配置"><a href="#4-web-server服务器配置" class="headerlink" title="4.web-server服务器配置"></a>4.web-server服务器配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban</span><br></pre></td></tr></table></figure><h3 id="4-1生成ssl证书"><a href="#4-1生成ssl证书" class="headerlink" title="4.1生成ssl证书"></a>4.1生成ssl证书</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore keystore -alias jetty -genkey -keyalg RSA</span><br></pre></td></tr></table></figure><p>运行此命令后，会提示输入当前生成keystore的密码和相关信息，输入的密码请记住（所以密码均为123456）。</p><p>然后将证书复制到web-server服务器根目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp keystore azkaban-web-server-0.1.0-SNAPSHOT/</span><br></pre></td></tr></table></figure><h3 id="4-2配置azkaban-properties"><a href="#4-2配置azkaban-properties" class="headerlink" title="4.2配置azkaban.properties"></a>4.2配置azkaban.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-web-server-0.1.0-SNAPSHOT/conf</span><br></pre></td></tr></table></figure><p>修改如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jetty.use.ssl</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">jetty.port</span>=<span class="string">8443</span></span><br><span class="line"></span><br><span class="line"><span class="attr">executor.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">executor.port</span>=<span class="string">12321</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jetty.keystore</span>=<span class="string">keystore</span></span><br><span class="line"><span class="attr">jetty.password</span>=<span class="string">123456</span></span><br><span class="line"><span class="attr">jetty.keypassword</span>=<span class="string">123456</span></span><br><span class="line"><span class="attr">jetty.truststore</span>=<span class="string">keystore</span></span><br><span class="line"><span class="attr">jetty.trustpassword</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">mysql.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="attr">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">mysql.password</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">azkaban.use.multiple.executors</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#注释</span></span><br><span class="line"><span class="comment">#azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span></span><br></pre></td></tr></table></figure><h3 id="4-3配置commonprivate-properties"><a href="#4-3配置commonprivate-properties" class="headerlink" title="4.3配置commonprivate.properties"></a>4.3配置commonprivate.properties</h3><p>新建文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p plugins/jobtypes</span><br></pre></td></tr></table></figure><p>新建commonprivate.properties文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim commonprivate.properties</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">azkaban.native.lib</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">execute.as.user</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">memCheck.enabled</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure><h2 id="4-exec-server服务器配置"><a href="#4-exec-server服务器配置" class="headerlink" title="4.exec-server服务器配置"></a>4.exec-server服务器配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /home/software/azkaban/azkaban-exec-server-0.1.0-SNAPSHOT/conf</span><br></pre></td></tr></table></figure><h3 id="4-1配置azkaban-properties"><a href="#4-1配置azkaban-properties" class="headerlink" title="4.1配置azkaban.properties"></a>4.1配置azkaban.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim azkaban.properties</span><br></pre></td></tr></table></figure><p>修改如下</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"></span><br><span class="line"><span class="attr">azkaban.webserver.url</span>=<span class="string">https://192.168.0.219:8443</span></span><br><span class="line"></span><br><span class="line"><span class="attr">mysql.host</span>=<span class="string">192.168.0.219</span></span><br><span class="line"><span class="attr">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="attr">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">mysql.password</span>=<span class="string">123456</span></span><br><span class="line"></span><br><span class="line"><span class="attr">executor.port</span>=<span class="string">12321</span></span><br></pre></td></tr></table></figure><h2 id="5-启动"><a href="#5-启动" class="headerlink" title="5.启动"></a>5.启动</h2><p>先启动exec-server<br>再启动web-server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-exec.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-web.sh</span><br></pre></td></tr></table></figure><p><strong>注意，必须在安装包根目录下执行</strong><br>启动web-server后进程失败，可以通过安装包下对应启动日志进行排查</p><p>访问<a href="https://114.116.24.98:8443/">https://114.116.24.98:8443</a><br>进入AzkabanWeb页面<br><strong>Username：azkaban<br>password：azkaban</strong></p><h2 id="6-exec-server激活问题"><a href="#6-exec-server激活问题" class="headerlink" title="6.exec-server激活问题"></a>6.exec-server激活问题</h2><p><strong>每次关闭exec-server后都要重新激活execute</strong></p><p>进入exec-server根目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/azkaban/azkaban-exec-server-0.1.0-SNAPSHOT</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;192.168.0.219:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure><p>然后再重新启动web-server</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器安装Hive</title>
      <link href="/post/b607d502.html"/>
      <url>/post/b607d502.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器安装Hive"><a href="#云服务器安装Hive" class="headerlink" title="云服务器安装Hive"></a>云服务器安装Hive</h1><h2 id="1-下载Hive-下载地址"><a href="#1-下载Hive-下载地址" class="headerlink" title="1.下载Hive(下载地址)"></a>1.下载Hive(<a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/">下载地址</a>)</h2><h2 id="2-上传至服务器并解压"><a href="#2-上传至服务器并解压" class="headerlink" title="2.上传至服务器并解压"></a>2.上传至服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software&#x2F;目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-3.1.2-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下"><a href="#3-添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下" class="headerlink" title="3.添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下"></a>3.添加MySQL驱动包，拷贝到Hive的安装目录的依赖库目录下</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mysql-connector-java-5.1.46-bin.jar /home/software/apache-hive-3.1.2-bin/lib/</span><br></pre></td></tr></table></figure><h2 id="4-添加环境变量"><a href="#4-添加环境变量" class="headerlink" title="4.添加环境变量"></a>4.添加环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/home/software/apache-hive-3.1.2-bin</span><br><span class="line">export PATH=$PATH:/$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>使环境变量立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="4-修改配置文件"><a href="#4-修改配置文件" class="headerlink" title="4.修改配置文件"></a>4.修改配置文件</h2><h3 id="4-1-配置hive-env-sh文件"><a href="#4-1-配置hive-env-sh文件" class="headerlink" title="4.1 配置hive-env.sh文件"></a>4.1 配置hive-env.sh文件</h3><p>进入hive下的conf目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/conf/</span><br></pre></td></tr></table></figure><p>拷贝hive-env.sh.template文件，并命名为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template  hive-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line"></span><br><span class="line">export HIVE_CONF_DIR=/home/software/apache-hive-3.1.2-bin/conf</span><br><span class="line"></span><br><span class="line">export HIVE_AUX_JARS_PATH=/home/software/apache-hive-3.1.2-bin/lib</span><br></pre></td></tr></table></figure><p>第一个为Hadoop目录，第二个为Hive配置目录，最后一个为驱动jar包路径</p><h3 id="4-2-配置hive-site-xml文件"><a href="#4-2-配置hive-site-xml文件" class="headerlink" title="4.2 配置hive-site.xml文件"></a>4.2 配置hive-site.xml文件</h3><p>新建hive-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 本地模式 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 URL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.0.219:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!-- jdbc 连接的 Driver--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--新版本8.0版本的驱动为com.mysql.cj.jdbc.Driver--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--旧版本5.x版本的驱动为com.mysql.jdbc.Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 username(MySQL用户名)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 password(MySQL密码) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!-- Hive 元数据存储版本的验证(Hive元数据默认是存储在Derby中，正常开启时它会去校验Derby，现在要使用MySQL存储元数据，</span></span><br><span class="line"><span class="comment">               就需要把这个关闭即可，如果开启，MySQL和Derby会导致Hive启动不起来的) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive  默认在 HDFS 的工作目录(可以不配置，因为默认就是/user/hive/warehouse，如果不使用默认的位置，可以进行手动修改) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hiveserver2运行绑定host --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 自定义hiveserver2运行绑定端口，默认10000 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>11240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 远程模式部署metastore 服务地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.0.219:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 关闭元数据存储授权  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在HDFS中创建hive工作目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/hive/warehouse</span><br></pre></td></tr></table></figure><h2 id="5-配置log日志文件"><a href="#5-配置log日志文件" class="headerlink" title="5.配置log日志文件"></a>5.配置log日志文件</h2><p>拷贝并重命名hive-log4j2.properties.template为 hive-log4j2.properties文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-log4j2.properties.template  hive-log4j2.properties</span><br></pre></td></tr></table></figure><p>修改内容 property.hive.log.dir 这个属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">property.hive.log.dir = /home/software/apache-hive-3.1.2-bin/temp</span><br></pre></td></tr></table></figure><p>并在hive目录下创建temp文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/software/apache-hive-3.1.2-bin/temp</span><br></pre></td></tr></table></figure><h2 id="6-初始化元数据库"><a href="#6-初始化元数据库" class="headerlink" title="6.初始化元数据库"></a>6.初始化元数据库</h2><p>进入hive安装目录下的bin目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/apache-hive-3.1.2-bin/bin</span><br></pre></td></tr></table></figure><p>初始化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><p>可能出现问题，<a href="https://blog.csdn.net/m0_59705760/article/details/125116629">解决方案</a></p><p>出现Initialization script completed<br>schemaTool completed则说明初始化成功</p><h2 id="7-启动Hive"><a href="#7-启动Hive" class="headerlink" title="7.启动Hive"></a>7.启动Hive</h2><h3 id="7-1-本地模式启动"><a href="#7-1-本地模式启动" class="headerlink" title="7.1 本地模式启动"></a>7.1 本地模式启动</h3><p>本地模式特点是：<strong>需要安装MySQL来存储元数据，但是不需要启动metastore服务</strong><br>直接使用hive命令在本地启动客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><h3 id="7-2-远程模式启动"><a href="#7-2-远程模式启动" class="headerlink" title="7.2 远程模式启动"></a>7.2 远程模式启动</h3><p>远程模式特点是：<strong>需要安装MySQL来存储元数据，需要单独启动metastore服务</strong></p><p><em>nohup 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。<br>nohup 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME&#x2F;nohup.out 文件中。<br>参数说明：&amp;：让命令在后台执行，终端退出后命令仍旧执行。</em></p><p>这里使用nohup命令将将HiveServer2远程连接进程在后台运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p>HiveServer2通过metastore服务读写元数据，所以远程模式下，启动HiveHiveServer2之前必须启动metastore服务<br>先开启metastore服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>再启动HiveServer2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><h2 id="8-Hive客户端"><a href="#8-Hive客户端" class="headerlink" title="8.Hive客户端"></a>8.Hive客户端</h2><p>第一代客户端（不推荐使用）：$HIVE_HOME&#x2F;bin&#x2F;hive，是一个shellUtil，主要功能：一是可以用于交互或批处理模式运行Hive查询；二是用于Hive相关服务的启动，比如metastore服务。</p><p>第二代客户端（推荐使用）：$HIVE_HOME&#x2F;bin&#x2F;beeline，是一个JDBC客户端，是官方强烈推荐使用的Hive命令行工具，和第一代客户端相比，性能加强安全性提高。<br><strong>在远程模式下，beeline需要通过Thrift连接到单独的HiveServer2服务上，所以在使用前需要启动metastore服务和HiveServer2服务。</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://192.168.0.219:11240</span><br><span class="line"></span><br><span class="line">Connecting to jdbc:hive2://192.168.0.219:11240</span><br><span class="line">Enter username for jdbc:hive2://192.168.0.219:11240: root</span><br><span class="line">Enter password for jdbc:hive2://192.168.0.219:11240: 123456</span><br></pre></td></tr></table></figure><p>或者一步到位：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://192.168.0.219:11240 -n root</span><br></pre></td></tr></table></figure><p>设置mapreduce本地运行</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> mapreduce.framework.name=<span class="built_in">local</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7---ZooKeeper伪分布式安装部署</title>
      <link href="/post/9c657266.html"/>
      <url>/post/9c657266.html</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS7—ZooKeeper伪分布式安装部署"><a href="#CentOS7—ZooKeeper伪分布式安装部署" class="headerlink" title="CentOS7—ZooKeeper伪分布式安装部署"></a>CentOS7—ZooKeeper伪分布式安装部署</h1><h2 id="1-下载ZooKeeper安装包：下载地址"><a href="#1-下载ZooKeeper安装包：下载地址" class="headerlink" title="1. 下载ZooKeeper安装包：下载地址"></a>1. 下载ZooKeeper安装包：<a href="https://www.apache.org/dyn/closer.cgi/zookeeper">下载地址</a></h2><p>注意，随着版本的更新，3.5版本以后的压缩包分成了两种<br>带bin的压缩包是真正的标准压缩包<br>而不带bin的压缩包是源码压缩包<br>我们需要使用文件名带有bin 的那个压缩包，例如：apache-zookeeper-3.6.3-bin.tar.gz 这样解压后才会有lib目录下的那些jar包。</p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2. 解压安装包"></a>2. 解压安装包</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-zookeeper-3.6.3-bin.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-复制oo-example-cfg文件并改名"><a href="#3-复制oo-example-cfg文件并改名" class="headerlink" title="3. 复制oo_example.cfg文件并改名"></a>3. 复制oo_example.cfg文件并改名</h2><p>进入解压后的路径zookeeper-3.4.10&#x2F;conf路径下，复制zoo_example.cfg配置文件，因为是伪分布式安装，一台机器上有3个zookeeper进程，所以这里复制3份。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp zoo_example.cfg zoo1.cfg</span><br><span class="line">cp zoo_example.cfg zoo2.cfg</span><br><span class="line">cp zoo_example.cfg zoo3.cfg</span><br></pre></td></tr></table></figure><h2 id="4-分别输入如下配置"><a href="#4-分别输入如下配置" class="headerlink" title="4.分别输入如下配置"></a>4.分别输入如下配置</h2><h2 id="zoo1-cfg"><a href="#zoo1-cfg" class="headerlink" title="zoo1.cfg"></a>zoo1.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir1</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><h2 id="zoo2-cfg"><a href="#zoo2-cfg" class="headerlink" title="zoo2.cfg"></a>zoo2.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir2</span><br><span class="line">clientPort=2182</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><h2 id="zoo3-cfg"><a href="#zoo3-cfg" class="headerlink" title="zoo3.cfg"></a>zoo3.cfg</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/software/apache-zookeeper-3.6.3-bin/dataDir3</span><br><span class="line">clientPort=2183</span><br><span class="line">server.1=localhost:2887:3887</span><br><span class="line">server.2=localhost:2888:3888</span><br><span class="line">server.3=localhost:2889:3889</span><br></pre></td></tr></table></figure><p>tickTime - 心跳的时间ms<br>initLimit - 初始化限制时间<br>syncLimit - 同步限制时间<br>dataDir - 数据存放物理路径<br>clientPort - 供客户端连接的端口<br>server.n&#x3D;host:mainPort:selectPort – 表示集群的配置，其中n表示集群中的n个节点，host表示集群对应的ip，mainPort – 表示该节点作为主节点对应的端口，selectPort – 表示集群主节点选举时彼此通信的端口</p><h2 id="5-新建对应的dataDir文件夹和myid文件"><a href="#5-新建对应的dataDir文件夹和myid文件" class="headerlink" title="5.新建对应的dataDir文件夹和myid文件"></a>5.新建对应的dataDir文件夹和myid文件</h2><p>在ZooKeeper根目录下，新建dataDir文件夹，分别是zoo1，zoo2，zoo3的dataDir路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir dataDir1</span><br><span class="line">mkdir dataDir2</span><br><span class="line">mkdir dataDir3</span><br></pre></td></tr></table></figure><p>然后分别在每个dataDir中创建myid文件并写入zoo1，zoo2，zoo3的id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt;&gt;  dataDir1/myid</span><br><span class="line">echo 2 &gt;&gt;  dataDir2/myid</span><br><span class="line">echo 3 &gt;&gt;  dataDir3/myid</span><br></pre></td></tr></table></figure><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6. 启动"></a>6. 启动</h2><p>进入zookeeper的bin目录下，通过脚本文件zkServer.sh启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh start ../conf/zoo1.cfg</span><br><span class="line">./zkServer.sh start ../conf/zoo2.cfg</span><br><span class="line">./zkServer.sh start ../conf/zoo3.cfg</span><br></pre></td></tr></table></figure><p>查看每个zoo的状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh status ../conf/zoo1.cfg</span><br><span class="line">./zkServer.sh status ../conf/zoo2.cfg</span><br><span class="line">./zkServer.sh status ../conf/zoo3.cfg</span><br></pre></td></tr></table></figure><p>通过集群暴露的客户端连接接口，可以直接访问集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> ZooKeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10. Kafka 关键原理加强</title>
      <link href="/post/216d6a00.html"/>
      <url>/post/216d6a00.html</url>
      
        <content type="html"><![CDATA[<h1 id="10-Kafka-关键原理加强"><a href="#10-Kafka-关键原理加强" class="headerlink" title="10. Kafka 关键原理加强"></a>10. Kafka 关键原理加强</h1><h2 id="10-1-日志分段切分条件"><a href="#10-1-日志分段切分条件" class="headerlink" title="10.1 日志分段切分条件"></a>10.1 日志分段切分条件</h2><p><font size=3><b>日志分段文件切分包含以下4个条件，满足其一即可：</b></font></p><p>(1) <font size=3><b>当前日志分段文件的大小超过了broker端参数<u>log.segment.bytes</u>配置的值。<br>log.segment.bytes参数的默认值为1073741824，即1GB</b></font><br>(2) <font size=3><b>当前日志分段中消息的最小时间戳与当前系统的时间戳的差值大于<u>log.roll.ms</u>或++log.roll.hours++参数配置的值。如果同时配置了log.roll.ms和log.roll.hours参数，那么log.roll.ms的优先级高。默认情况下，只配置了log.roll.hours参数，其值为168，即7天。</b></font><br>(3) <font size=3><b>偏移量索引文件或时间戳索引文件的大小达到broker端参数<u>log.index.size.max.bytes</u>配置的值。log.index.size.max.bytes的默认值为10485760，即10MB</b></font><br>(4) <font size=3><b>追加的消息的偏移量与当前日志分段的起始偏移量之间的差值大于Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量(offset - baseOffset &gt; Integer.MAX_VALUE)。<br></b></font></p><h2 id="10-2-controller控制器"><a href="#10-2-controller控制器" class="headerlink" title="10.2 controller控制器"></a>10.2 controller控制器</h2><p><font size=3><b><u>Controller简单来说，就是kafka集群的状态管理者</u> <br>在kafka集群中会有一个或者多个broker，<u>其中有一个broker会被选举为控制器（Kafka Controller），</u><br><u>它负责维护整个集群中所有分区和副本的状态及分区leader的选举</u>。当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。当使用kafka-topic.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重新分配。</b></font>   </p><p><font size=3><b>Kafka中的控制器选举的工作依赖于Zookeeper，成功竞选为控制器的broker会在Zookeeper中创建&#x2F;controller这个临时(EPHEMERAL)节点，此临时节点的内容参考如下: <br>{“version”:1,”broker”:0,”timestamp”:”1529210278988”}<br>其中version在目前版本中固定为1，brokerid表示成为控制器的broker的id编号，timestamp表示竞选成为控制器时的时间戳。 <br><u>在任意时刻，集群中有且仅有一个控制器。</u>每个broker启动的时候会去尝试读取zookeeper上的&#x2F;controller节点的brokerid的值，如果读取到brokerid的值不为-1，则表示已经有其它broker节点成功竞选为控制器，所有当前就会放弃竞选；如果zookeeper不存在&#x2F;controller这个节点，或者这个节点中的数据异常，那么就会尝试去创建&#x2F;controller这个节点，当前broker去创建节点的时候，也有可能其它broker同时去尝试创建这个节点，只有创建成功的那个broker才会成为控制器，而创建失败的broker则表示竞选失败。每个broker都会在内存中保存当前控制器的brokerid值，这个值可以标识为activeControllerId。</b></font></p><p><font size=3><b><u>controller竞选机制，简单说，先来先上</u><br></b></font></p><p>具备控制器身份的broker需要比其它普通的broker多一些职责，具体细节如下：</p><ul><li><p>监听partition相关变化<br>  对Zookeeper中的&#x2F;admin&#x2F;reassign_partitions节点注册PartitionReassignmentListener，用来处理分区重分配的动作。<br>  对Zookeeper中的&#x2F;isr_change_notification节点注册IsrChangeNotificationListener，用来处理ISR集合变更的动作。<br>  对Zookeeper中的&#x2F;admin&#x2F;preferred-replica-election节点添加PreferredReplicaElectionListener，用来处理优先副本选举。</p></li><li><p>监听topic增减变化<br>  对Zookeeper中的&#x2F;brokers&#x2F;topics节点添加TopicChangeListener，用来处理topic增减的变化。<br>  对Zookeeper中的&#x2F;admin&#x2F;delete_topics节点添加TopicDeletionListener，用来处理删除topic的动作。</p></li><li><p>监听broker相关的变化<br>  对Zookeeper中的&#x2F;brokers&#x2F;ids&#x2F;节点添加BrokerChangeListener，用来处理broker增减的变化。</p></li><li><p>更新集群的元数据信息<br>  从Zookeeper中读取获取当前所有与topic、partition以及broker有关的信息并进行相应的管理，对各topic所对应的Zookeeper中的&#x2F;brokers&#x2F;topics&#x2F;[topic]节点添加PartitionModificationsListener，用来监听topic中的分区分配变化，并将最新信息同步给其它所有broker。</p></li><li><p>启动并管理分区状态机和副本状态机</p></li><li><p>如果参数auto.leader.rebalance.enable设置为true，则还会开启一个名为“auto-leader-rebalance-task”的定时任务来维护分区的leader副本的均衡</p></li></ul><h2 id="10-3-分区的负载分布"><a href="#10-3-分区的负载分布" class="headerlink" title="10.3 分区的负载分布"></a>10.3 分区的负载分布</h2><p><font size=3><b>客户端请求创建一个topic时，每一个分期副本在broker上的分配，是由集群controller来决定；<br>其分布策略源码如下：<br></b></font></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokersRackUnaware</span></span>(nPartitions: <span class="type">Int</span>,</span><br><span class="line">                                                 replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                                                 brokerList: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">                                                 fixedStartIndex: <span class="type">Int</span>,</span><br><span class="line">                                                 startPartitionId: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]]()</span><br><span class="line">    <span class="keyword">val</span> brokerArray = brokerList.toArray</span><br><span class="line">    <span class="keyword">val</span> startIndex = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">var</span> currentPartitionId = math.max(<span class="number">0</span>, startPartitionId)</span><br><span class="line">    <span class="keyword">var</span> nextReplicaShift = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nPartitions) &#123;</span><br><span class="line">      <span class="keyword">if</span> (currentPartitionId &gt; <span class="number">0</span> &amp;&amp; (currentPartitionId % brokerArray.length == <span class="number">0</span>))</span><br><span class="line">        nextReplicaShift += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length</span><br><span class="line">      <span class="keyword">val</span> replicaBuffer = mutable.<span class="type">ArrayBuffer</span>(brokerArray(firstReplicaIndex))</span><br><span class="line">      <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until replicationFactor - <span class="number">1</span>)</span><br><span class="line">        replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))</span><br><span class="line">      ret.put(currentPartitionId, replicaBuffer)</span><br><span class="line">      currentPartitionId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex : <span class="type">Int</span>, secondReplicaShift : <span class="type">Int</span>, replicaIndex : <span class="type">Int</span>, nBrokers : <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)</span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>副本因子不能大于Broker的个数；</li><li>partition_0的第1个副本(leader副本)放置位置是随机从brokerList选择的；</li><li>其它分区的第1个副本放置位置相对与partition_0分区依次往后移（也就是说如果我们有5个Broker，5个分区，假设partition0分区放在broker4上，那么partition1将会放在broker5上；partition2将会放在broker1上，partition3在broker2，依此类推）</li><li>各分区剩余的副本相对于分区前一个副本偏移随机数nextReplicaShift</li></ul><h2 id="10-4-分区Leader的选举机制"><a href="#10-4-分区Leader的选举机制" class="headerlink" title="10.4 分区Leader的选举机制"></a>10.4 分区Leader的选举机制</h2><p><u>分区leader副本的选举由控制器controller负责具体实施。</u><br>当创建分区(创建主题或增加分区都有创建分区的动作)或Leader下线(此时分区需要选举一个新的leader上线来对外提供服务)的时候都需要执行leader的选举动作。</p><p><u>选举策略：按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中；</u><br>一个分区的AR集合在partition分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变；</p><h2 id="10-5-分区数与吞吐量"><a href="#10-5-分区数与吞吐量" class="headerlink" title="10.5 分区数与吞吐量"></a>10.5 分区数与吞吐量</h2><p>Kafka本身提供用于生产者性能测试的 kafka-producer-perf-test.sh 和用于消费者性能测试的 kafka-consumer-perf-test.sh，主要参数如下：</p><ul><li>topic用来指定生产者发送消息的目标主题；</li><li>num-records用来指定发送消息的总条数</li><li>record-size用来设置每条消息的字节数；</li><li>producer-props参数用来指定生产者的配置，可同时指定多组配置，各组配置之间以空格分隔与producer-props参数对应的还有一个producer-config参数，它用来指定生产者的配置文件</li><li>throughput用来进行限流控制，当设定的值小于0时不限流，当设定的值大于0时，当发送的吞吐量大于该值时就会被阻塞一段时间。</li></ul><h2 id="10-6-生产者原理解析"><a href="#10-6-生产者原理解析" class="headerlink" title="10.6 生产者原理解析"></a>10.6 生产者原理解析</h2><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114154.jpeg" alt="Kafka图9"></p><p>一个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender线程。<br>在主线程中由kafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存<br>到消息累加器(RecordAccumulator，也称为消息收集器)中。</p><p>Sender线程负责从RecordAccumulator获取消息并将其发送到Kafka中；</p><p>RecordAccumulator主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。RecordAccumulator缓存的大小可以通过生产者客户端参数++buffer.memory++配置，默认值为33554432B，即32M。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer.send()方法调用要么被阻塞，要么抛出异常，这个取决于参数++max.block.ms++的配置，此参数的默认值为60000，即60秒。</p><p>主线程中发送过来的消息都会被追加到RecordAccumulator的某个双端队列(Deque)中，++RecordAccumulator内部为每个分区都维护了一个双端队列++，即Deque&lt;ProducerBatch&gt;。<br>消息写入缓存时，追加到双端队列的尾部；</p><p>Sender读取消息时，从双端队列的头部读取。注意：ProducerBatch是指一个消息批次；<br>与此同时，会将较小的ProducerBatch凑成一个较大ProducerBatch，也可以减少网络请求的次数以<br>提升整体的吞吐量。</p><p>ProducerBatch大小和++batch.size++参数也有着密切的关系。当一条消息(ProducerRecord)流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端<br>队列的尾部获取一个ProducerBatch(如果没有则新建)，查看ProducerBatch中是否还可以写入这个ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的Producer Batch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数大小，如果不超过，那么就以batch.size参数的大小来创建ProducerBatch。</p><p>如果生产者客户端需要向很多分区发送消息，则可以将buffer.memory参数适当调大以增加整体的吞吐量。</p><p>Sender从RecordAccumulator获取缓存的消息之后，会进一步将&lt;分区，Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node，List&lt;ProducerBatch&gt;&gt;的形式，其中Node表示Kafka集群broker节点。对于网络连接来说，生产者客户端是与具体broker节点建立的连接，也就是向具体的broker节点发送消息，而并不关心消息属于哪一个分区；而对于KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I&#x2F;O层面的转换。<br>在转换成&lt;Node，List&lt;ProducerBatch&gt;&gt;的形式之后，Sender会进一步封装成&lt;Node，Request&gt;的形式，这样就可以将Request请求发往各个Node了，这里的Request是Kafka各种协议请求；</p><p>请求在从sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为Map&lt;Nodeld，Deque&lt;request&gt;&gt;，++它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld是一个String类型，表示节点的id编号)++。与此同时，InFlightRequests还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与Node之间的连接)最多缓存的请求数。这个配置参数为++max.in.flight.request.per.connection++，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response)。++通过比较Deque&lt;Request&gt;的size与这个参数的大小来判断对应的Node中是否己经堆积了很多未响应的消息，如果真是如此，那么说明这个Node节点负载较大或网络连接有问题++，再继续发送请求会增大请求超时的可能。</p><h2 id="10-7-重要的生产者参数"><a href="#10-7-重要的生产者参数" class="headerlink" title="10.7 重要的生产者参数"></a>10.7 重要的生产者参数</h2><h3 id="10-7-1-acks"><a href="#10-7-1-acks" class="headerlink" title="10.7.1 acks"></a>10.7.1 acks</h3><p><font size=3><b>acks是控制生产者在发送出消息后如何得到确认；<br>生产者根据得到的确认信息，来判断消息发送是否成功；<br></b></font></p><table><thead><tr><th>acks</th><th>含义</th></tr></thead><tbody><tr><td>0</td><td>Producer往集群发送数据不需要等到集群的确认信息，不确保消息发送成功。安全性最低但是效率最高。</td></tr><tr><td>1</td><td>Producer往集群发送数据只要leader成功写入消息就可以发送下一条，只确保Leader接收成功。</td></tr><tr><td>-1或all</td><td>Producer往集群发送数据需要所有的ISR Follower都完成从Leader的同步才会发送下一条，确保Leader发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</td></tr></tbody></table><p>生产者将acks设置为all，是否就一定不会丢数据呢？<br>否！如果在某个时刻ISR列表只剩leader自己了，那么就算acks&#x3D;all，收到这条数据还是只有一个节点；</p><p>可以配合另外一个参数缓解此情况：最小同步副本数 &gt;&#x3D; 2<br>Broker端参数：min.insync.replicas（默认1）</p><h3 id="10-7-2-max-request-size"><a href="#10-7-2-max-request-size" class="headerlink" title="10.7.2 max.request.size"></a>10.7.2 max.request.size</h3><p><font size=3><b>这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即1MB<br>一般情况下，这个默认值就可以满足大多数的应用场景了。<br>这个参数还涉及一些其它参数的联动，比如broker端（topic级别参数）的++message.max.bytes++参数<br>(默认1000012)，如果配置错误可能会引起一些不必要的异常：比如将broker端的<br>message.max.bytes参数配置为10，而max.request.size参数配置为20，那么当发送一条大小为15B<br>的消息时，生产者客户端就会报出异常；<br></b></font></p><h3 id="10-7-3-retries-和-retry-backoff-ms"><a href="#10-7-3-retries-和-retry-backoff-ms" class="headerlink" title="10.7.3 retries 和 retry.backoff.ms"></a>10.7.3 retries 和 retry.backoff.ms</h3><p><u>retries参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作</u>。<br>消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、leader副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置retries大于0的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。<u>如果重试达到设定的次数，那么生产者就会放弃重试并返回异</u>常。</p><p>重试还和另一个参数<u>retry.backoff.ms</u>有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。</p><p>Kafka可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。对于某些应用来说，顺序性非常重要，比如MySQL binlog的传输，如果出现错误就会造成非常严重的后果；</p><p>如果将retries参数配置为非零值，并且<u>max.in.flight.requests.per.connection</u>参数配置为大于1的值，那可能会出现错序的现象：如果批次1消息写入失败，而批次2消息写入成功，那么生产者会重试发送批次1的消息，此时如果批次1的消息写入成功，那么这两个批次的消息就出现了错序。</p><p>一般而言，在需要保证消息顺序的场合建议把参数<u>max.in.flight.requests.per.connection</u>配置为1，而不是把retries配置为0，不过这样也会影响整体的吞吐。</p><h3 id="10-7-4-compression-type"><a href="#10-7-4-compression-type" class="headerlink" title="10.7.4 compression.type"></a>10.7.4 compression.type</h3><p><font size=3><b>这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。<br>该参数还可以配置为”gzip”，”snappy”和”lz4”。<br>对消息进行压缩可以极大地减少网络传输、降低网络I&#x2F;O，从而提高整体的性能。<br>消息压缩是一种以时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩；<br></b></font></p><h3 id="10-7-5-batch-size"><a href="#10-7-5-batch-size" class="headerlink" title="10.7.5 batch.size"></a>10.7.5 batch.size</h3><p><font size=3><b>每个Batch要存放batch.size大小的数据后，才可以发送出去。比如说++batch.size默认值是16KB++，那么里面凑够16KB的数据才会发送。<br>理论上来说，提升batch.size的大小，可以允许更多的数据缓冲在recordAccumulator里面，那么一次Request发送出去的数据量就更多了，这样吞吐量可能会有所提升。<br>但是batch.size也不能过大，要是数据老是缓冲在Batch里迟迟不发送出去，那么发送消息的延迟就<br>会很高。<br>一般可以尝试把这个参数调节大些，利用生产环境发消息负载测试一下。<br></b></font></p><h3 id="10-7-6-linger-ms"><a href="#10-7-6-linger-ms" class="headerlink" title="10.7.6 linger.ms"></a>10.7.6 linger.ms</h3><p><font size=3><b>这个参数用来指定生产者发送ProducerBatch之前等待更多消息(ProducerRecord)加入<br>ProducerBatch时间，默认值为0。<br>生产者客户端会在ProducerBatch填满或等待时间超过linger.ms值时发送出去。<br>增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。<br></b></font></p><h3 id="10-7-7-enable-idempotence"><a href="#10-7-7-enable-idempotence" class="headerlink" title="10.7.7 enable.idempotence"></a>10.7.7 enable.idempotence</h3><p>是否开启幂等性功能，详见后续原理加强；<br><u>幂等性，就是一个操作重复做，也不会影响最终的结果！</u><br>int a &#x3D; 1;<br>a++;  &#x2F;&#x2F;非幂等操作<br>val map &#x3D; new HashMap()<br>map.put(a,l); &#x2F;&#x2F;幂等操作</p><p>在kafka中，同一条消息，生产者如果多次重试发送，在服务器中的结果如果还是只有一条，这就是<br>具备幂等性；否则，就不具备幂等性！</p><h3 id="10-7-8-partitioner-class"><a href="#10-7-8-partitioner-class" class="headerlink" title="10.7.8 partitioner.class"></a>10.7.8 partitioner.class</h3><p>用来指定分区器，默认：org.apache.kafka.internals.DefaultPartitioner<br>默认分区器的分区规则：</p><ul><li>如果数据中有key，则按key的murmur hash值 % topie分区总数得到目标分区</li><li>如果数据只有value，则在各个分区间轮询</li></ul><p>自定义partitioner需要实现org.apache.kafka.clients.producer.Partitioner接口</p><h2 id="10-8-消费者组再均衡分区分配策略"><a href="#10-8-消费者组再均衡分区分配策略" class="headerlink" title="10.8 消费者组再均衡分区分配策略"></a>10.8 消费者组再均衡分区分配策略</h2><p>消费者组的意义何在？为了提高数据处理的并行度！</p><p>当以下事件发生时，kafka将会进行一次分区分配：</p><ul><li>同一个consumer group内新增或减少了消费者</li><li>订阅的主题新增分区</li><li>订阅的主题增加<br>  将分区的消费权从一个消费者移到另一个消费者称为再均衡(rebalance)，如何rebalance也涉及到分区分配策略。<br>  kafka内部存在两种的分区分配策略：range(默认)和round robin。<br>  (消费者组的分区分配策略&#x2F;消费者组的负载均衡策略&#x2F;消费者组的再均衡策略)</li></ul><h3 id="10-8-1-Range-Strategy"><a href="#10-8-1-Range-Strategy" class="headerlink" title="10.8.1 Range Strategy"></a>10.8.1 Range Strategy</h3><p><font size=3><b>先将消费者按照client.id字典排序，然后按topic逐个处理；<br>针对一个topic，将其partition总数&#x2F;消费者数 得到商n和余数m，则每个consumer至少分到n<br>个分区，且前m个consumer每人多分一个分区；<br></b></font></p><p><font size=3><b>举例说明1：假设有TOPIC_A有5个分区，由3个consumer(C1,C2,C3)来消费；<br>5&#x2F;3得到商1，余2，则每个消费者至少分1个分区，前两个消费者各多1个分区<br>C1:2个分区，C2:2个分区，C3:1个分区<br>接下来，就按照“区间”进行分配：<br>C1：TOPIC_A-0 TOPIC_A-1<br>C2：TOPIC_A-2 TOPIC_A_3<br>C3：TOPIC_A-4<br></b></font></p><p>举例说明2：假设TOPIC_A有5个分区，TOPIC_B有3个分区，由2个consumer(C1,C2)来消费</p><ul><li>先分配TOPIC_A<br>  5&#x2F;2得到商2，余1，则C1有3个分区，C2有2个分区，得到结果<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_A-2<br>  C2：TOPIC_A-3 TOPIC_A-4</li><li>再分配TOPIC_B<br>  3&#x2F;2得到商1，余1，则C1有2个分区，C2有1个分区，得到结果<br>  C1：TOPIC_B-0 TOPIC_B-1<br>  C2：TOPIC_B-2</li><li>最终分配结果：<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_A-2 TOPIC_B-0 TOPIC_B-1<br>  C2：TOPIC_A-3 TOPIC_A-4 TOPIC_B-2</li></ul><h3 id="10-8-2-Round-Robin-Strategy"><a href="#10-8-2-Round-Robin-Strategy" class="headerlink" title="10.8.2 Round-Robin Strategy"></a>10.8.2 Round-Robin Strategy</h3><p><font size=3><b>将所有主题分区组成TopicAndPartition列表，并对TopicAndPartition列表按照其hashCode排序,然后，以轮询的方式分配给各消费者<br></b></font></p><p>以上述“例2”来举例：</p><ul><li>先对TopicAndPartition的hashCode排序，假如排序结果如下：<br>  TOPIC_A-0 TOPIC_B-0 TOPIC_A-1 TOPIC_A-2 TOPIC_B-1 TOPIC_A-3 TOPIC_A-4 TOPIC_B-2</li><li>然后按轮询方式分配<br>  C1：TOPIC_A-0 TOPIC_A-1 TOPIC_B-1 TOPIC_A-4<br>  C2：TOPIC_B-0 TOPIC_A-2 TOPIC_A-3 TOPIC_B-2</li></ul><p>我们可以通过++partition.assignment.strategy++参数选择range或roundrobin。<br>partition.assignment.strategy参数默认的值是range。<br>partition.assignment.strategy&#x3D;org.apache.kafka.clients.consumer.RoundRobinAssignor<br>partition.assignment.strategy&#x3D;org.apache.kafka.clients.consumer.RangeAssignor</p><p>这个参数属于“消费者”参数！</p><h3 id="10-8-3-Sticky-Strategy"><a href="#10-8-3-Sticky-Strategy" class="headerlink" title="10.8.3 Sticky Strategy"></a>10.8.3 Sticky Strategy</h3><p>对应的类叫做：org.apache.kafka.clients.consumer.StickyAssignor<br>sticky策略的特点：</p><ul><li>要去达成最大化的均衡</li><li>尽可能保留各消费者原来分配的分区</li></ul><p>再均衡的过程中，还是会让各消费者先取消自身的分区，然后再重新分配（只不过是分配过程中会尽<br>量让原来属于谁的分区依然分配给谁)</p><h3 id="10-8-3-CooperativeSticky-Strategy"><a href="#10-8-3-CooperativeSticky-Strategy" class="headerlink" title="10.8.3 CooperativeSticky Strategy"></a>10.8.3 CooperativeSticky Strategy</h3><p>对应的类叫做：org.apache.kafka.clients.consumer.ConsumerPartitionAssignor<br>sticky策略的特点：</p><ul><li>逻辑与sticky策略一致</li><li>支持cooperative再均衡机制（再均衡的过程中，不会让所有消费者取消掉所有分区然后再进行重分配）</li></ul><h2 id="10-9-消费者组再均衡流程"><a href="#10-9-消费者组再均衡流程" class="headerlink" title="10.9 消费者组再均衡流程"></a>10.9 消费者组再均衡流程</h2><p><font size=3><b>消费组在消费数据的时候，有两个角色进行组内的各事务的协调：<br>角色1：Group Coordinator（组协调器）位于服务端（就是某个broker）<br>角色2：Group Leader（组长）位于消费端（就是消费组中的某个消费者）<br></b></font></p><h3 id="10-9-1-GroupCoordinator介绍"><a href="#10-9-1-GroupCoordinator介绍" class="headerlink" title="10.9.1 GroupCoordinator介绍"></a>10.9.1 GroupCoordinator介绍</h3><p><font size=3><b>每个消费组在服务端对应一个GroupCoordinator进行管理，GroupCoordinator是Kafka服务端中用<br>于管理消费组的组件。<br>消费者客户端中由ConsumerCoordinator组件负责与GroupCoordinator进行交互；<br>ConsumerCoordinator和GroupCoordinator最重要的职责就是负责执行消费者rebalance操作，包括前面提及的分区分配工作也是在rebalance期间完成的。<br></b></font></p><p>会触发rebalance的事件可能是如下任意一种：</p><ul><li>有新的消费者加入消费组。</li><li>有消费者宕机下线，消费者并不一定需要真正下线，例如遇到长时间的GC、网络延迟导致消<br>  费者长时间未向GroupCoordinator发送心跳等情况时，GroupCoordinator会认为消费者己下线。</li><li>有消费者主动退出消费组（发送LeaveGroupRequest请求）：比如客户端调用了unsubscrible()<br>  方法取消对某些主题的订阅。</li><li>消费组所对应的GroupCoorinator节点发生了变更。</li><li>消费组内所订阅的任一主题或者主题的分区数量发生变化。</li></ul><h3 id="10-9-2-再均衡流程"><a href="#10-9-2-再均衡流程" class="headerlink" title="10.9.2 再均衡流程"></a>10.9.2 再均衡流程</h3><p><font color=green size=4><b>阶段1：定位Group Coordinator<br></b></font></p><p>coordinator在我们组记偏移量的__consumer_offsets分区的leader所在broker上查找Group Coordinator的方式：</p><ul><li>先根据消费组groupid的hashcode值计算它应该所在__consumer_offsets中的分区编号；<br>  Utils.abc(gropId.hashCode)%groupMetadataTopicPartitionCount<br>  groupMetadataTopicPartitionCount为__consumer_offsets的分区总数，这个可以通过broker端参数<br>  offset.topic.num.partitions来配置，默认值是50；</li><li>找到对应的分区号后，再寻找此分区leader副本所在broker节点，则此节点即为自己的Grouping Coordinator；</li></ul><p><font color=green size=4><b>阶段2：加入组join the group<br></b></font></p><p>此阶段的重要操作之1：选举消费组的leader<br>private val members &#x3D; new mutable.HashMap[String, MemberMetadata]<br>var leaderid &#x3D; members.keys.head</p><p>消费组leader的选举，策略就是：随机！</p><p>此阶段的重要操作之2：选择分区分配策略<br>最终选举的分配策略基本上可以看作被各个消费者支持的最多的策略，具体的选举过程如下：<br>(1) 收集各个消费者支持的所有分配策略，组成候选集candidates<br>(2) 每个消费者从候选集candidates找出第一个自身支持的策略，为这个策略投上一票。<br>(3) 计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。</p><p>其实，此逻辑并不需要consumer来执行，而是由Group Coordinator来执行</p><p><font color=green size=4><b>阶段3：组信息同步SYNC group<br></b></font></p><p><font size=3><b>此阶段，主要是由消费组leader将分区分配方案，通过Group Coordinator来转发给组中各消费者<br></b></font></p><p><font color=green size=4><b>阶段4：心跳联系 HEART BEAT<br></b></font></p><p><font size=3><b>进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。<br>各消费者在消费数据的同时，保持与Group Coordinator的心跳通信；<br></b></font></p><p>消费者的心跳间隔时间由参数++heartbeat.interval.ms++指定，默认值为3000，即这个参数必须比++session.timeout.ms++参数设定的值要小；一般情况下heartbeat.interval.ms的配置值不能超过session.timeout.ms配置值的l&#x2F;3。这个参数可以调整得更低，以控制正常重新平衡的预期时间；</p><p>如果一个消费者发生崩溃，并停止读取消息，那么GroupCoordinator会等待一小段时间确认这个消费者死亡之后才会触发再均衡。在这一小段时间内，死掉的消费者并不会读取分区里的消息。<br>这个一小段时间由session.timeout.ms参数控制，该参数的配置值必须在broker端参数</p><h3 id="10-9-3-再均衡监听器"><a href="#10-9-3-再均衡监听器" class="headerlink" title="10.9.3 再均衡监听器"></a>10.9.3 再均衡监听器</h3><p><u>一个消费组中，一旦有消费者的增减发生，会触发消费者组的rebalance再均衡；</u><br>如果想控制消费者在发生再均衡时执行一些特定的工作，可以通过订阅主题时注册“再均衡监听器”来实现；</p><p>场景举例：在发生再均衡时，处理消费位移<br>如果A消费者消费掉的一批消息还没来得及提交offset，而它所负贵的分区在rebalance中转移给了B<br>消费者，则有可能发生数据的重复消费处理。此情形下，可以通过再均衡监听器做一定程度的补救；</p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">&quot;tpc_5&quot;</span>), <span class="keyword">new</span> <span class="title class_">ConsumerRebalanceListener</span>()&#123;</span><br><span class="line">        <span class="comment">//被取消旧分区后被调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">            <span class="comment">//store the current offset to db</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//分配到新的分区后被调用</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">            <span class="comment">//fetch the current offset from db</span></span><br><span class="line">I   &#125;</span><br><span class="line">&#125;)；</span><br></pre></td></tr></table></figure><h2 id="10-10-Kafka系统的CAP保证"><a href="#10-10-Kafka系统的CAP保证" class="headerlink" title="10.10 Kafka系统的CAP保证"></a>10.10 Kafka系统的CAP保证</h2><h3 id="10-10-1-分布式系统的CAP理论"><a href="#10-10-1-分布式系统的CAP理论" class="headerlink" title="10.10.1 分布式系统的CAP理论"></a>10.10.1 分布式系统的CAP理论</h3><p>CAP理论作为分布式系统的基础理论，它描述的是一个分布式系统在以下三个特性中：</p><ul><li>一致性(Consistency)</li><li>可用性(Availability)</li><li>分区容错性(Partition tolerance)</li></ul><p>最多满足其中的两个特性。也就是下图所描述的。分布式系统要么满足CA，要么CP，要么AP。无法同时满足CAP。</p><p>分区容错性：指的是分布式系统中的某个节点或者网络分区出现了故障的时候，整个系统仍然能对外提供满足一致性和可用性的服务。也就是说部分故障不影响整体使用。<br>事实上我们在设计分布式系统是都会考虑到bug，硬件，网络等各种原因造成的故障，所以即使部分节点或者网络出现故障，我们要求整个系统还是要继续使用的<br>(不继续使用，相当于只有一个分区，那么也就没有后续的一致性和可用性了)</p><p>可用性：一直可以正常的做读写操作。简单而言就是客户端一直可以正常访问并得到系统的正常响应。用户角度来看就是不会出现系统操作失败或者访问超时等问题。</p><p>一致性：在分布式系统完成某些操作后任何读操作，都应该获取到该写操作写入的那个最新的值。相当于要求分布式系统中的各节点时时刻刻保持数据的一致性。</p><p>Kafka作为一个商业级消息中间件，数据可靠性和可用性是优先考虑的重点，兼顾尽可能保证数据一致性；</p><h3 id="10-10-2-分区副本机制"><a href="#10-10-2-分区副本机制" class="headerlink" title="10.10.2 分区副本机制"></a>10.10.2 分区副本机制</h3><p><font size=3><b>kafka从0.8.0版本开始引入了分区副本：引入了数据冗余<br>也就是说每个分区可以人为的配置几个副本（创建主题的时候指定replication-factor，也可以在broker级别进行配置default.replication.factor）；<br>在众多的分区副本里面有一个副本是Leader，其余的副本是follower，所有的读写操作都是经过Leader<br>进行的，同时follower会定期地去leader上复制数据。当Leader挂了的时候，其中一个follower会<br>重新成为新的Leader。通过分区副本，引入了数据冗余，同时也提供了kafka的数据可靠性。<br>++Kaka的分区多副本架构是Kaka可靠性保证的核心，把消息写入多个副本可以使Kaka在发生<br>崩溃时仍能保证消息的持久性。++<br></b></font></p><h3 id="10-10-3-ISR同步副本列表"><a href="#10-10-3-ISR同步副本列表" class="headerlink" title="10.10.3 ISR同步副本列表"></a>10.10.3 ISR同步副本列表</h3><p><font size=3><b>ISR概念：（同步副本）。每个分区的leader会维护一个ISR列表，ISR列表里面就是follower副本的broker编号，只有跟得上Leader的follower副本才能加入到ISR里面，这个是通过<u>replica.lag.time.max.ms</u>&#x3D;l0000(默认值)参数配置的，<u>只有ISR里的成员才有被选为leader的可能。</u><br></b></font></p><h3 id="10-10-4-分区副本的数据一致性解决方案"><a href="#10-10-4-分区副本的数据一致性解决方案" class="headerlink" title="10.10.4 分区副本的数据一致性解决方案"></a>10.10.4 分区副本的数据一致性解决方案</h3><p><font size=3><b>kafka让分区多副本同步的基本手段是：follower副本定期向leader请求数据同步！<br>既然是定期同步，则leader和follower之间必然存在各种数据不一致的情景！<br></b></font></p><p>动态过程中的副本数据不一致，是很难解决的；<br>kafka先尝试着解决上述“消费者所见不一致”及“分区数据最终不一致”的问题；</p><p>解决方案的核心思想</p><ul><li>在动态不一致的过程中，维护一条步进式的“临时一致线”（既所谓的High Watermark);</li><li>高水位线HW&#x3D;ISR副本中最小LEO+1；</li><li>底层逻辑就是：offset &lt; HW的message，是各副本间一致的且安全的！</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114824.png" alt="Kafka图11"></p><p><font size=3><b>如上图所示：offset &lt; HW:3 的message，是所有副本都已经备份好的数据<br></b></font></p><p><font color=DeepPink size=3><b>解决“消费者所见不一致”（消费者只允许看到HW以下的message）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114562.png" alt="Kafka图12"></p><p><font color=DeepPink size=3><b>解决“分区副本数据最终不一致”（follower数据按HW截断）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114124.png" alt="Kafka图13"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114196.png" alt="Kafka图14"></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114067.png" alt="Kafka图15"></p><h3 id="10-10-5-HW方案的天生缺陷"><a href="#10-10-5-HW方案的天生缺陷" class="headerlink" title="10.10.5 HW方案的天生缺陷"></a>10.10.5 HW方案的天生缺陷</h3><p>如前所述，看似HW解决了“分区数据最终不一致”的问题，以及“消费者所见不一致”的问题，但其实，这里面存在一个巨大的隐患，导致：</p><ul><li>“分区数据最终不一致”的问题依然存在</li><li>producer设置acks&#x3D;al后，依然有可能丢失数据的问题</li></ul><p>产生如上结果的根源是：++HW高水位线的更新，与数据同步的进度，存在迟滞！++</p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114646.png" alt="Kafka图16"></p><p>Step 1： leader和follower副本处于初始化值，follower副本发送fetch请求，由于leader副本没有数据，因此不会进行同步操作；</p><p>Step2： 生产者发送了消息m1到分区leader副本，写入该条消息后leader更新LEO&#x3D;1；</p><p>Step3： follower发送fetch请求，携带当前最新的offset&#x3D;0，leader处理fetch请求时，更新remote LEO&#x3D;0，对比LEO值最小为0，所以HW&#x3D;0，leader副本响应消息数据及leader HW&#x3D;0给follower，follower写入消息后，更新LEO值，同时对比leader HW值，取最小的作为新的HW值，此时follower HW&#x3D;0，这也意味着，follower HW是不会超过leader HW值的。</p><p>Step4： follower发送第二轮fetch请求，携带当前最新的offset&#x3D;l，leader处理fetch请求时，更新remote LEO&#x3D;l，对比LEO值最小为l，所以HW&#x3D;l，此时leader没有新的消息数据，所以直接返回leader HW&#x3D;1给follower，follower对比当前最新的LEO值与leader HW值，取最小的作为新的HW值，此时follower HW&#x3D;1。</p><p><font size=3><b>从以上步骤可看出，leader中保存的remote LEO值的更新（也即HW的更新)总是需要额外一轮<br>fetch RPC请求才能完成，这意味着在leader切换过程中，会存在数据丢失以及数据不一致的问题！<br></b></font></p><h3 id="10-10-6-HW会产生数据丢失和副本最终不一致问题"><a href="#10-10-6-HW会产生数据丢失和副本最终不一致问题" class="headerlink" title="10.10.6 HW会产生数据丢失和副本最终不一致问题"></a>10.10.6 HW会产生数据丢失和副本最终不一致问题</h3><p><font color=Crimson size=3><b>数据丢失的问题（即使produce设置acks&#x3D;all，依然会发生）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114586.png" alt="Kafka图17"></p><p><font color=red size=3><b>注意：leader中的HW值是在follower下一轮fetch RPC请求中完成更新的<br></b></font></p><p>如上图所示：</p><ul><li>状态起始：B为leader，A为follower；最新消息m2已同步，但B的HW比A的HW大1</li><li>A在此时崩溃（即follower没能通过下一轮请求来更新HW值）</li><li>A重启时，会自动将LEO值调整到之前的HW值，即会进行日志截断</li><li>B重启后，会从向A发送fetch请求，收到fetch响应后，拿到HW值，并更新本地HW值，这时B会做日志截断，因此，offsets&#x3D;1的消息被永久地删除了。</li></ul><p><font color=Crimson size=3><b>副本间数据最终不一致的问题（即使produce设置acks&#x3D;all，依然会发生）<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114328.png" alt="Kafka图18"></p><p>如上图所示：</p><ul><li>状态起始：A为leader，B为follower；最新消息m2已同步，但B的HW比A的HW大1</li><li>A在此时崩溃（即follower没能通过下一轮请求来更新HW值）</li><li>B先重启，会自动将LEO值调整到之前的W值，即会进行日志截断，并在此刻接收了新的消息m3，HW随之上升为2</li><li>然后，A重启上线，会从向B发送fetch请求，收到fetch响应后，拿到HW值，并更新本<br>  地HW值，发现不需要截断，从而己经产生了“副本间数据最终不一致”！</li></ul><p><font  size=3><b>只要新一届leader在老leader重启上线前，接收了新的数据，就可能发生上图中的场景，根源也在于HW的更新落后于数据同步进度<br></b></font></p><h3 id="10-10-7-Leader-Epoch机制的引入"><a href="#10-10-7-Leader-Epoch机制的引入" class="headerlink" title="10.10.7 Leader-Epoch机制的引入"></a>10.10.7 Leader-Epoch机制的引入</h3><p><font size=3><b>为了解决HW更新时机是异步延迟的，而HW又是决定日志是否备份成功的标志，从而造成数据丢失和数据不一致的现象，Kafka引入了leader epoch机制；<br>在每个副本日志目录下都创建一个leader-epoch-checkpoint文件，用于保存leader的epoch信息；<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132114771.png" alt="Kafka图19"></p><p><font size=3><b>它的格式为(epoch offset)，epoch指的是leader版本，它是一个单调递增的一个正整数值，每次leader变更，epoch版本都会+l，offset是每一代leader写入的第一条消息的位移值，比如：<br>(0,0)<br>(1,300)<br>以上第2个版本是从位移300开始写入消息，意味着第一个版本写入了0-299的消息。<br></b></font></p><p>leader epoch具体的工作机制</p><ul><li><p>当副本成为leader时：<br>  这时，如果此时生产者有新消息发送过来，会首先更新leader epoch以及LEO，并添加到<br>  leader–epoch-checkpoint文件中；</p></li><li><p>当副本变成follower时：<br>  发送LeaderEpochRequest请求给leader副本，该请求包括了follower中最新的epoch版本；<br>  leader返回给follower的响应中包含了一个LastOffset，如果follower last epoch&#x3D;leader last epoch(纪元相同)，则LastOffset&#x3D;leader LEO，否则取follower last epoch中最小的leader epoch的start offset值；</p></li></ul><p><font size=3><b>举个例子：假设follower last epoch&#x3D;1，此时leader有(1,20)(2,80)(3,120)，则LastOffset&#x3D;80；follower拿到LastOffset之后，会对比当前LEO值是否大于LastOffset，如果当前LEO大于LastOffset，则从LastOffset截断日志；<br>follower开始发送fetch请求给leader保持消息同步。<br></b></font></p><p><font color=OrangeRed size=3><b>解决数据丢失：<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132115476.png" alt="Kafka图20"></p><p><font size=3><b>如上图所示：<br>A重启之后，发送LeaderEpochRequest请求给B，由于B还没追加消息，此时epoch&#x3D;request epoch<br>&#x3D;0，因此返LastOffset&#x3D;leader LEO&#x3D;2给A<br>A拿到LastOffset之后，发现等于当前LEO值，故不用进行日志截断。就在这时B宕机了，A成为leader，在B启动回来后，会重复A的动作，同样不需要进行日志截断，数据没有丢失。<br></b></font></p><p><font color=OrangeRed size=3><b>解决数据最终不一致问题：<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132115693.png" alt="Kafka图21"></p><p>如上图所示：</p><ul><li>A和B同时宕机后，B先重启回来成为分区leader，这时候生产者发送了一条消息过来，leader<br>  epoch更新到1</li><li>此时A启动回来后，发送LeaderEpochRequest(follower epoch&#x3D;0)给B，B判断follower epoch<br>  不等于最新的epoch，于是找到大于follower epoch最小的epoch&#x3D;l，即LastOffset&#x3D;epoch start offset&#x3D;1</li><li>A拿到LastOffset后，判断小于当前LEO值，于是从LastOffset位置进行日志截断，接着开<br>  始发送fetch请求给B开始同步消息，避免了消息不一致&#x2F;离散的问题。</li></ul><h3 id="10-10-8-LEO-HW-LSO等相关术语速查"><a href="#10-10-8-LEO-HW-LSO等相关术语速查" class="headerlink" title="10.10.8 LEO&#x2F;HW&#x2F;LSO等相关术语速查"></a>10.10.8 LEO&#x2F;HW&#x2F;LSO等相关术语速查</h3><p><font size=3><b>LEO：(last end offset) 就是该副本中消息的最大偏移量的值+1；<br>HW：(high watermark) 各副本中LEO的最小值。这个值规定了消费者仅能消费HW之前的数据；<br>LW：(low watermark) 一个副本的log中，最小的消息偏移量；     <br>LS0：(last stable offset) 最后一个稳定的offset；<br>对未完成的事务而言，LSO的值等于事务<br>中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同HW相同；<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8-9 Kafka 系统架构</title>
      <link href="/post/b2c5a78a.html"/>
      <url>/post/b2c5a78a.html</url>
      
        <content type="html"><![CDATA[<h1 id="8-Kafka-系统架构"><a href="#8-Kafka-系统架构" class="headerlink" title="8. Kafka 系统架构"></a>8. Kafka 系统架构</h1><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051118.png" alt="Kafka图2"></p><p><font size=3><b>自我推导设计：</b></font></p><ul><li><font size=3><b>kafka是用来存数据的</b></font></li><li><font size=3><b>现实世界数据有分类，所以存储系统也应有数据分类管理功能，如mysql的表；kafka有topic</b></font></li><li><font size=3><b>如一个topic的数据全部交给一台server存储和管理，则读写吞吐量有限</b></font></li><li><font size=3><b>所以，一个topic的数据应该可以分成多个部分(partition)分别交给多台server存储和管理</b></font></li><li><font size=3><b>如果一台server宕机，这台server负责的partition将不可用，所以，一个partition应有多个副本</b></font></li><li><font size=3><b>一个partition有多个副本，则副本间的数据一致性难以保证，因此要有一个leader统领读写</b></font></li><li><font size=3><b>一个leader万一挂掉，则该partition又不可用，因此还要有leader的动态选举机制</b></font></li><li><font size=3><b>集群有哪些topic，topic有哪几个分区，server在线情况，等等元信息和状态信息需要在集群内部及客户端之间共享，则引入了zookeeper</b></font></li><li><font size=3><b>客户端在读取数据时，往往需要知道自己所读取的位置，因而要引入消息偏移量维护机制<br>  </b></font></li></ul><h2 id="8-1-broker服务器"><a href="#8-1-broker服务器" class="headerlink" title="8.1 broker服务器"></a>8.1 broker服务器</h2><p><font size=3><b>一台kafka服务器就是一个broker，一个kafka集群由多个broker组成。<br></b></font></p><h2 id="8-2-生产者producer"><a href="#8-2-生产者producer" class="headerlink" title="8.2 生产者producer"></a>8.2 生产者producer</h2><p><font size=3><b>消息生产者，就是向kafka broker发消息的客户端。<br></b></font></p><h2 id="8-3-消费者consumer"><a href="#8-3-消费者consumer" class="headerlink" title="8.3 消费者consumer"></a>8.3 消费者consumer</h2><p><font size=3><b>consumer：消费者，从kafka broker取消息的客户端。<br>consuemr group：消费组，单个或多个consumer可以组成一个消费组。<br></b></font></p><h2 id="8-4-主题Topic与分区Partition"><a href="#8-4-主题Topic与分区Partition" class="headerlink" title="8.4 主题Topic与分区Partition"></a>8.4 主题Topic与分区Partition</h2><p><font size=3><b>在 Kafka 中消息是以 Topic 为单位进行归类的，Topic 在逻辑上可以被认为是一个 Queue，Producer 生产的每一条消息都必须指定一个 Topic，然后 Consumer 会根据订阅的 Topic 到对应的 broker 上去拉取消息。</b></font></p><p><font size=3><b>为了提升整个集群的吞吐量，Topic 在物理上还可以细分多个分区，一个分区在磁盘上对应一个文件夹。由于一个分区只属于一个主题，很多时候也会被叫做主题分区(Topic-Partition)。<br></b></font></p><h2 id="8-5-分区副本replica"><a href="#8-5-分区副本replica" class="headerlink" title="8.5 分区副本replica"></a>8.5 分区副本replica</h2><p><font size=3><b>每个topic的每个partition都可以配置多个副本(replica)，以提高数据的可靠性；<br>每个partition的所有副本中，必有一个leader副本，其它的就是follow副本(observer副本)；<br>follow定期找leader同步最新的数据，对外提供服务只有leader；<br></b></font></p><h2 id="8-6-分区副本leader"><a href="#8-6-分区副本leader" class="headerlink" title="8.6 分区副本leader"></a>8.6 分区副本leader</h2><p><font size=3><b>partition replica中的一个角色，在一个partition的多个副本中，会存在一个副本角色为leader；<br>producer和consumer只能跟leader交互(读写数据)；<br></b></font></p><h2 id="8-7-分区follower"><a href="#8-7-分区follower" class="headerlink" title="8.7 分区follower"></a>8.7 分区follower</h2><p><font size=3><b>partition replica中的一个角色，它通过心跳通信不断从leader中拉取、复制数据(只负责备份)。如果leader所在节点宕机，follower中会选举出新的leader；<br></b></font></p><h2 id="8-8-消息偏移量offset"><a href="#8-8-消息偏移量offset" class="headerlink" title="8.8 消息偏移量offset"></a>8.8 消息偏移量offset</h2><p><font size=3><b>partition中每条消息都会被分配一个递增id(offset)，通过offset可以快速定位到消息的存储位置；kafka只保证按一个partition中的消息的顺序，不保证一个topic整体(多个partition间)的顺序。<br></b></font></p><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051824.jpeg" alt="Kafka图3"></p><h1 id="9-Kafka的数据存储结构"><a href="#9-Kafka的数据存储结构" class="headerlink" title="9. Kafka的数据存储结构"></a>9. Kafka的数据存储结构</h1><h1 id="9-1-Kafka的整体存储结构"><a href="#9-1-Kafka的整体存储结构" class="headerlink" title="9.1 Kafka的整体存储结构"></a>9.1 Kafka的整体存储结构</h1><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051828.jpg" alt="Kafka图5"></p><h1 id="9-2-物理存储目录结构"><a href="#9-2-物理存储目录结构" class="headerlink" title="9.2 物理存储目录结构"></a>9.2 物理存储目录结构</h1><ul><li><p><font size=3><b>存储目录 名称规范：topic名称-分区号 <br>  例如：t1-0、t1-1<br>  “t1”即为一个topic的名称； <br>  而”t1-0&#x2F;t1-1”则表明这个目录是t1这个topic的哪个partition<br>  </b></font></p></li><li><p><font size=3><b>数据文件 名称规范 <br>  生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低，kafka采取了分片和索引机制； <br>  每个partition的数据将分为多个segment存储   <br>  每个segment对应两个文件，“.index文件”和“.log文件” <br>  index和log文件以当前segment的第一条消息的offset命名。<br>  </b></font></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051462.jpeg" alt="Kafka图4"></p><p><font size=3><b>index索引文件中的数据为：消息offset -&gt; log文件中该消息的物理偏移量位置；</b></font></p><p><font size=3><b>Kakfa中的索引文件以稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引；每当写入一定量(由broker端参数log.index.interval.bytes指定，默认值为4096，即4KB)的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应的可以缩小或增加索引项的密度； <br>查询指定偏移量时，使用二分查找法来快速定位偏移量的位置。<br></b></font></p><h1 id="9-3-消息的message存储结构"><a href="#9-3-消息的message存储结构" class="headerlink" title="9.3 消息的message存储结构"></a>9.3 消息的message存储结构</h1><p><font size=3><b>在客户端编程代码中，消息的封装有两种：ProducerRecord、ConsumerRecord<br>简单来说，++kafka中的每个message由一对key-value构成；++<br>Kafka中的message格式经历了3个版本的变化：v0、v1、v2<br></b></font></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v0</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051939.jpg" alt="Kafka图6"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v1</td></tr></table><p><font size=3><b>各个字段的含义介绍如下：</b></font></p><ul><li><font size=3><b>crc：占用4个字节，主要用于校验消息的内容；</b></font></li><li><font size=3><b>magic：这个占用1个字节，主要用于标识Kafka版本。Kafka 0.10.x magic默认值为1</b></font></li><li><font size=3><b>attributes：占用1个字节，这里面存储了消息压缩使用的编码以及Timestamp类型。目前Kafka支持gzip、snappy以及lz4(0.8.2引入)三种压缩格式；后四位如果是0001则表示gzip压缩，如果是0010则是snappy压缩，如果是0011则是lz4压缩，如果是0000则表示没有使用压缩。第4个bit位如果为0，代表使用create time；如果为1代表append time；其余位(第5~8位)保留</b></font></li><li><font size=3><b>key length：占用4个字节，主要标识Key的内容的长度；</b></font></li><li><font size=3><b>key：占用N个字节，存储的是key的具体内容；</b></font></li><li><font size=3><b>value length：占用4个字节，主要标识value的内容的长度；</b></font></li><li><font size=3><b>value：value即是消息的真实内容，在Kafka中这个也叫做payload。<br>  </b></font></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051829.jpg" alt="Kafka图7"></p><table><tr><td bgcolor=Gainsboro><font size=4><b>v2</td></tr></table><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132051007.jpg" alt="Kafka图8"></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5-7 Kafka API开发</title>
      <link href="/post/fa7bdefa.html"/>
      <url>/post/fa7bdefa.html</url>
      
        <content type="html"><![CDATA[<h1 id="5-Kafka-生产者API"><a href="#5-Kafka-生产者API" class="headerlink" title="5. Kafka 生产者API"></a>5. Kafka 生产者API</h1><p><font size=3><b>一个正常的生产逻辑需要具备以下几个步骤： <br>（1）配置生产者参数及创建相应的生产者实例   <br>（2）构建待发送的消息       <br>（3）发送消息   <br>（4）关闭生产者实例<br></b></font></p><p><font size=3><b>首先引入maven依赖<br></b></font></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//泛型K：要发送的数据中的key</span></span><br><span class="line">        <span class="comment">//泛型V：要发送的数据中的value</span></span><br><span class="line">        <span class="comment">//隐含之意：kafka中的message，是key-value结构的（可以没有key）</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;doit01:9092,doit02:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//因为kafka底层的存储是没有类型维护机制的，用户所发的所有数据类型，都必须变成序列化后的byte[]</span></span><br><span class="line">        <span class="comment">//所以，kafka的producer需要一个针对用户要发送的数据类型的序列化工具</span></span><br><span class="line">        <span class="comment">//且这个序列化工具类，需要实现kafka所提供的序列工具接口，org.apache.kafka.common.serialization.Serializer</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 代码中进行客户端参数配置的另一种写法</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092,doit02:9092&quot;</span>);</span><br><span class="line">        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        props.setProperty(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造一个生产者客户端</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//将业务数据封装成客户端所能发送的封装格式</span></span><br><span class="line">            <span class="comment">//0-&gt;abc0</span></span><br><span class="line">            <span class="comment">//1-&gt;abc1</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; message = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;abcx&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;abc&quot;</span> + i);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//调用客户端去发送</span></span><br><span class="line">            <span class="comment">//数据的发送动作在producer的底层是异步线程去异步发送的</span></span><br><span class="line">            producer.send(message);</span><br><span class="line"></span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭客户端</span></span><br><span class="line">        <span class="comment">//producer.flush();</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font size=3><b>消息对象ProducerRecord，除了包含业务数据外，还包含了多个属性：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br></pre></td></tr></table></figure><h1 id="6-Kafka-消费者API"><a href="#6-Kafka-消费者API" class="headerlink" title="6. Kafka 消费者API"></a>6. Kafka 消费者API</h1><h2 id="6-1-Kafka-消费者API示例"><a href="#6-1-Kafka-消费者API示例" class="headerlink" title="6.1 Kafka 消费者API示例"></a>6.1 Kafka 消费者API示例</h2><p><font size=3><b>一个正常的消费逻辑需要具备以下几个步骤： <br>（1）配置消费者客户端参数及创建相应的消费者实例   <br>（2）订阅主题topic <br>（3）拉取消息并消费 <br>（4）定期向__consumer_offsets主题提交消费位移offset   <br>（5）关闭消费者实例<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构建一个properties来存放消费者客户端参数</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092&quot;</span>);</span><br><span class="line">        props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//kafka的消费者，默认是从所属组之前所记录的偏移量开始消费，如果找不到之前的记录的偏移量，则从如下参数配置的策略来确定消费起始位移</span></span><br><span class="line">        <span class="comment">//可以选择：earliest（自动重置到每个分区的最前一条消息），latest（自动重置到每个分区的最新一条消息），none（没有重置策略）</span></span><br><span class="line">        props.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">&quot;latest&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置消费者所属的组id</span></span><br><span class="line">        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;d30-1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置自动提交最新的消费位移，默认是开启的</span></span><br><span class="line">        props.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自动提交最新消费位移的时间间隔，默认值就是5000ms</span></span><br><span class="line">        props.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">&quot;5000&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造一个消费者客户端</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//订阅主题（可以是多个）</span></span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">&quot;abcx&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//显示指定消费起始偏移量</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">abcxP0</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;abcx&quot;</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">abcxP1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;abcx&quot;</span>, <span class="number">1</span>);</span><br><span class="line">        consumer.seek(abcxP0, <span class="number">10</span>);</span><br><span class="line">        consumer.seek(abcxP1, <span class="number">15</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//循环往复拉取数据</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">condition</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (condition) &#123;</span><br><span class="line">            <span class="comment">//客户端去拉取数据的时候，如果服务器没有数据响应，会保持连接等待服务端响应</span></span><br><span class="line">            <span class="comment">//poll中传入的超时时长参数，是指等待的最大时长</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMinutes(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                <span class="comment">//ConsumerRecord中，不光有用户的业务数据，还有kafka塞入的元数据</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> record.key();</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> record.value();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//本条数据所属的topic</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> record.topic();</span><br><span class="line">                <span class="comment">//本条数据所属的分区</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> record.partition();</span><br><span class="line">                <span class="comment">//本条数据的offset</span></span><br><span class="line">                <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> record.offset();</span><br><span class="line">                <span class="comment">//当前这条数据所在分区的leader的朝代纪元</span></span><br><span class="line">                Optional&lt;Integer&gt; leaderEpoch = record.leaderEpoch();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//在kafka的数据底层存储中，不光有用户的业务数据，还有大量元数据</span></span><br><span class="line">                <span class="comment">//timestamp就是其中只一，记录本条数据的时间戳</span></span><br><span class="line">                <span class="comment">//但是时间戳有两种类型，本条数据的创建时间（生产者），本条数据的追加时间（borker写入log文件的时间）</span></span><br><span class="line">                <span class="type">TimestampType</span> <span class="variable">timestampType</span> <span class="operator">=</span> record.timestampType();</span><br><span class="line">                <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> record.timestamp();</span><br><span class="line"></span><br><span class="line">                <span class="comment">//数据头是生产者在写入数据时附加进去的（相当于用户自己自定义的元数据）</span></span><br><span class="line">                <span class="type">Headers</span> <span class="variable">headers</span> <span class="operator">=</span> record.headers();</span><br><span class="line"></span><br><span class="line">                System.out.println(String.format(<span class="string">&quot;数据key：%s, 数据value：%s, 数据所属的topic：%s, 数据所属的partition：%d,&quot;</span> +</span><br><span class="line">                                <span class="string">&quot;数据的offset：%d, 数据所属leader的纪元：%s, 数据时间戳类型：%s, 数据的时间戳：%d,&quot;</span>,</span><br><span class="line">                        key, value, topic, partition, offset, leaderEpoch.toString(), timestampType.name(), timestamp));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭客户端</span></span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-2-subscribe订阅主题"><a href="#6-2-subscribe订阅主题" class="headerlink" title="6.2 subscribe订阅主题"></a>6.2 subscribe订阅主题</h2><p><font size=3><b>subscribe有如下重载方法：</b></font></p><ul><li><font size=3><b>public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener)</b></font></li><li><font size=3><b>public void subscribe(Collection<String> topics)</b></font></li><li><font size=3><b>public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)</b></font></li><li><font size=3><b>public void subscribe(Pattern pattern)<br>  </b></font></li></ul><ol><li><font size=3><b>指定集合方式订阅主题： <br> consumer.subscribe(Arrays.asList(topic1));</b></font></li><li><font size=3><b>正则方式订阅主题<br> 如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅，在之后的过程中，如果有人又创建了新的主题，并且主题名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。<br> 正则表达式的方式订阅的示例如下<br> consumer.subscribe(Pattern.compile(“topic.*”));<br> 利用正则表达式订阅主题，可实现动态订阅。</b></font></li></ol><h2 id="6-3-assign订阅主题"><a href="#6-3-assign订阅主题" class="headerlink" title="6.3 assign订阅主题"></a>6.3 assign订阅主题</h2><p><font size=3><b>消费者不仅可以通过KafkaConsumer.subscribe()方法订阅主题，还可以直接订阅某些主题的指定分区；<br>在KafkaConsumer中提供了assign()方法来实现这些功能，此方法的具体定义如下:   <br>public void assign(Collection<TopicPartition> partitions) <br>这个方法只接受参数partitions，用来指定需要订阅的分区集合，示例如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;tpc_1&quot;</span>, <span class="number">0</span>), <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;tpc_2&quot;</span>, <span class="number">1</span>)));</span><br></pre></td></tr></table></figure><h2 id="6-4-subscribe和assign的区别"><a href="#6-4-subscribe和assign的区别" class="headerlink" title="6.4 subscribe和assign的区别"></a>6.4 subscribe和assign的区别</h2><ul><li><font color=red size=3><b>通过subscribe()方法订阅主题具有消费者自动再均衡功能；<br>  </b></font></li></ul><p><font size=3><b>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。<br></b></font></p><ul><li><font color=red size=3><b>assign()方法订阅分区时，是不具备消费者自动均衡功能的；<br>  </b></font></li></ul><p><font size=3><b>其实这一点从assign()方法参数可以看出端倪，两种类型subscribe()都有ConsumerRebalanceListener类型参数的方法，而assign()方法却没有。<br></b></font></p><h2 id="6-5-取消订阅"><a href="#6-5-取消订阅" class="headerlink" title="6.5 取消订阅"></a>6.5 取消订阅</h2><p><font size=3><b>既然有订阅，那么就有取消订阅。 <br>可以使用KafkaConsumer中的unsubscribe()方法取消主题的订阅，这个方法即可以取消通过subscribe(Collection)方式实现的订阅； <br>也可以取消通过subscribe(Pattern)方式实现的订阅，还可以取消通过assign(Collection)方式实现的订阅，示例如下：  <br>consumer.unsubscribe();   <br>如果将subscribe(Collection)或assign(Collection)集合参数设置为空集合，作用与unsubscribe()方法相同，如下示例中三行代码的效果相同：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe();</span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;());</span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure><h2 id="6-6-消息的消费模式"><a href="#6-6-消息的消费模式" class="headerlink" title="6.6 消息的消费模式"></a>6.6 消息的消费模式</h2><p><font size=3><b><u>Kafka中的消费是基于拉取模式的。</u></b></font><br><font size=3><b>消息的消费一般有两种模式：推送模式和拉取模式。推送模式是服务端主动将消息推送给消费者，而拉取模式消费者主动向服务端发起请求来拉取消息。</b></font></p><p><font size=3><b>Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复的调用poll()方法，poll()方法返回的是所订阅的主题（分区）上的一组消息。 <br>对于poll()方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空，如果订阅的所有分区中都没有可供消费的消息，那么poll()方法返回为空的信息集；</b></font></p><p><font size=3><b>poll()方法具体定义如下:   <br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)   <br>超过时间参数timeout，用来控制poll()方法的阻塞时间，在消费者的缓冲区里没有可用数据时会发生阻塞，如果消费者程序只用来单纯拉取并消费数据，则为了提高吞吐率，可以把timeout设置为Long.MAX_VALUE<br></b></font></p><h2 id="6-7-自动提交消费者偏移量"><a href="#6-7-自动提交消费者偏移量" class="headerlink" title="6.7 自动提交消费者偏移量"></a>6.7 自动提交消费者偏移量</h2><p><font size=3><b>Kafka中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数<u>enable.auto.commit</u>配置，默认值为true。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数<u>auto.commit.interval.ms</u>配置，默认值为5秒，此参数生效的前提是enable.auto.commit 参数为true。</b></font></p><p><font size=3><b>在默认的方式下，消费者每隔5秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。</b></font></p><p><font size=3><b>Kafka消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</b></font></p><ul><li><p><font size=3><b>重复消费 <br>  假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。</b></font></p></li><li><p><font size=3><b>丢失消息 <br>  按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看下图中的情形： <br>  拉取线程不断地拉取消息并存入本地缓存，比如在BlockingQueue中，另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第y+1次拉取，以及第m次位移提交的时候，也就是x+6之前的位移己经确认提交了，处理线程却还正在处理x+3的消息；此时如果处理线程发生了异常，待其恢复之后会从第m次位移提交处，也就是x+6的位置开始拉取消息，那么x+3至x+6之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。<br>  </b></font></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132042073.png" alt="Kafka图10"></p><h2 id="6-8-手动提交消费者偏移量-调用kafka-api"><a href="#6-8-手动提交消费者偏移量-调用kafka-api" class="headerlink" title="6.8 手动提交消费者偏移量(调用kafka api)"></a>6.8 手动提交消费者偏移量(调用kafka api)</h2><p><font size=3><b>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免；同时，自动位移提交也无法做到精确的位移管理。在Kafka中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。</b></font></p><p><font size=3><b>很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费；</b></font></p><p><font size=3><b>手动的提交方式可以让开发人员根据程序的逻辑在合适的时机进行位移提交。开启手动提交功能的前<br>提是消费者客户端参数++enable.auto.commit++配置为false，示例如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p><font size=3><b>手动提交可以细分为同步提交和异步提交，对应于KafkaConsumer中的commitSync()和commitAsync()两种类型的方法；<br></b></font></p><ul><li><font size=3><b>同步提交的方式 <br>  commitSync()方法的定义如下：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">    <span class="comment">//do something to process record.</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font size=3><b>对于采用commitSync()的无参方法，它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的，如果想寻求更细粒度的、更精准的提交，那么就需要使用commitSync()的另一个有参方法，具体定义如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, offsetAndMetadata&gt; offsets)</span></span><br></pre></td></tr></table></figure><p><font size=3><b>示例代码如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt;r : records)(</span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">        <span class="comment">//do something to process record.</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">        consumer.commitsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">offsetAndMetadata</span> (offset+<span class="number">1</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><font color=red size=3><b>提交的偏移量 &#x3D; 消费完的record的偏移量 + 1<br>因为，__consumer_offsets中记录的消费偏移量，代表的是，消费者下一次要读取的位置！！!<br></b></font></p><ul><li><font size=3><b>异步提交的方式 <br>  异步提交的方式(commitAsync())在执行的时候消费者线程不会被阻塞；可能在提交消费位移的结果还未返回之前就开始了新一次的拉取。异步提交可以让消费者的性能得到一定的增强。 <br>  commitAsync方法有一个不同的重载方法，具体定义如下：<br>  </b></font></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, offsetCommitCallback callback)</span></span><br></pre></td></tr></table></figure><p><font size=3><b>示例代码如下：<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset ()</span><br><span class="line">        <span class="comment">//do something to process record.</span></span><br><span class="line">        <span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">        consumer.commitSync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">        consumer.commitAsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> <span class="title class_">offsetAndMetadata</span>(offset+<span class="number">1</span>)), <span class="keyword">new</span> <span class="title class_">offsetcommitcallback</span>() &#123;</span><br><span class="line">        <span class="meta">@override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onComplete</span><span class="params">(Map&lt;TopicPartition, offsetAndMetadata&gt; map, Exception e)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(e == <span class="literal">null</span> ) &#123;</span><br><span class="line">                System.out.printIn (map);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                System.out.println (<span class="string">&quot;error commit offset&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-9-消费者提交偏移量方式的总结"><a href="#6-9-消费者提交偏移量方式的总结" class="headerlink" title="6.9 消费者提交偏移量方式的总结"></a>6.9 消费者提交偏移量方式的总结</h2><p><font size=3><b>consumer的消费位移提交方式：</b></font></p><ol><li><font size=3><b>全自动 auto.offset..commit &#x3D; true -&gt; 定时提交到consumer_offsets</b></font></li><li><font size=3><b>半自动 auto.offset.commit &#x3D; false；然后手动触发提交consumer.commitSync()； -&gt; 提交到consumer_offsets</b></font></li><li><font size=3><b>全手动 auto.offset.commit &#x3D; false；写自己的代码去把消费位移保存到你自己的地方<br> mysql&#x2F;zk&#x2F;redis&#x2F; -&gt; 提交到自己所涉及的存储；初始化时也需要自己去从自定义存储中查询到消费位移</b></font></li></ol><h2 id="6-7-其它重要参数"><a href="#6-7-其它重要参数" class="headerlink" title="6.7 其它重要参数"></a>6.7 其它重要参数</h2><ul><li><font size=3><b>fetch.min.bytes&#x3D;1B    （一次拉取的最小字节数）</b></font></li><li><font size=3><b>fetch.max.bytes&#x3D;50M   （一次拉取的最大数据量）</b></font></li><li><font size=3><b>fetch.max.wait.ms&#x3D;500ms    (拉取时的最大等待时长</b></font>)</li><li><font size=3><b>max.partition.fetch.bytes&#x3D;1MB    （每个分区一次拉取的最大数据量）</b></font></li><li><font size=3><b>max.poll.records&#x3D;500    （一次拉取的最大条数）</b></font></li><li><font size=3><b>connections.max.idle.ms&#x3D;540000ms    （网络连接的最大闲置时长）</b></font></li><li><font size=3><b>request.timeout.ms&#x3D;30000ms    （一次请求等待响应的最大超时时长，consumer等待请求响应的最长时间）</b></font></li><li><font size=3><b>metadata.max.age.ms&#x3D;300000    （元数据在限定时间内没有进行更新，则会被强制更新）</b></font></li><li><font size=3><b>reconnect.backoff.ms&#x3D;50ms    （尝试重新连接指定主机之前的退避时间）</b></font></li><li><font size=3><b>retry.backoff.ms&#x3D;100ms    （尝试重新拉取数据的重试间隔）<br>  </b></font></li></ul><h1 id="7-topic管理-API示例"><a href="#7-topic管理-API示例" class="headerlink" title="7. topic管理 API示例"></a>7. topic管理 API示例</h1><p><font size=3><b>如果希望将管理类的功能集成到公司内部的系统中，打造集管理、监控、运营、告警为一体的生态平台，那么就需要以程序调用API方式去实现。 <br>工具类KafkaAdminClient可以用来管理broker、配置和ACL(Access Control List)，管理topic。<br></b></font></p><ul><li><font size=3><b>创建主题：CreateTopicsResult createTopics(Collection&lt;NewTopic&gt; newTopics)</b></font></li><li><font size=3><b>删除主题：DeleteTopicsResult deleteTopics(Collection&lt;String&gt; topics)</b></font></li><li><font size=3><b>列出所有可用的主题：ListTopicsResult listTopics()</b></font></li><li><font size=3><b>查看主题的信息：DescribeTopicsResult describeTopics(Collection&lt;String&gt; topicNames)</b></font></li><li><font size=3><b>查询配置信息：DescribeConfigsResult describeConfigs(Collection&lt;\ConfigResource&gt;<br>  resources)</b></font></li><li><font size=3><b>修改配置信息：AlterConfigsResult alterConfigs(Map&lt;ConfigResource,Config&gt; configs)</b></font></li><li><font size=3><b>增加分区：CreatePartitionsResult createPartitions(Map&lt;String,NewPartitions&gt; newPartitions)<br>  </b></font></li></ul><p><font size=3><b>构造一个KafkaAdminClient<br></b></font></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br></pre></td></tr></table></figure><h2 id="7-1-列出主题"><a href="#7-1-列出主题" class="headerlink" title="7.1 列出主题"></a>7.1 列出主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics();</span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get();</span><br><span class="line">System.out.printIn(topics);</span><br></pre></td></tr></table></figure><h2 id="7-2-查看主题信息"><a href="#7-2-查看主题信息" class="headerlink" title="7.2 查看主题信息"></a>7.2 查看主题信息</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList (<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>));</span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line">Set&lt;String&gt; ksets = res.keySet();</span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123;</span><br><span class="line">    System.out.println(res.get(k));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7-3-创建主题"><a href="#7-3-创建主题" class="headerlink" title="7.3 创建主题"></a>7.3 创建主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;doit01:9092,doit02:9092,doit03:</span></span><br><span class="line"><span class="string">9092&quot;</span>)；</span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">3000</span>);</span><br><span class="line"><span class="comment">//创建admin client对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br><span class="line"><span class="comment">//由服务端controller自行分配分区及副本所在broker</span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>);</span><br><span class="line"><span class="comment">//手动指定分区及副本的broker分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"><span class="comment">//分区0，分配到broker0,broker1</span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>, Arrays.asList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line"><span class="comment">//分区1，分配到broker0,broker2</span></span><br><span class="line">replicaAssignments.put(<span class="number">1</span>, Arrays.asList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments);</span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3, tpc_4));</span><br><span class="line"></span><br><span class="line"><span class="comment">//从future中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    result.all().get()；</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure><h2 id="7-4-删除主题"><a href="#7-4-删除主题" class="headerlink" title="7.4 删除主题"></a>7.4 删除主题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, </span><br><span class="line"><span class="string">&quot;tpc_1&quot;</span>)):</span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line">System.out.printIn(values);</span><br></pre></td></tr></table></figure><h2 id="7-5-其它管理"><a href="#7-5-其它管理" class="headerlink" title="7.5 其它管理"></a>7.5 其它管理</h2><p><font size=3><b>除了进行topic管理外，KafkaAdminClient也可以进行诸如动态参数管理，分区管理等各类管理操作；<br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. Kafka 命令行工具</title>
      <link href="/post/79d4ebb8.html"/>
      <url>/post/79d4ebb8.html</url>
      
        <content type="html"><![CDATA[<h1 id="4-Kafka-命令行工具"><a href="#4-Kafka-命令行工具" class="headerlink" title="4. Kafka 命令行工具"></a>4. Kafka 命令行工具</h1><h2 id="4-1-topic管理操作：kafka-topics"><a href="#4-1-topic管理操作：kafka-topics" class="headerlink" title="4.1 topic管理操作：kafka-topics"></a>4.1 topic管理操作：kafka-topics</h2><h3 id="4-1-1-查看topic列表"><a href="#4-1-1-查看topic列表" class="headerlink" title="4.1.1 查看topic列表"></a>4.1.1 查看topic列表</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper doit01:2181</span><br></pre></td></tr></table></figure><h3 id="4-1-2-查看topic状态信息"><a href="#4-1-2-查看topic状态信息" class="headerlink" title="4.1.2 查看topic状态信息"></a>4.1.2 查看topic状态信息</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper doit01:2181 --describe --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/YiYuTET/ImageStorage/202304132030431.png" alt="Kafka图1"></p><p><font size=3><b>从上面的结果中，可以看出，topic的分区数量，以及每个分区的副本数量，以及每个副本所在的broker节点，以及每个分区的leader副本所在borker节点，以及每个分区的ISR副本列表；<br>ISR：in synchronized replicas 同步副本（当然也包含leader自身replica.lag.time.max.ms&#x3D;10000默认值） <br>OSR：out of synchronized replicas 失去同步的副本（该副本上次请求leader同步数据距现在的时间间隔超出配置阈值）<br></b></font></p><h3 id="4-1-3-创建topic"><a href="#4-1-3-创建topic" class="headerlink" title="4.1.3 创建topic"></a>4.1.3 创建topic</h3><p><font size=3><b>(1)基本方式<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --create --replication-factor 3 --partitions 3 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><font size=3><b>参数解释：</b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic名称</span><br></pre></td></tr></table></figure><p><font size=3><b>(2)手动指定分配方案：分区数，副本数，存储位置<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --create --topic <span class="built_in">test</span> --replica-assignment 0:1:3,1:2:6</span><br></pre></td></tr></table></figure><p><font size=3><b>该topic，将有如下partition：<br>partition0：所在节点：broker0、borker1、borker3   <br>partition1：所在节点：borker1、borker2、borker6<br></b></font></p><h3 id="4-1-4-删除topic"><a href="#4-1-4-删除topic" class="headerlink" title="4.1.4 删除topic"></a>4.1.4 删除topic</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --delete --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><p><font size=3><b>删除topic，server.properties中需要一个参数处于启用状态：delete.topic.enable&#x3D;true<br>使用kafka-topics.sh脚本删除主题的行为本质上只是在ZooKeeper中的&#x2F;admin&#x2F;delete_topics路径下建一个与待删除主题同名的节点，以标记该主题为待删除的状态，然后由kafka控制器异步完成。<br></b></font></p><h3 id="4-1-5-增加分区数"><a href="#4-1-5-增加分区数" class="headerlink" title="4.1.5 增加分区数"></a>4.1.5 增加分区数</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --partitions 3</span><br></pre></td></tr></table></figure><p><font size=3><b>kafka只支持增加分区，不支持减少分区 <br>原因是：减少分区，代价太大(数据的转移，日志段拼接合并) <br>如果真的需要实现此功能，则完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去。<br></b></font></p><h3 id="4-1-6-动态配置topic参数"><a href="#4-1-6-动态配置topic参数" class="headerlink" title="4.1.6 动态配置topic参数"></a>4.1.6 动态配置topic参数</h3><p><font size=3><b>通过管理命令，可以为已创建的topic增加、修改、删除topic level参数<br>添加&#x2F;修改  指定topic的配置参数：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --connfig compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b>–config compression.type&#x3D;gzip 修改或添加参数配置<br>–add-config compression.type&#x3D;gzip 添加参数配置   <br>–delete-config compression.type 删除参数配置<br></b></font></p><h2 id="4-2-生产者：kafka-console-producer"><a href="#4-2-生产者：kafka-console-producer" class="headerlink" title="4.2 生产者：kafka-console-producer"></a>4.2 生产者：kafka-console-producer</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-producer.sh --broker-list doit01:9092 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure><h2 id="4-3-消费者：kafka-console-consumer"><a href="#4-3-消费者：kafka-console-consumer" class="headerlink" title="4.3 消费者：kafka-console-consumer"></a>4.3 消费者：kafka-console-consumer</h2><p><font color=OrangeRed size=3><b>消费者在消费的时候，需要指定要订阅的主题，还可以指定消费的起始偏移量<br>消费的起始偏移量有3种策略： <br>earliest：从最早的消息开始消费 <br>lastes：从最新的消息开始消费 <br>指定offset（分区号，偏移量）：从指定的位置开始消费<br>kafka的topic中的消息，是有序号的（序号叫消费偏移量），而且消息的偏移量是在各个partition中独立维护的，在各个分区内，都是从0开始递增编号！<br></b></font></p><p><font size=3><b>(1)消费消息<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure><p><font size=3><b>(2)指定要消费的分区，和要消费的起始offset<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic <span class="built_in">test</span> --offset 2 --partition 0</span><br></pre></td></tr></table></figure><p><font size=3><b>(3）消费组 <br>消费组是kafka为了提高消费并行度的一种机制。 <br>消费组内的各个消费者之间，分担数据读取任务的最小单位是：partition<br>在kafka的底层逻辑中，任何一个消费者都有自己所属的组   <br>组和组之间，没有任何关系，大家都可以消费到目标topic的所有数据，但是组内的各个消费者，就只能读取到自己所分配到的partitions   <br>kafka中的消费组，可以动态增减消费者，而且消费组中的消费者数量发生任意变动，都会重新分配分区消费任务。<br></b></font></p><h2 id="4-4-消费位移的记录"><a href="#4-4-消费位移的记录" class="headerlink" title="4.4 消费位移的记录"></a>4.4 消费位移的记录</h2><p><font size=3><b>kafka的消费者，可以记录自己所消费到的消息偏移量，记录的这个偏移量就叫消费位移，记录这个消费到的位置，作用就在于消费者重启后可以接续上一次消费到位置来继续往后面消费。<br></b></font></p><p><font color=OrangeRed size=3><b>消费位移，是组内共享的！！！<br></b></font></p><p><font color=OrangeRed size=3><b>consumer去记录偏移量的时候，不是读到一条或一批数据就记录一次，而是周期性的去提交当前的位移<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server doit01:9092 --topic __consumer_offsets --formatter <span class="string">&quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot;</span></span><br></pre></td></tr></table></figure><p><font size=3><b>通过指定formatter工具类，来对__consumer_offsets主题中的数据进行解析；<br>如果需要获取某个特定consumer-group的消费偏移量信息，则需要计算该消费组的偏移量记录所在分区：Math.abs(groupID.hashCode())%numPartitions<br>__consumer_offsets的分区数为：50<br></b></font></p><h2 id="4-5-配置管理kafka-config"><a href="#4-5-配置管理kafka-config" class="headerlink" title="4.5 配置管理kafka-config"></a>4.5 配置管理kafka-config</h2><p><font size=3><b>kafka-config.sh脚本是专门用来对参数配置进行操作的，这里的操作是运行状态修改原有的配置，因此可以达到动态变更的目的； <br>动态配置的参数，会被存储在zookeeper上，因而是持久生效的 <br>kafka-configs.sh脚本包含：变更alter、查看describe这两种指令类型；<br>kafka-configs.sh支持主题、broker、用户和客户端这4个类型的配置。 <br>kafka-configs.sh脚本使用entity-type参数来指定操作配置的类型，并且使entity-name参数来指定配置的名称。<br></b></font></p><p><font size=3><b>比如查看topic的配置可以按如下方式执行：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --describe --entity-type topics --entity-name tpc_2</span><br></pre></td></tr></table></figure><p><font size=3><b>比如查看broker的动态配置可以按如下方式执行：<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --describe --entity-type brokers --entity-name tpc_2</span><br></pre></td></tr></table></figure><p><font color=#0066FF size=3><b>entity-type和entity-name的对应关系<br></b></font></p><table><thead><tr><th><strong>entity-type的释义</strong></th><th><strong>entity-name的释义</strong></th></tr></thead><tbody><tr><td><strong>主题类型的配置，取值为topics</strong></td><td><strong>指定主题的名称</strong></td></tr><tr><td><strong>broker类型的配置，取值为brokers</strong></td><td><strong>指定brokerId值，即broker中broker.id参数配置的值</strong></td></tr><tr><td><strong>客户端类型的配置，取值为clients</strong></td><td><strong>指定clientId值，即KafkaProducer或KafkaConsumer的client.id参数配置的值</strong></td></tr><tr><td><strong>用户类型的配置，取值为users</strong></td><td><strong>指定用户名</strong></td></tr></tbody></table><p><font size=3><b>示例：添加topic级别参数<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit:2181 --alter --entity-type topics --entity-name tpc22 --add-config cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p><font size=3><b>使用kafka-configs.sh脚本来变更(alter)配置时，会在ZooKeeper中创建一个命名形式为：&#x2F;config&#x2F;<entity-type>&#x2F;<entity name>的节点，并将变更的配置写入这个节点<br></b></font></p><h3 id="4-5-1-动态配置topic参数"><a href="#4-5-1-动态配置topic参数" class="headerlink" title="4.5.1 动态配置topic参数"></a>4.5.1 动态配置topic参数</h3><p><font size=3><b>通过管理命令，可以为已创建的topic增加、修改、删除topic level参数</b></font></p><ul><li><font size=3><b>添加&#x2F;修改  指定topic的配置参数：<br>  </b></font></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper doit01:2181 --alter --topic <span class="built_in">test</span> --connfig compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b>如果利用kafka-configs.sh脚本来对topic、producer、consumer、borker等进行参数动态配置</b></font></p><ul><li><font size=3><b>添加、修改配置参数<br>  </b></font></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit01:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip</span><br></pre></td></tr></table></figure><p><font size=3><b><br>-删除s配置参数<br></b></font></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper doit01:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1-3 Kafka 简介</title>
      <link href="/post/58eeb88f.html"/>
      <url>/post/58eeb88f.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Kafka-基本概念"><a href="#1-Kafka-基本概念" class="headerlink" title="1. Kafka 基本概念"></a>1. Kafka 基本概念</h1><h2 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h2><p><font size=3><b>Kafka最初是由LinkedIn即领英公司基于Scala和Java语言开发的分布式消息发布-订阅系统，现已捐献给Apache软件基金会。其具有高吞吐、低延迟的特性，许多大数据实时流式处理系统比如Storm、Spark、Flink等都能很好的与之集成。<br></b></font></p><p><font size=3><b>总的来讲，Kafka通常具有3重角色：</b></font></p><ul><li><font size=3><b>存储系统：通常消息队列会把消息持久化到磁盘，防止消息丢失，保证消息可靠性。Kafka的消息持久化机制和多副本机制使其能够作为通用数据存储系统使用。</b></font></li><li><font size=3><b>消息系统：Kafka和传统的消息队列比如RabbitMQ、RocketMQ、ActiveMQ类似，支持流量削峰、服务解耦、异步通信等核心功能。</b></font></li><li><font size=3><b>流处理平台：Kafka不仅能够与大多数流式计算框架完美整合，并且自身也提供了一个完整的流式处理库，即Kafka Streaming。Kafka Streaming提供了类似Flink中的窗口、聚合、变换、连接等功能。<br>  </b></font></li></ul><p><font size=3><b><u>一句话概括：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列(Message Queue)，在业界主要应用于大数据实时流式计算领域。</u><br></b></font></p><h2 id="1-2-kafka的特点"><a href="#1-2-kafka的特点" class="headerlink" title="1.2 kafka的特点"></a>1.2 kafka的特点</h2><ul><li><font size=3><b>高吞吐、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition，由多个consumer group对partition进行consume操作。</b></font></li><li><font size=3><b>可扩展性：kafka集群支持热扩展</b></font></li><li><font size=3><b>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</b></font></li><li><font size=3><b>容错性：允许集群中有节点失败(若副本数量为n，则允许n-1个节点失败)</b></font></li><li><font size=3><b>高并发：支持数千个客户端同时写入<br>  </b></font></li></ul><p><font size=3><b>Kafka在各种应用场景中，起到的作用可以归纳为这么几个术语：削峰填谷、解耦！在大数据流式计算场景领域中，kafka主要作为计算系统的前置缓存和输出结果缓存。<br></b></font></p><h1 id="2-安装部署"><a href="#2-安装部署" class="headerlink" title="2. 安装部署"></a>2. 安装部署</h1><ul><li>上传安装包</li><li>解压</li><li>修改配置文件<br>  (1)进入配置文件系统</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@doit01 apps]# cd kafka_2.12-2.3.1/config</span><br></pre></td></tr></table></figure><p>(2)编辑配置文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为依次增长的：0、1、2、3、4，集群中唯一id</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 数据存储的目录</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/opt/apps/data/kafkadata</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 底层存储的数据(日志)留存时长(默认7天)</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 底层存储的数据(日志)留存量(默认1G)</span></span><br><span class="line"><span class="attr">log.retention.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 指定zk集群地址</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">doit01:2181,doit02:2181,doit03:2181</span></span><br></pre></td></tr></table></figure><ul><li>分发安装包</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..3&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">scp -r kafka_2.11-2.2.2 linux0<span class="variable">$i</span>:<span class="variable">$PWD</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>安装包分发后，记得修改broker.id</p><ul><li>配置环境变量</li><li>启停集群(在各个节点上启动)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon /opt/apps/kafka_2.11-2.2.2/config/server.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止集群</span></span><br><span class="line">bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure><h1 id="3-Kafka运维监控"><a href="#3-Kafka运维监控" class="headerlink" title="3. Kafka运维监控"></a>3. Kafka运维监控</h1><p><font color=red size=3><b>详见 <a href="https://www.bilibili.com/video/BV1Xr4y1t7mQ?p=6&vd_source=dd05d982b6c8a7e631e7f07548a539b1">https://www.bilibili.com/video/BV1Xr4y1t7mQ?p=6&amp;vd_source=dd05d982b6c8a7e631e7f07548a539b1</a><br></b></font></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Kafka</title>
      <link href="/post/8716a180.html"/>
      <url>/post/8716a180.html</url>
      
        <content type="html"><![CDATA[<h1 id="云服务器伪分布式部署kafka"><a href="#云服务器伪分布式部署kafka" class="headerlink" title="云服务器伪分布式部署kafka"></a>云服务器伪分布式部署kafka</h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><p>本次安装的kafka使用了独立的zookeeper，所以需先搭建好zookeeper集群。</p><p><a href="https://yiyuyyds.cn/archives/centos7---zookeeper%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2">ZooKeeper伪分布式安装部署</a></p><p><a href="https://kafka.apache.org/downloads">Kafka下载地址</a></p><h2 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2.解压安装包"></a>2.解压安装包</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HOME=/home/software/kafka_2.12-3.0.0</span><br><span class="line">export PATH=$PATH:/$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="4-复制server-properties文件并改名"><a href="#4-复制server-properties文件并改名" class="headerlink" title="4.复制server.properties文件并改名"></a>4.复制server.properties文件并改名</h2><p>进入kafka的config路径下，复制server.properties配置文件，因为是伪分布式安装，一台机器上有3个kafka进程，所以这里复制3份。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp server.properties server_1.properties</span><br><span class="line">cp server.properties server_2.properties</span><br><span class="line">cp server.properties server_3.properties</span><br></pre></td></tr></table></figure><h2 id="5-分别输入如下配置"><a href="#5-分别输入如下配置" class="headerlink" title="5.分别输入如下配置"></a>5.分别输入如下配置</h2><h3 id="server-1-properties"><a href="#server-1-properties" class="headerlink" title="server_1.properties"></a>server_1.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_1</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9092</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h3 id="server-2-properties"><a href="#server-2-properties" class="headerlink" title="server_2.properties"></a>server_2.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_2</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9093</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9093</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h3 id="server-3-properties"><a href="#server-3-properties" class="headerlink" title="server_3.properties"></a>server_3.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">log.dirs=/home/software/kafka_2.12-3.0.0/logs/log_server_3</span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:9094</span><br><span class="line">advertised.listeners=PLAINTEXT://114.116.24.98:9094</span><br><span class="line">zookeeper.connect=192.168.0.219:2181</span><br></pre></td></tr></table></figure><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6.启动"></a>6.启动</h2><p>进入kafka的bin目录下，运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon ../config/server_1.properties</span><br><span class="line">kafka-server-start.sh -daemon ../config/server_2.properties</span><br><span class="line">kafka-server-start.sh -daemon ../config/server_3.properties</span><br></pre></td></tr></table></figure><p>通过jps命令可以查看3个kafka进程说明部署成功。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 安装教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通义万相AI绘画大模型体验</title>
      <link href="/post/139d3597.html"/>
      <url>/post/139d3597.html</url>
      
        <content type="html"><![CDATA[<h1 id="通义万相AI绘画大模型体验"><a href="#通义万相AI绘画大模型体验" class="headerlink" title="通义万相AI绘画大模型体验"></a>通义万相AI绘画大模型体验</h1><p>官网：<a href="https://wanxiang.aliyun.com/">https://wanxiang.aliyun.com</a></p><p>通义万相是<a href="https://baike.baidu.com/item/%E9%98%BF%E9%87%8C%E4%BA%91/297128?fromModule=lemma_inlink">阿里云</a>通义系列AI绘画创作大模型，该模型可辅助人类进行图片创作，于2023年7月7日正式上线。</p><p>前两天通过了通义万相的适用体验，一番试用下来，感触最深的便是，当今AI横行，唯有融入并去接受他们才能适应时代。</p><p>在我的使用场景下，一般是生成封面图，或者一些唯美超现实主义的图片。</p><p>作为阿里推出的一款大模型AI绘画，每日免费使用50次，一般的用户基本都能满足，这点让我极其舒适（偷笑😏），能够使用中文绘制。</p><p><img src="https://s3.bmp.ovh/imgs/2024/03/02/4d2787ff83cec98b.png" alt="AI生成图"></p><p>可选的绘制比例有<code>1:1</code>,<code>16:9</code>,<code>9:16</code></p><p>绘制时间一般在<code>1分钟以内</code>，算是比较快的了，可能和描述的多少有关。</p><p>有时候它也会无法绘制（会返还灵感值），猜测使用英文绘制会更好，也许中文支持并没有那么高。</p><p><img src="https://s3.bmp.ovh/imgs/2024/03/02/5f02cd1d21849720.png" alt="AI生生成图"></p><p>绘制后会产生4张图片，也能够继续选择风格继续生成。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务器伪分布式部署Hadoop</title>
      <link href="/post/da2f3447.html"/>
      <url>/post/da2f3447.html</url>
      
        <content type="html"><![CDATA[<h2 id="云服务器伪分布式部署Hadoop"><a href="#云服务器伪分布式部署Hadoop" class="headerlink" title="云服务器伪分布式部署Hadoop"></a>云服务器伪分布式部署Hadoop</h2><h2 id="1-hadoop安装包下载-下载地址"><a href="#1-hadoop安装包下载-下载地址" class="headerlink" title="1.hadoop安装包下载 下载地址"></a>1.hadoop安装包下载 <a href="https://downloads.apache.org/hadoop/common/">下载地址</a></h2><p>下载后缀名为.tar.gz的文件</p><h2 id="2-上传安装包到服务器并解压"><a href="#2-上传安装包到服务器并解压" class="headerlink" title="2.上传安装包到服务器并解压"></a>2.上传安装包到服务器并解压</h2><p>这里我解压到&#x2F;home&#x2F;software目录下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.2.2.tar.gz  -C /home/software/</span><br></pre></td></tr></table></figure><h2 id="3-配置ssh免密登录"><a href="#3-配置ssh免密登录" class="headerlink" title="3.配置ssh免密登录"></a>3.配置ssh免密登录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &#x27;&#x27; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="4-添加环境变量"><a href="#4-添加环境变量" class="headerlink" title="4.添加环境变量"></a>4.添加环境变量</h2><p>修改&#x2F;etc&#x2F;profile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/software/hadoop-3.2.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>使环境变量立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="5-配置hadoop"><a href="#5-配置hadoop" class="headerlink" title="5.配置hadoop"></a>5.配置hadoop</h2><p>进入hadoop安装目录下的etc&#x2F;hadoop文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hadoop-3.2.2/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="5-1-配置hadoop-env-sh"><a href="#5-1-配置hadoop-env-sh" class="headerlink" title="5.1 配置hadoop-env.sh"></a>5.1 配置hadoop-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p>添加如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/software/jdk1.8.0_202</span><br><span class="line"></span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><h3 id="5-2-配置core-site-xml"><a href="#5-2-配置core-site-xml" class="headerlink" title="5.2 配置core-site.xml"></a>5.2 配置core-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.0.219:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置HDFS数据块和元数据保存的目录，一定要修改--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意，要在hadoop目录下创建tmp目录</p><h3 id="5-3-配置hdfs-site-xml"><a href="#5-3-配置hdfs-site-xml" class="headerlink" title="5.3 配置hdfs-site.xml"></a>5.3 配置hdfs-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp/nameNodeDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/software/hadoop-3.2.2/tmp/dataNodeDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 &#x2F;tmp&#x2F;hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p><h3 id="5-4-配置mapred-site-xml"><a href="#5-4-配置mapred-site-xml" class="headerlink" title="5.4 配置mapred-site.xml"></a>5.4 配置mapred-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史服务器端地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5-5-配置yarn-site-xml"><a href="#5-5-配置yarn-site-xml" class="headerlink" title="5.5 配置yarn-site.xml"></a>5.5 配置yarn-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><p>yarn默认端口是8088，但是这里必须要修改yarn端口，原因是因为像yarn、redis这种8088和6379端口是近年来挖矿病毒的热门踩点端口，具体的可参考这篇文章<a href="https://segmentfault.com/a/1190000015264170">https://segmentfault.com/a/1190000015264170</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--配置Yarn的节点--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--NodeManager执行MR任务的方式是Shuffle洗牌--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--修改yarn端口为7776--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.0.219:7776<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2022-6-21-更新：以下步骤不需要了，直接跳过"><a href="#2022-6-21-更新：以下步骤不需要了，直接跳过" class="headerlink" title="2022&#x2F;6&#x2F;21 更新：以下步骤不需要了，直接跳过"></a>2022&#x2F;6&#x2F;21 更新：以下步骤不需要了，直接跳过</h2><p><del>### 5.5 sbin目录修改4个文件<br>在Hadoop安装目录下找到sbin文件夹<br>在里面修改四个文件<br>对于start-dfs.sh和stop-dfs.sh文件，在文件开头添加下列参数：</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/env bash</span></span><br><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure><p><del>对于start-yarn.sh和stop-yarn.sh文件，在文件开头添加下列参数：</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/env bash</span></span><br><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><h2 id="6-执行-NameNode-的格式化"><a href="#6-执行-NameNode-的格式化" class="headerlink" title="6.执行 NameNode 的格式化"></a>6.执行 NameNode 的格式化</h2><p>进入hadoop目录下的bin目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/software/hadoop-3.2.2/bin/</span><br></pre></td></tr></table></figure><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>成功的话，会看到 “Storage directory XXX&#x2F;XXX&#x2F;XXX has been successfully formatted” 的提示。</p><h2 id="6-启动"><a href="#6-启动" class="headerlink" title="6.启动"></a>6.启动</h2><h3 id="6-1启动hdfs"><a href="#6-1启动hdfs" class="headerlink" title="6.1启动hdfs"></a>6.1启动hdfs</h3><p>进入hadoop目录下的bin目录，启动hdfs</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>查看java进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如有以下5个进程，说明hadoop伪分布部署成功<br>NameNode<br>SecondaryNameNode<br>ResourceManager<br>NodeManager<br>DataNode</p><p>访问服务器9870端口，出现hadoop的Web界面。<br>访问服务器7776端口，出现yarn界面。</p><h3 id="6-2启动历史服务器"><a href="#6-2启动历史服务器" class="headerlink" title="6.2启动历史服务器"></a>6.2启动历史服务器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p>出现JobHistoryServer进程，访问服务器19888端口，出现hadoop的JobHistory界面。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装教程 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
